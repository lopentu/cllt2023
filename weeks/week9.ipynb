{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Week 9 計算語意\"\n",
        "author: \"謝舒凱 台大語言所\"\n",
        "format: \n",
        "    revealjs:\n",
        "        incremental: true\n",
        "        slide-number: true\n",
        "        preview-links: auto\n",
        "        # chalkboard: true\n",
        "        logo: ../images/logo_round.png\n",
        "\n",
        "bibliography: ../cllt.bib\n",
        "---"
      ],
      "id": "744cb4d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 計算語意\n",
        "\n",
        ">  一切都跟語意有關！（文本的表徵就是文本語意的表徵，結構是附加學習到的）\n",
        "\n",
        "語意表徵 (在底層) 推動 NLP/AI 的發展: **feature learning, architecture learning, objective learning, prompt engineering** \n",
        "\n",
        "## NLP 極簡史\n",
        "\n",
        "![feature engineering to architecture learning:資料不夠，重視手工特徵工程，知識驅動，到神經網路的架構學習。](../images/nlp1.png){width=\"80%\"}\n",
        "\n",
        "\n",
        "\n",
        "![pre-train and fine-tune: a model with fixed architecture is pre-trained as a language model, fine-tuned to different downstream tasks.](../images/nlp2.png){width=\"80%\"}\n",
        "\n",
        "<!-- With the introduction of transformers and transfer learning, all that was needed to adapt a language model for different tasks was a few small layers at the end of the network (the head) and a little fine-tuning. -->\n",
        "\n",
        "![(pre-trained,) prompt and predict: NLP tasks are reformulated with the help of (textual) prompt.](../images/nlp3.png){width=\"80%\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 語言模型 (Language Model)\n",
        "\n",
        "- 語言模型是一個函式，它可以計算一個句子的機率。\n",
        "\n",
        "- NLP 的發展幾乎可以歸納爲語言模型的訓練發展。（from *n-gram* statistical LMs to neural LMs）\n",
        "\n",
        "![語言模型對自己的定義 ](../images/lang_model.png){width=\"90%\"}\n",
        "\n",
        "\n",
        "\n",
        "# 機器學習\n",
        "[快速參考](https://www.youtube.com/watch?v=phQK8xZpgoU)\n",
        "\n",
        "\n",
        "- NLP 的發展與機器學習密不可分。\n",
        "\n",
        "- 希望機器能從餵給它的資料中學到一個預測方法。數學上地說，就是學到一個函式。\n",
        "\n",
        "\n",
        "\n",
        "## 機器學習概覽\n",
        "\n",
        "![types: source internet](../images/ml_type.webp){width=\"80%\"}\n",
        "\n",
        "# 機器學習的種類\n",
        "\n",
        "- 按照所預測輸出的類型爲類別 (label) 或數值 (real-valued) 之不同，可分為分類 (classification) 、迴歸 (regression) 、生成式學習 (Generative Learning) 等。\n",
        "\n",
        "- 按照學習的行爲可分\n",
        "\n",
        "    - 監督式學習 (Supervised/Predictive learning) ：給定標準答案，學輸入與輸出的**映射 (mapping)**。\n",
        "    <!-- （半監督式學習 (Semi-supervised learning) ：給定少量標準答案。 -->\n",
        "    - 非監督式學習 (Unsupervised/Descriptive learning) ：給定輸入，學輸入的模式**分佈 (distribution)**。\n",
        "\n",
        "    - 強化式學習 (Reinforcement learning) ：給定獎勵或懲罰，學**如何行動或行為** (how to act ot behave)。\n",
        "    \n",
        "\n",
        "\n",
        "## Parametric vs. Non-parametric\n",
        "\n",
        "\n",
        "- Parametric model：參數化的模型，參數的數目是固定的，模型的複雜度是固定的。\n",
        "- Non-parametric model：非參數化的模型，參數的數目是不固定的，隨着資料大小而不同。\n",
        "\n",
        "## 生成式學習 (Generative Learning) \n",
        "a.k.a. **結構化學習 (structured learning)**\n",
        "\n",
        "- 結構化學習是指輸出爲一個結構化的物件，如語音辨識、機器翻譯、語言模型等。\n",
        "<!-- http://violin-tao.blogspot.com/2018/01/ml-lecture-21-structure-learning.html -->\n",
        "\n",
        "\n",
        "## 機器學習\n",
        "\n",
        "- 選擇 (候選) 的函式集合\n",
        "<!-- model assumption? -->\n",
        "\n",
        "- 決定評測函式的方法 (e.g., loss function)\n",
        "\n",
        "- 找最好的函式 (最佳化 optimization)\n",
        " \n",
        ":::{.callout-note}\n",
        "chatGPT (貌似) 將生成式學習解成分類問題。\n",
        ":::\n",
        "\n",
        "\n",
        "# 深度學習的數學\n",
        "\n",
        "- 深度學習是一種機器學習方法，它使用多層神經網路來進行學習。參數是**權重 (weight)** 與**偏差 (bias)**。\n",
        "\n",
        "- 深度學習的核心是**反向傳播算法 (backpropagation)**，它可以自動地計算損失函數對模型參數的梯度。\n",
        "\n",
        "- 深度學習的損失函數是**交叉熵 (cross-entropy)**。\n",
        "\n",
        "- 深度學習的最佳化方法是**隨機梯度下降法 (stochastic gradient descent)**。\n",
        "\n",
        "> 涉及的數學知識：函數概念、統計與機率、微積分、線性代數（向量內積、矩陣運算）、多變數函數的微分（梯度下降）等。\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 我們先來複習一下需要的數學知識\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 現在用一個例子來說明\n",
        "假設我們的任務是 Linear Regression\n",
        "\n",
        "- 線性迴歸是一種迴歸模型，它假設輸入與輸出之間存在線性關係。\n",
        "- 目標是找到一條線，使得所有點到該線的距離之和最小。\n",
        "- 參數是線的斜率與截距。\n",
        "- 損失函數是均方誤差 (Mean Squared Error, MSE)。\n",
        "- 最佳化方法是梯度下降法 (Gradient Descent)。\n",
        "\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "- 邏輯回歸是一種分類模型，它假設輸入與輸出之間存在線性關係。\n",
        "\n",
        "\n",
        "\n",
        "<!-- 2nd hour -->\n",
        "\n",
        "# 回到語意學習\n",
        "先從字詞（語意）的表徵方法開始\n",
        "\n",
        "- 詞的表徵方法：將一個詞用一個向量表徵，該向量的每個元素代表該詞的某種特徵 (不見得可以解讀出來)。\n",
        "    \n",
        "- 方法有很多種，例如：one-hot encoding、distributional semantics、word embeddings等。\n",
        "\n",
        "\n",
        "\n",
        "## 獨熱表示法 | One-hot Encoding \n",
        "\n",
        "\n",
        "- 用一個詞表大小的向量來表示一個詞，（假設詞表爲 $V$，其大小爲 $|\\mathbb{V}|$），然後將該詞在詞表中的位置設爲 1，其餘位置設爲 0。\n",
        "\n",
        "![](../images/onehot.png){width=\"100%\"}\n",
        "\n",
        "如此在該向量中，詞表裡第 $i$ 個詞在第 $i$ 維上被設爲 1，其餘維均爲 0。 \n",
        "\n",
        "\n",
        "- 問題：不同的詞的向量完全不同，因此無法用來表示詞的相似性（e.g., cosine similarity = 0）; 資料稀疏 (sparse) 時，無法充分學習詞的意義。\n",
        "\n",
        "# 詞的分佈語意表徵 | distributional semantics\n",
        "\n",
        "- 分佈假說 (**distributional hypothesis**)：詞彙意義是由它所出現的上下文分佈語境 (context) 決定的。\n",
        "\n",
        ":::{.callout-important}\n",
        "怎麼決定 operationalize 語境呢？window size?\n",
        ":::\n",
        "\n",
        "- 兩種 Co-occurrence Matrix\n",
        "\n",
        "    - **term-document matrix**\n",
        "    - **term-term matrix**\n",
        "\n",
        "- 問題：高頻詞誤導、higher-order 無法導出（A-B, B-C $\\rightarrow$ A-C）、一樣有資料稀疏性問題。\n",
        "\n",
        "\n",
        "## 解法一：Pointwise Mutual Information (PMI)\n",
        "\n",
        "> 想法：若一個詞和許多詞共現，則降低其權重；反之提高。\n",
        "\n",
        "- Information theory 中有一個 PMI 剛好用來衡量兩個詞的共現機率與各自單獨出現的機率之比值。\n",
        "\n",
        "\n",
        "\n",
        "## 解法二：tf-idf\n",
        "tf-idf (term frequency-inverse document frequency) \n",
        "\n",
        "> 想法：一般來說，詞 t 在文件 d 中出現的次數越多，該詞對於文件 d 的重要性越高；但是，如果詞 t 在語料庫 D 中出現的文件數量越多，則該詞對於語料庫 D 的重要性越低，因為該詞對於語料庫 D 的特徵性不強。\n",
        "\n",
        "- 一種統計方法，用以評估一個詞對於一個文件集或一個語料庫中的其中一份文件的重要程度。\n",
        "    - $tf(t,d)$: term frequency, 詞 t 在文件 d 中出現的次數。\n",
        "    - $idf(t,D)$: inverse document frequency, 詞 t 在語料庫 D 中出現的文件數量的倒數。\n",
        "\n",
        "- 取個平衡 $tf-idf(t,d,D) = tf(t,d) \\times idf(t,D)$\n",
        "\n",
        "\n",
        "## 解法三：奇異值分解 | Singular Value Decomposition (SVD)\n",
        "\n",
        "- 一種矩陣分解方法，將一個共現矩陣 ($M$) 分解成三個矩陣的乘積。\n",
        "\n",
        "    - $M = U \\Sigma V^T$， 其中 $U$ 為一個 $m \\times m$ 的矩陣，$\\Sigma$ 為一個 $m \\times n$ 的矩陣，$V$ 為一個 $n \\times n$ 的矩陣。\n",
        "\n",
        "- 用於降維，例如：將一個 $m \\times n$ 的矩陣分解成三個 $m \\times k$ 的矩陣的乘積，其中 $k$ 為一個較小的數字，這樣就可以將原本 $m \\times n$ 的矩陣降維到 $m \\times k$ 的矩陣。\n"
      ],
      "id": "d8d590ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SVD example\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define a PMI matrix\n",
        "M = np.array([[0,\n",
        "\n",
        "\n",
        "U, s, V = np.linalg.svd(M)\n",
        "words = ['我','很','愛','你','吃','飯','吃飯','吃飯','很','愛','你','睡覺','也','愛','你']\n",
        "for i, word in enumerate(words):\n",
        "    plt.text(U[i, 0], U[i, 1], word[i])\n",
        "plt.show()"
      ],
      "id": "bfe85b57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 令人困擾的語言表徵問題\n",
        "Natural Language as (discrete) **symbols** or (continuous) **signals**?\n",
        "\n",
        "![source: Joty 2022](../images/nl_signal.png){width=\"90%\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Vector Semantics Embeddings\n",
        "- 一種表徵方法，將一個物件用一個向量表徵。例如，word embeddings 將一個詞用一個向量表徵，該向量的每個元素代表該詞的某個特徵。\n",
        "\n",
        "<!-- - 一般來說，該向量的元素是實數，且該向量的長度是固定的。 -->\n",
        "\n",
        "- count-based and prediction-based methods\n",
        "\n",
        "## 之前做法\n",
        "\n",
        "- one-hot encoding\n",
        "\n",
        "- tf-idf (term frequency-inverse document frequency) $\\leftarrow$ 現在可當成 baseline\n",
        "\n",
        "- **詞袋假說** bag-of-words (BoW) assumption： 詞的順序 (order)、語境 (context) 不重要，只看詞的出現次數。\n",
        "\n",
        "\n",
        "## cosine similarity\n",
        "\n",
        "\n",
        "## PMI (Pointwise Mutual Information)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Sparse versus dense vectors\n",
        "\n",
        "\n",
        "\n",
        "## 用深度學習學習表徵 (Representation Learning)\n",
        "- 自動學習特徵 (features) 或表徵 (representations)。\n",
        "\n",
        "- 從輸入 $x$ ，利用多層神經網路學習多層次表徵 ($h^{1}, h^{2}, h^{3}, ..$) 以及最終的輸出 $h^{4}$。\n",
        "\n",
        "- 這種學習表徵的方法被稱為 **representation learning** (對比於需要手工的 **feature engineering**)。\n",
        "\n",
        "![](../images/dl.jpeg){width=\"30%\"}\n",
        "\n",
        ":::{.callout-note}\n",
        "$x$ 可以是 sound, pixels, words, etc. 深度學習在 **speech, vision, language** 上取得了很大的進展。\n",
        ":::\n",
        "\n",
        "\n",
        "## `Word2Vec`\n",
        "“Neural Language Model”-inspired models\n",
        "\n",
        "\n",
        "\n",
        "- 計算 embeddings 的方法：**skip-gram with negative sampling**\n",
        "\n",
        "\n",
        ":::{.callout-note}\n",
        "被 Mikolov 在 2013 年提出，並包成一個 package (`word2vec`)。\n",
        ":::\n",
        "\n",
        "## Self-supervised Learning\n",
        "\n",
        "- 鄰近詞就是好答案，對supervised learning 而言就不再需要人爲標記。\n",
        "\n",
        "## Skip-gram with Negative Sampling\n",
        "\n",
        "1. Treat the target word *t* and a neighboring context word *c* as positive examples\n",
        "2. Randomly sample other words in the lexicon to get negative examples\n",
        "3. Use logistic regression to train a classifier to distinguish those two cases\n",
        "4. Use the learned weights as the embeddings\n",
        "\n",
        "\n",
        "\n",
        "## Skip-Gram classifier\n",
        "轉成分類問題：\n",
        "\n",
        "- 假定目標詞是杏仁，語境則操作化成 $\\pm 2$ 個詞，例如 *(杏仁,果醬)*, *(杏仁,食蟻獸)*, ... 等 $(w,c)$ pairs。我們要訓練一個分類器來判斷 $(w,c)$ 是正例還是負例。(或者說 assigns each pair a probability: $P(+|w,c)$)\n",
        "\n",
        "- 怎麼算 *P* 呢？用 embedding similarity 來計算：越接近目標詞的 embeddings 越有可能是正例。\n",
        "\n",
        "- 用內積來計算 similarity $(c \\cdot w)$\n",
        "\n",
        "- 用 sigmoid 函數來將內積轉成機率 $P(+|w,c) = \\sigma(c \\cdot w) = \\frac{1}{1+exp(-c,w)}$\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "- 當訓練完成之後，Embedding 的向量即是從此網絡當中的隱藏層（Hidden Layer）所取出的權重（Weight）。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# GPT X 學到了詞彙語意知識了嗎?\n"
      ],
      "id": "19365b3c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}