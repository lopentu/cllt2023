[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ë®àÁÆóË™ûË®ÄÂ≠∏ËàáË™ûË®ÄÂ≠∏ÁêÜË´ñ",
    "section": "",
    "text": "ÈÄ±Ê¨°\nÊó•Êúü\n‰∏ªÈ°å\nÈ†êÁøí üìñ\nÊäïÂΩ±Áâá\nÂä©ÊïôË™≤\nË™≤Â†ÇÁ∑¥Áøí\n‰ΩúÊ•≠\n\n\n\n\n1\n02/23\nË™≤Á®ãÂ∞éË´ñ\n\norientation\n\n\n\n\n\n2\n03/02\nË®àÁÆóË™ûË®ÄÂ≠∏Â∞éË´ñ\n\nw2\nslides\ncolab\nHW1, HW1_Ans\n\n\n3\n03/09\nË®àÁÆóÊßãË©ûËàáË©ûÂΩô (1)\nslp3-02\nw3\nslides\ncolab\nHW2, HW2_Ans\n\n\n4\n03/16\nË®àÁÆóÊßãË©ûËàáË©ûÂΩô (2)\nslp3-23\nw4\n\ncolab\nHW3, HW3_Ans\n\n\n5\n03/23\nË®àÁÆóÊßãË©ûËàáË©ûÂΩô (3)\nslp3-21\nw5\nguidline\ncolab, gsheet\nHW4\n\n\n6\n03/30\nË®àÁÆóÊßãË©ûËàáË©ûÂΩô (4)\nslp3-25\nw6\n\ncolab\nHW5\n\n\n7\n04/06\nÂè•Ê≥ïÁêÜË´ñËàáÂâñÊûêÂô®\nslp3-18; (Linzen and Baroni, 2021)\n\n\n\n\n\n\n8\n04/13\nË™çÁü•Ë™ûÊ≥ïËàáË®àÁÆó\ncognitive linguistics meets computational linguistics\n\n\n\n\n\n\n9\n04/20\nË™ûÊÑèË°®Âæµ (1)\nslp3-06\n\n\n\n\n\n\n10\n04/27\nË™ûÊÑèË°®Âæµ (2)\nslp3-10\n\n\n\n\n\n\n11\n05/04\nË™ûÊÑèË°®Âæµ (3) & Ê•≠Â∏´ÂàÜ‰∫´\nslp3-11\n\n\n\n\n\n\n12\n05/11\nË™ûÊÑèË°®Âæµ (4)\nslp3-22\n\n\n\n\n\n\n13\n05/18\nBakeoff practice\n\n\n\n\n\n\n\n14\n05/25\nË™ûÈü≥„ÄÅÂ§öÊ®°ÊÖãËàáË®ÄË´áÂàÜÊûê\nslp3-15; slp3-28\n\n\n\n\n\n\n15\n06/01\nÁ∂úÂêàË®éË´ñ\n\n\n\n\n\n\n\n16\n06/08\nÊúüÊú´Â†±Âëä"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you‚Äôre welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that‚Äôs fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called ‚ÄúReferences‚Äù at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called ‚ÄúAppendix‚Äù.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you‚Äôre using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n‚ùå NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n‚úÖ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %>%\n  count(manufacturer) %>%\n  mutate(manufacturer = str_to_title(manufacturer)) %>%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel <- lm(mpg ~ hp, data = mtcars)\ntidy(model) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon‚Äôt use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n‚ùå There is a negative linear relationship between mpg and hp.\n‚úÖ There is a negative linear relationship between a car‚Äôs fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don‚Äôt assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the ‚Äúso what‚Äù: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e.¬†what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n‚ùå For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n‚úÖ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it‚Äôs from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation¬†1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation¬†2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation¬†1 and Equation¬†2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e.¬†\\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation¬†6 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation¬†5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{7}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{8}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e.¬†\\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation¬†8 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{9}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{10}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{11}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook‚Äôs distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation¬†1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation¬†2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation¬†1 and Equation¬†2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation¬†4 and Equation¬†5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation¬†6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e.¬†outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation¬†4 using Equation¬†7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e.¬†\\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation¬†8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e.¬†\\(E(e_i) = 0\\). From Equation¬†9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook‚Äôs distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook‚Äôs Distance can be calculated without deleting observations one at a time, since Equation¬†12 below is mathematically equivalent to Equation¬†11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation¬†1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation¬†1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation¬†2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation¬†3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation¬†4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that‚Äôs more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation¬†5 and Equation¬†6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation¬†7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation¬†1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation¬†2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation¬†2 for interpretations and predictions, we will use Equation¬†3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation¬†2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.¬†\\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation¬†5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.¬†\\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.¬†Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike‚Äôs Information Criterion (AIC) and Schwarz‚Äôs Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation¬†1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation¬†1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation¬†2, i.e.¬†maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e.¬†the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation¬†4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike‚Äôs Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let‚Äôs focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export‚Ä¶ If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you‚Äôve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you‚Äôll upload your PDF and them mark the page(s) where each question can be found. It‚Äôs OK if a question spans multiple pages, just mark them all. It‚Äôs also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I‚Äôd rather you didn‚Äôt, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we‚Äôre using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you‚Äôre working in the containers we have provided for you. If you‚Äôre working on your local setup, we can‚Äôt guarantee being able to resolve your issues, though we‚Äôre happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I‚Äôd like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can‚Äôt wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr.¬†Mine √áetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that‚Äôs not appropriate for the public forum, you are welcome to email me directly. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\nüçÄ Good luck! üçÄ\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for ‚ÄúInstructors in this site‚Äù only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the ‚ÄúWorkflow & formatting‚Äù section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "weeks/week5.html",
    "href": "weeks/week5.html",
    "title": "Week 5 Word Meaning (II)",
    "section": "",
    "text": "Ë©ûÂΩôÈóú‰øÇ„ÄÅË©ûÂΩôÁ∂≤Ë∑Ø„ÄÅÁü•Ë≠òÊú¨È´î\n‰∫ã‰ª∂Ë™ûÊÑèËàáË®àÁÆóË°®Âæµ"
  },
  {
    "objectID": "weeks/week5.html#in-class-exercise.1",
    "href": "weeks/week5.html#in-class-exercise.1",
    "title": "Week 5 Word Meaning (II)",
    "section": "In-class exercise.1",
    "text": "In-class exercise.1\nCreate a tiny knowledge graph based on the news article in Chapter 21 of the textbook. ÂèÉËÄÉ"
  },
  {
    "objectID": "weeks/week5.html#relations-in-named-entity-tasks",
    "href": "weeks/week5.html#relations-in-named-entity-tasks",
    "title": "Week 5 Word Meaning (II)",
    "section": "Relations in Named Entity Tasks",
    "text": "Relations in Named Entity Tasks\n\nNamed entity recognition (NER) is the task of identifying and classifying named entities in text into pre-defined categories such as person names, organization names, locations, etc."
  },
  {
    "objectID": "weeks/week5.html#regular-polysemy-detecection",
    "href": "weeks/week5.html#regular-polysemy-detecection",
    "title": "Week 5 Word Meaning (II)",
    "section": "Regular Polysemy Detecection",
    "text": "Regular Polysemy Detecection\nSemEval 2010-Task 8"
  },
  {
    "objectID": "weeks/week5.html#qualia-structure",
    "href": "weeks/week5.html#qualia-structure",
    "title": "Week 5 Word Meaning (II)",
    "section": "Qualia structure",
    "text": "Qualia structure\n\n\n\nPustejovsky"
  },
  {
    "objectID": "weeks/week5.html#rdf-resource-description-framework",
    "href": "weeks/week5.html#rdf-resource-description-framework",
    "title": "Week 5 Word Meaning (II)",
    "section": "RDF (Resource Description Framework)",
    "text": "RDF (Resource Description Framework)\n\nRDF is a standard metalanguage (W3 recommendation) for data interchange on the Web.\nRDF triples are the basic unit of data in RDF, a triple consists of < subject, predicate, object >\nRDF triples are used to describe resources (e.g., people, places, things, etc.) and their properties (e.g., name, age, etc.).\nDBpedia and Wikidata are two popular knowledge bases that use RDF triples to represent knowledge."
  },
  {
    "objectID": "weeks/week5.html#relation-extraction-algorithms",
    "href": "weeks/week5.html#relation-extraction-algorithms",
    "title": "Week 5 Word Meaning (II)",
    "section": "Relation Extraction Algorithms",
    "text": "Relation Extraction Algorithms\nFive main classes of relation extraction algorithms:\n\nPattern-based (Hearst patterns)\nFeature-based supervised relation classifier\nNeural supervised relation classifiers\nSemi-/un- supervised"
  },
  {
    "objectID": "weeks/week5.html#spanbert",
    "href": "weeks/week5.html#spanbert",
    "title": "Week 5 Word Meaning (II)",
    "section": "SpanBERT",
    "text": "SpanBERT\n(Joshi et al. 2020) exceeds BERT by 3.3% F1."
  },
  {
    "objectID": "weeks/week5.html#semisupervised-relation-extraction",
    "href": "weeks/week5.html#semisupervised-relation-extraction",
    "title": "Week 5 Word Meaning (II)",
    "section": "Semisupervised Relation Extraction",
    "text": "Semisupervised Relation Extraction\nbootstrapping\n\nBootstrapping is a semi-supervised learning method that uses a small amount of labeled data to train a classifier, and then uses the classifier to label a large amount of unlabeled data."
  },
  {
    "objectID": "weeks/week5.html#semisupervised-relation-extraction-1",
    "href": "weeks/week5.html#semisupervised-relation-extraction-1",
    "title": "Week 5 Word Meaning (II)",
    "section": "Semisupervised Relation Extraction",
    "text": "Semisupervised Relation Extraction\nDistant Supervision\n\nDistant supervision is a semi-supervised learning method that uses a large amount of unlabeled data and a small amount of labeled data to train a classifier."
  },
  {
    "objectID": "weeks/week5.html#evaluation-metrics",
    "href": "weeks/week5.html#evaluation-metrics",
    "title": "Week 5 Word Meaning (II)",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics"
  },
  {
    "objectID": "weeks/week5.html#ÂØ¶ÂãôÈÅãÁî®‰∏äÁöÑÁ∑¥Áøí",
    "href": "weeks/week5.html#ÂØ¶ÂãôÈÅãÁî®‰∏äÁöÑÁ∑¥Áøí",
    "title": "Week 5 Word Meaning (II)",
    "section": "ÂØ¶ÂãôÈÅãÁî®‰∏äÁöÑÁ∑¥Áøí",
    "text": "ÂØ¶ÂãôÈÅãÁî®‰∏äÁöÑÁ∑¥Áøí\n\n\n\n\n\n\nTip\n\n\n\nExtract money and currency values (entities labelled as MONEY) and find the noun phrase they are referring to - for example: ‚ÄúNet income was $9.4 million compared to the prior year of $2.7 million.‚Äù\n$9.4 million ‚Üí Net income.\n$2.7 million ‚Üí the prior year"
  },
  {
    "objectID": "weeks/week5.html#‰∏ÄËà¨Ëß£Ê≥ï",
    "href": "weeks/week5.html#‰∏ÄËà¨Ëß£Ê≥ï",
    "title": "Week 5 Word Meaning (II)",
    "section": "‰∏ÄËà¨Ëß£Ê≥ï",
    "text": "‰∏ÄËà¨Ëß£Ê≥ï\n\nStep 1: use spaCy‚Äôs named entity recognizer to extract money and currency values (entities labelled as MONEY)\nStep2: use spaCy‚Äôs dependency parser to find the noun phrase they are referring to."
  },
  {
    "objectID": "weeks/week5.html#gpt4-Ëß£Ê≥ï",
    "href": "weeks/week5.html#gpt4-Ëß£Ê≥ï",
    "title": "Week 5 Word Meaning (II)",
    "section": "GPT4 Ëß£Ê≥ï",
    "text": "GPT4 Ëß£Ê≥ï"
  },
  {
    "objectID": "weeks/week5.html#ÂØ¶ÂãôÁ∑¥Áøí‰∫å",
    "href": "weeks/week5.html#ÂØ¶ÂãôÁ∑¥Áøí‰∫å",
    "title": "Week 5 Word Meaning (II)",
    "section": "ÂØ¶ÂãôÁ∑¥Áøí‰∫å",
    "text": "ÂØ¶ÂãôÁ∑¥Áøí‰∫å\n\nÁî® wordnet ÂÅöÁü•Ë≠òÂúñË≠úÔºåweek5.ipnb\nË®àÁÆóË©ûÂΩôÁöÑË©ûÊÑèÈ†ªÁéá"
  },
  {
    "objectID": "weeks/week5.html#event-types-aktionsart-lexical-aspect",
    "href": "weeks/week5.html#event-types-aktionsart-lexical-aspect",
    "title": "Week 5 Word Meaning (II)",
    "section": "Event types (Aktionsart; Lexical Aspect)",
    "text": "Event types (Aktionsart; Lexical Aspect)\n\n\nstate, activitis, accomplishments, achievements, semelfactives\n\n\n\n\n\n\n\nTip\n\n\n\nAktionsart is a German term that refers to the type of action that a verb expresses."
  },
  {
    "objectID": "weeks/week5.html#event-representation",
    "href": "weeks/week5.html#event-representation",
    "title": "Week 5 Word Meaning (II)",
    "section": "Event Representation",
    "text": "Event Representation\n\nEvent representation is the task of representing events in a structured format."
  },
  {
    "objectID": "weeks/week5.html#event-extraction",
    "href": "weeks/week5.html#event-extraction",
    "title": "Week 5 Word Meaning (II)",
    "section": "Event Extraction",
    "text": "Event Extraction\n\nEvent extraction is the task of identifying mentions of events and classifying them in text.\nan event mention is a span of text (expressions) that refers to an event that can be assigned to a particular point or interval in time."
  },
  {
    "objectID": "weeks/week5.html#chinese-framenet",
    "href": "weeks/week5.html#chinese-framenet",
    "title": "Week 5 Word Meaning (II)",
    "section": "Chinese FrameNet",
    "text": "Chinese FrameNet\n\n(‰∏≠Âúã) Chinese FrameNet project (CFN) by the State Key Laboratory of Intelligent Technology and Systems at Tsinghua University in Beijing. Â§±ÊïàÁöÑË®àÂäÉÈÄ£Êé•\n(Âè∞ÁÅ£) NTU ÂçäËá™ÂãïÁîüÊàêÁâà\n(È¶ôÊ∏Ø) ‰∏≠Êñá VerbNet"
  },
  {
    "objectID": "weeks/week5.html#‰ª•-caused-motion-ÁÇ∫‰æã",
    "href": "weeks/week5.html#‰ª•-caused-motion-ÁÇ∫‰æã",
    "title": "Week 5 Word Meaning (II)",
    "section": "‰ª• CAUSED-MOTION ÁÇ∫‰æã",
    "text": "‰ª• CAUSED-MOTION ÁÇ∫‰æã"
  },
  {
    "objectID": "weeks/week5.html#frames-constructions-and-framenet",
    "href": "weeks/week5.html#frames-constructions-and-framenet",
    "title": "Week 5 Word Meaning (II)",
    "section": "Frames, constructions, and FrameNet",
    "text": "Frames, constructions, and FrameNet\nCharles J. Fillmore ÈÅé‰∏ñÂâçÁöÑÁ∂ìÂÖ∏Ë´ñÊñá"
  },
  {
    "objectID": "weeks/week5.html#Âª∂‰º∏Âá∫‰æÜÁöÑË™çÁü•Ë™ûË®ÄÁ†îÁ©∂",
    "href": "weeks/week5.html#Âª∂‰º∏Âá∫‰æÜÁöÑË™çÁü•Ë™ûË®ÄÁ†îÁ©∂",
    "title": "Week 5 Word Meaning (II)",
    "section": "Âª∂‰º∏Âá∫‰æÜÁöÑË™çÁü•Ë™ûË®ÄÁ†îÁ©∂",
    "text": "Âª∂‰º∏Âá∫‰æÜÁöÑË™çÁü•Ë™ûË®ÄÁ†îÁ©∂\nË¶ñËßí (perspective) ÊÄéÈ∫ºÂèçÊáâÂú®Ë™ûË®Ä‰∏äÔºü\n\n\n\ncommercial_event frame"
  },
  {
    "objectID": "weeks/week-1.html#Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ",
    "href": "weeks/week-1.html#Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ",
    "title": "Week1 Orientation",
    "section": "Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ",
    "text": "Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ\n\nÁ†îÁ©∂ÊÖ£‰æã‰∏äÔºåÂ∞áË™ûË®ÄË≥áË®äÁöÑÂïèÈ°åÂàáÂàÜÊàê‰∏çÂêåÁöÑÂ≠ê‰ªªÂãôÔºåÊàêÁÜü‰πãÂæåÂ∞±ËÆäÊàêÂ§ßÂÆ∂ËºÉÁÜüÊÇâÁöÑÊáâÁî®„ÄÇ\n\nË™ûÈü≥Ëæ®Ë≠òËàáË™ûÈü≥ÂêàÊàê\nÊñáÊú¨ÊëòË¶Å\nÊ©üÂô®ÁøªË≠Ø\nËá™ÂãïÂïèÁ≠î\n„ÄÇ„ÄÇ„ÄÇ„ÄÇ\n\n\n\n\nÊõæÁ∂ìË™ûÈü≥ËàáÂ§öÊ®°ÊÖã(multimodality)ÊòØÊØîËºÉ‰∏çÊ≠∏È°ûÂú® NLP ÁöÑÈ†òÂüüÔºå‰ΩÜÊúÄËøëÂπæÂπ¥ÁöÑÈÄ≤Â±ïËàáË∂®Âã¢Ôºå‰πüËàá NLP ÁöÑÁ†îÁ©∂Á§æÁæ§ÂØÜÂàá‰∫íÂãï„ÄÇ"
  },
  {
    "objectID": "weeks/week-1.html#Ë®àÁÆóË™ûË®ÄÂ≠∏-1",
    "href": "weeks/week-1.html#Ë®àÁÆóË™ûË®ÄÂ≠∏-1",
    "title": "Week1 Orientation",
    "section": "Ë®àÁÆóË™ûË®ÄÂ≠∏",
    "text": "Ë®àÁÆóË™ûË®ÄÂ≠∏\n\n\nÊº¢Ë™ûÁöÑÈõôÈü≥ÁØÄÂåñÊòØÊÄéÈ∫ºÊºîÂåñÁöÑ Ôºàcomputational historical linguisticsÔºâ\nÊñ∞Ë©û (neologism)„ÄÅÊñ∞ÊßãÂºè (constructions) ÊòØÊÄéÈ∫ºÂú®Á§æÁæ§‰∏≠Á™ÅÁèæ„ÄÅÂÇ≥ÈÅû„ÄÅÊ∂à‰∫°ÁöÑ Ôºàcomputational sociolinguisticsÔºâ\nÈù¢Â∞çÂÆåÂÖ®Ê≤íÊúâÊé•Ëß∏ÈÅéÁöÑË™ûË®ÄÔºåÂ¶Ç‰ΩïÁêÜËß£ËàáÁ∑®ÂØ´Ë™ûÊ≥ïÁ≥ªÁµ±ÔºüÔºàcomputational linguistic typology/ xenolinguisticsÔºâ\n‚Ä¶‚Ä¶. (‰Ω†ÁöÑÊÉ≥Ê≥ïÔºü)"
  },
  {
    "objectID": "weeks/week-1.html#ÂæûË™ûË®ÄÁöÑËôïÁêÜÂà∞ÁêÜËß£",
    "href": "weeks/week-1.html#ÂæûË™ûË®ÄÁöÑËôïÁêÜÂà∞ÁêÜËß£",
    "title": "Week1 Orientation",
    "section": "ÂæûË™ûË®ÄÁöÑËôïÁêÜÂà∞ÁêÜËß£",
    "text": "ÂæûË™ûË®ÄÁöÑËôïÁêÜÂà∞ÁêÜËß£\nFrom Natural Language Processing to National Language Understanding\n\nÂ∞çÊñºËôïÁêÜËàáÁêÜËß£ÔºåÈÉΩÊòØÊìç‰ΩúÂûãÂÆöÁæ© (operationalized)„ÄÇ"
  },
  {
    "objectID": "weeks/week-1.html#ÂÇ≥Áµ±Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÁôºÂ±ï",
    "href": "weeks/week-1.html#ÂÇ≥Áµ±Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÁôºÂ±ï",
    "title": "Week1 Orientation",
    "section": "ÂÇ≥Áµ±Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÁôºÂ±ï",
    "text": "ÂÇ≥Áµ±Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÁôºÂ±ï\n\n\n\n‰∏≠Êñá AI Áü•Ë≠òÂ∫´ËàáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊ†∏ÂøÉÂ•ó‰ª∂Á†îË®éÊúÉ"
  },
  {
    "objectID": "weeks/week-1.html#ÂÄã‰∫∫ËßÄÈªû",
    "href": "weeks/week-1.html#ÂÄã‰∫∫ËßÄÈªû",
    "title": "Week1 Orientation",
    "section": "(ÂÄã‰∫∫ËßÄÈªû)",
    "text": "(ÂÄã‰∫∫ËßÄÈªû)\n\n\n(ÂΩ¢Âºè)Ë™ûË®ÄÂ≠∏ËàáÂÇ≥Áµ±Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁöÑ Modularity È†êË®≠Ôºå ÊòØÂ•ΩÁî®ÁöÑËôõÊßã (useful fiction, Haugen and Dil, 1972)ÔºåÊñπ‰æøÊï¥ÁêÜÁü•Ë≠òÔºå‰ΩÜÈùûÁúüÂØ¶ÁöÑË™ûË®ÄÁèæË±°„ÄÇ\nË™ûË®ÄÔºö‰∫∫È°ûÔºåÁà≤‰∫ÜÊ∫ùÈÄöÁõÆÁöÑÔºåÂú®ÊôÇÁ©∫ËÑàÁµ°‰∏ãÔºåÁôºÂ±ïÂá∫ÁöÑÂΩ¢ÊÑèÁ¨¶ËôüÁ≥ªÁµ±„ÄÇ (cultural Species; form-meaning pairing; symbolic; self-adaptive complex system; situated and embodied cognition; etc.)\nÁ¨¶Á¢ºÊú¨Ë≥™ÊòØÂΩ¢ÊÑèÊò†Â∞ÑÔºåËÄåÊåáÊ∂âÊÑèÁæ©ÊòØÁ¥ÑÂÆö‰øóÊàê„ÄÅÂæàÂêÉÊÉÖÂ¢ÉÁöÑ„ÄÇÂõ†Ê≠§Á¨¶Á¢ºÈÅãÁÆóËàáË°®ÂæµÊòØÁµ¶‰∫∫ÁúãÁöÑ„ÄÇË™ûË®ÄÁöÑÁêÜËß£ÔºåÊ∂âÂèäÂà∞Êõ¥Ë§áÈõúÁöÑÈÄ£Á∫åÁöÑÈÅéÁ®ãÔºåËÄåÈùû‰∏ÄÂÄãÂàÜÊÆµ„ÄÅÂàÜËß£ÁöÑÈÅéÁ®ã„ÄÇ\nË™ûË®ÄÂ≠∏‰∏çÁ≠âÊñº rule-based„ÄÇ"
  },
  {
    "objectID": "weeks/week-1.html#Â§ßÂûãÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã",
    "href": "weeks/week-1.html#Â§ßÂûãÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã",
    "title": "Week1 Orientation",
    "section": "Â§ßÂûãÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã",
    "text": "Â§ßÂûãÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã\npre-trained Large Language Models (pre-LLMs)\n\nÊîπËÆä‰∫Ü(ÁõÆÂâç)ÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ†îÁ©∂ÊñπÂêë"
  },
  {
    "objectID": "weeks/week-1.html#gpt-3-and-beyond",
    "href": "weeks/week-1.html#gpt-3-and-beyond",
    "title": "Week1 Orientation",
    "section": "GPT-3 and Beyond",
    "text": "GPT-3 and Beyond\nÂæàÊ∏ÖÊ•öÁöÑÁßëÊôÆ‰ªãÁ¥π"
  },
  {
    "objectID": "weeks/week-1.html#post-ai-nlp-in-context-learning-prompt-training",
    "href": "weeks/week-1.html#post-ai-nlp-in-context-learning-prompt-training",
    "title": "Week1 Orientation",
    "section": "post-AI NLP: in-context learning (prompt-training)",
    "text": "post-AI NLP: in-context learning (prompt-training)\na new paradigm of NLP that is based on the recent advances in AI, especially in the field of language modeling.\n\nRecently, we have seen dramatic advances in natural language processing (NLP) driven by huge pre-trained language models such as GPT-3 and DALLE-2. Instead of building many small task-specific models, there is a movement to create and use these more all-purpose huge language models for many NLP applications.\n\nThe most intriguing finding is that these models employ a new learning paradigm:¬†in-context learning, where they learn to do a downstream task simply by conditioning on a prompt consisting of a few input-output examples without any parameter updates"
  },
  {
    "objectID": "weeks/week-1.html#Â¶Ç‰ΩïÂΩ±ÈüøÊïôÂ≠∏",
    "href": "weeks/week-1.html#Â¶Ç‰ΩïÂΩ±ÈüøÊïôÂ≠∏",
    "title": "Week1 Orientation",
    "section": "Â¶Ç‰ΩïÂΩ±ÈüøÊïôÂ≠∏",
    "text": "Â¶Ç‰ΩïÂΩ±ÈüøÊïôÂ≠∏\n(È†Ü‰æø‰ªãÁ¥πÂä©ÊïôÁæ§ üòÑ)"
  },
  {
    "objectID": "weeks/week-1.html#Áà≤‰ªÄÈ∫ºÊàëÂÄëÂèØ‰ª•ÊîæÂøÉÊàñÊáâË©≤ÊìîÂøÉ",
    "href": "weeks/week-1.html#Áà≤‰ªÄÈ∫ºÊàëÂÄëÂèØ‰ª•ÊîæÂøÉÊàñÊáâË©≤ÊìîÂøÉ",
    "title": "Week1 Orientation",
    "section": "Áà≤‰ªÄÈ∫ºÊàëÂÄëÂèØ‰ª•ÊîæÂøÉÔºàÊàñÊáâË©≤ÊìîÂøÉÔºâÔºü",
    "text": "Áà≤‰ªÄÈ∫ºÊàëÂÄëÂèØ‰ª•ÊîæÂøÉÔºàÊàñÊáâË©≤ÊìîÂøÉÔºâÔºü\naka. ÁÑ°‰∏≠ÁîüÊúâÔºåË∑üÁúüÁöÑ‰∏ÄÊ®£"
  },
  {
    "objectID": "weeks/week-1.html#ÊàëÂÄëÊÄéÈ∫ºÂØ¶‰ΩúÈÄôÈñÄË™≤",
    "href": "weeks/week-1.html#ÊàëÂÄëÊÄéÈ∫ºÂØ¶‰ΩúÈÄôÈñÄË™≤",
    "title": "Week1 Orientation",
    "section": "ÊàëÂÄëÊÄéÈ∫ºÂØ¶‰ΩúÈÄôÈñÄË™≤Ôºü",
    "text": "ÊàëÂÄëÊÄéÈ∫ºÂØ¶‰ΩúÈÄôÈñÄË™≤Ôºü\n\nÊàëÂÄëÁöÑ‰∏ªË¶ÅÂ≠∏ÁøíÈáçÈªû‰∏çÊúÉÊîæÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂéüÁêÜ (Ë´ãÂ§ßÂÆ∂Âú®ÂÖ∂‰ªñÁöÑË™≤Á®ã‰∏≠‰∏¶Ë°åÂ≠∏Áøí)ÔºåËÄåÊòØÊîæÂú®Ë™ûË®ÄÂ≠∏ÁêÜË´ñËàáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈóú‰øÇ„ÄÇ\nÈÅ∏ÊìáÊúâË™ûË®ÄÂ≠∏ËÉåÊôØÁöÑÊïôÊùê (Â¶Ç Jurafsky & Martin SLP3, Christopher Potts cs224u, etc.) ‰æÜÈÄ≤Ë°åË™≤Á®ãÁöÑÊïôÂ≠∏Ôºå‰∏¶Ë£úÂÖÖ‰∏çÂêåÁöÑËßÄÈªû„ÄÇ\n\n\n\n\n\n\n\nWarning\n\n\n\nÈÄôÊòØÊ≤íÊúâÊïôÁßëÊõ∏ÁöÑÂ≠∏ÈñÄ\n\n\n\nÂØ¶‰ΩúÈÉ®ÂàÜÂÅ¥ÈáçÊáâÁî®„ÄÇ"
  },
  {
    "objectID": "weeks/week-1.html#ÁúãÁúãÁï∂ÂâçÁöÑÁ†îÁ©∂ÊïôÂ≠∏",
    "href": "weeks/week-1.html#ÁúãÁúãÁï∂ÂâçÁöÑÁ†îÁ©∂ÊïôÂ≠∏",
    "title": "Week1 Orientation",
    "section": "ÁúãÁúãÁï∂ÂâçÁöÑÁ†îÁ©∂ÊïôÂ≠∏",
    "text": "ÁúãÁúãÁï∂ÂâçÁöÑÁ†îÁ©∂ÊïôÂ≠∏\nIntroduction and course overview"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2 Introduction",
    "section": "",
    "text": "ÂØ¶ÂãôÊáâÁî®‰∏äÊúÉ‰ΩøÁî® SOTA ÁöÑÂ∑•ÂÖ∑„ÄÇÁßëÂ≠∏Êé¢Á¥¢‰∏äË¶ÅÂ≠∏ÁøíËÉåÂæåÁöÑÁêÜË´ñÊ≠∑Âè≤„ÄÇ\n\n\n\n\nËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåË®àÁÆóË™ûË®ÄÂ≠∏ÁöÑÁï∞Âêå\n‰ΩÜÂÖ©ËÄÖÁöÑÂü∫Á§éÈ°û‰ºº (Ê©üÂô®Â≠∏Áøí„ÄÅÁ®ãÂºèËÉΩÂäõ„ÄÅÊºîÁÆóÊ≥ï)\n\nÂâçËÄÖÁõÆÂâçÊõ¥ÂÅ¥ÈáçË™ûË®ÄÊ®°ÂûãÁöÑÂ∑•Á®ãË®ìÁ∑¥\nÂæåËÄÖÂÅèÂêëÈÇèËºØÊé®ÁêÜËàáÊ®°ÁµÑËß£ÊûêÁöÑË™ûË®ÄÁêÜË´ñÁ†îÁ©∂\n\n\n\n\n\n\nNLP Á∞°Âè≤\nË™ûË®ÄÁêÜË´ñËàáÊ∑±Â∫¶Â≠∏Áøí\nÂÖ•ÈñÄÂØ¶Áøí\n\n\n\n\n\n\nimage source\n\n\n\n\n\n\nÂú®Â§ßÊï∏ÊìöÂ§ßÊºîÁÆóÂäõÁöÑÊôÇ‰ª£Ôºåauto-regressive LLMs (‰ª• chatGPT (Brown et al. 2020) Áà≤‰ª£Ë°®) ÊîπËÆä‰∫ÜÁï∂Ââç NLP ÁîüÊÖãÁöÑÁôºÂ±ïÊñπÂêë„ÄÇ \n\n\n\n\nNLP Process\n\n\n\n\nNLP Process with a Language Model\n\n\nimage source\n\n\n\n\n\nLearning from a few examples in context: Â∞á \\(X\\) ‰ª•ÂèäÂèØËÉΩÊîØÊåÅÂÖ∂È†êÊ∏¨ \\(Y\\) ÁöÑ‰∏Ä‰∫õ contextÔºàÂèØËÉΩÊòØÂÖ∂‰ªñÊ®ôË®òÊï∏ÊìöÔºåËÉåÊôØÁü•Ë≠òÔºåÁõ∏ÈóúÊñáÊ™îÁ≠âÔºâ‰∏ÄËµ∑Ëº∏ÂÖ•Ê®°Âûã‰∏≠ÔºåÂØ¶ÁèæÂ∞ç \\(Y\\) ÁöÑÈ†êÊ∏¨„ÄÇ\nÁï∂‰∏ãÂ≠∏ÁøíÔºå‰∏çÈáçÊñ∞Ë™øÂèÉÊï∏„ÄÇDifferent from supervised learning requiring a training stage that uses backward gradients to update model parameters, ICL does not conduct parameter updates and directly performs predictions on the pretrained language models.\n\n\n\n\n\n\n\\(A:B=C:?\\)\n\n\n\n\n\n\nImportant\n\n\n\n\nÊúÉÁî®Ëá™ÁÑ∂Ë™ûË®ÄÂØ´ prompting templates„ÄÅÊéíÂ∫èËàáÊåëÈÅ∏ËÑàÁµ°ÔºåËÆäÊàê‰∏ÄÁ®ÆËß£ÊûêÊäÄËÉΩ„ÄÇ(Zhou et al. 2022)\n\n\n\n\n\n\nÈÇèËºØÊé®ÁêÜÊòØÊ©üÂô®ÂèØÂ≠∏ÁøíÁöÑÂóéÔºü\n\na chain of thought (i.e., a series of intermediate reasoning steps) significantly improves the ability of large language models to perform complex reasoning.(Wei et al. 2022)\nÁúãÁúãËß£Ë™ûË®ÄË¨éÈ°åÁöÑÁµêÊûú Translation Puzzles are In‚Äëcontext Learning Tasks\n\n\n\n\n\n\n\n\nË™ûË®ÄÊ®°ÂûãÂæûÂ§ßÈáèË™ûÊñô‰∏≠Â≠∏ÁøíË™ûË©ûÁöÑÊ©üÁéáÂàÜ‰Ωà (e.g., how likely a word or sequence can occur in the context).\nTransformer-based language models„ÄÇGPT-3 Âíå BERT ÊòØÂÖ∂‰∏≠ÂÖ∑‰ª£Ë°®ÊÄßÁöÑ„ÄÇ\nÁõÆÂâçÁÅ´Á¥ÖÁöÑ chatGPT (Brown et al. 2020) (GPT-3 variants) ÈÇÑÂà©Áî®‰∫∫Â∑•ÂèçÈ•ã‰πãÂº∑ÂåñÂ≠∏ÁøíÂæÆË™ø pre-trained on a huge corpus of web and book data and fine-tuned with human conversation examples through supervised learning and reinforcement learning from human feedback (RLHF).\n\n\n\n\n\n\n\nÊòØ‰∏ÄÁ®Æ encoder-decoder Á•ûÁ∂ìÁ∂≤Ë∑ØÈ°ûÂûãÔºå\n\n\n\n\n\nTransformer ‰ΩøÁî®ÁöÑÊ≥®ÊÑèÂäõË®àÁÆóÊ©üÂà∂ (attention)ÔºåÂèØ‰ª•Áî®‰æÜÂæàÂ•ΩÁöÑËôïÁêÜÂ∫èÂàóÊï∏Êìö (e.g., ÊñáÊú¨„ÄÅË™ûÈü≥„ÄÅÈü≥Ê®Ç)„ÄÇ\nÊ≥®ÊÑèÂäõÊ©üÂà∂ËÆìÊ®°ÂûãÊõ¥ÂéªÊ≥®ÊÑèÂà∞Â∫èÂàóÂÖÉÁ¥†‰πãÈñìÁöÑÁõ∏ÈóúÊÄß„ÄÇ(e.g., He-Tom, apples-them) \n\n\nsource\n\n\n\n\n\nË™ûË®ÄË°®ÈÅîËÉΩÂäõÊµÅÊö¢ (ÂÑòÁÆ°Ë™™ÁöÑÊòØÁúüÊòØÂÅá‰∏çÁ¢∫ÂÆö)\nË≤å‰ººÂèØ‰ª•ÈÄ≤Ë°åË®éË´ñ (‰ΩÜÊòØËÉåÂæåÊé®ÁêÜÊ©üÂà∂‰∏çÈÄèÊòé) making probabilistic predictions in the generation process based on the corpus it has been trained on, which can lead to false claims about facts. Typical hedging behavior of ChatGPT will certainly increase the safety of the system.\n‰∏çÈñãÊ∫êÔºõÈùûÂïÜÊ•≠ÁµÑÁπîÂ¶ÇÂ§ßÂ≠∏ÔºåÈÄöÂ∏∏Áº∫‰πèË∑ü‰∏äË™ûË®ÄÊ®°ÂûãÂø´ÈÄüÁôºÂ±ïÊâÄÈúÄÁöÑË®àÁÆóÂíåË≤°ÂãôË≥áÊ∫ê„ÄÇ\n\n\n\n\n\n\n\n\n\nÁé©‰∏ÄÈ°åË™ûÂ•ß (NACLO, UKLO, Panini)„ÄÇÂàÜÁµÑË®éË´ñ prompt/in-context ÁöÑË®≠Ë®àÔºåÊØîÁúãÁúã ChrF ÁöÑÊïàÊûú„ÄÇ"
  },
  {
    "objectID": "weeks/week-2.html#prellms-Â≠∏Âà∞‰∫Ü‰ªÄÈ∫º",
    "href": "weeks/week-2.html#prellms-Â≠∏Âà∞‰∫Ü‰ªÄÈ∫º",
    "title": "Week 2 Introduction",
    "section": "preLLMs Â≠∏Âà∞‰∫Ü‰ªÄÈ∫ºÔºü",
    "text": "preLLMs Â≠∏Âà∞‰∫Ü‰ªÄÈ∫ºÔºü\n\nÂ≠∏ÁøíÂà∞ÁöÑ‰∏çÂÉÖÂÉÖÊòØÈÑ∞ËøëÂ≠ó‰∏≤Ê©üÁéáÔºåÈÇÑÊúâ‰∏Ä‰∫õÁµêÊßãÊÄßÁöÑÁü•Ë≠ò„ÄÇ ‰ª•Ë®±Â§öË™ûË®ÄÁöÑ long-distance number agreement ÁÇ∫‰æã„ÄÇ(Baroni 2021):p8\nÂ¶ÇÊûúÂÖàÁû≠Ëß£Ë™ûË®ÄÊÄéÈ∫ºË¢´ÊàëÂÄëÂ≠∏ÁøíÂà∞ÁöÑ usage-based theory of language acquisition\n\nÁµêÊßãÊÄßÁöÑÊù±Ë•øÔºåÂú®‰∏ÄÂÆöÁöÑÊï∏ÊìöÂ≠∏Áøí‰πãÂæåÔºåÊúÉ emerged„ÄÇ‰∏çÈúÄË¶ÅÈ†êÂÖàË¶èÂäÉËàáÁµ¶ÂÆö„ÄÇ\n‰ª• Papadimitriou and Jurafsky EMNLP 2020 Áà≤‰æã„ÄÇ\n\nÈÄôÂ∞±ÊòØÁà≤‰ΩïÂΩ¢ÂºèË™ûË®ÄÂ≠∏ËàáË™çÁü•ÂäüËÉΩË™ûË®ÄÂ≠∏ÁöÑÈúáÈ©öÁ®ãÂ∫¶‰∏çÂêå üòÜ"
  },
  {
    "objectID": "weeks/week-2.html#Ë™ûË®ÄÂ≠∏ËàáÊ©üÂô®Â≠∏ÁøíÁöÑÈóú‰øÇ-imo",
    "href": "weeks/week-2.html#Ë™ûË®ÄÂ≠∏ËàáÊ©üÂô®Â≠∏ÁøíÁöÑÈóú‰øÇ-imo",
    "title": "Week 2 Introduction",
    "section": "Ë™ûË®ÄÂ≠∏ËàáÊ©üÂô®Â≠∏ÁøíÁöÑÈóú‰øÇ (IMO)",
    "text": "Ë™ûË®ÄÂ≠∏ËàáÊ©üÂô®Â≠∏ÁøíÁöÑÈóú‰øÇ (IMO)\n\nË¶èÂâáÂ≠∏Áøí | ÂΩ¢ÂºèË™ûË®ÄÂ≠∏ (formal syntax)\nÁâπÂæµÂ≠∏Áøí | Áµ±Ë®àË™ûË®ÄÂ≠∏ (statistical linguistics)\nÊ∑±Â∫¶Â≠∏Áøí | Ë™çÁü•ËàáÂäüËÉΩË™ûË®ÄÂ≠∏ (cognitive/functional linguistics)"
  },
  {
    "objectID": "weeks/week-2.html#ÁêÜË´ñË™ûË®ÄÂ≠∏-theorectical-linguistics",
    "href": "weeks/week-2.html#ÁêÜË´ñË™ûË®ÄÂ≠∏-theorectical-linguistics",
    "title": "Week 2 Introduction",
    "section": "ÁêÜË´ñË™ûË®ÄÂ≠∏ | Theorectical Linguistics",
    "text": "ÁêÜË´ñË™ûË®ÄÂ≠∏ | Theorectical Linguistics\n(mostly known as Formal Syntax, or Generative Grammar)1\n\n\nÁîüÊàêË™ûÊ≥ïÈ†êË®≠Ë™ûË®ÄÊòØÂÄãÁî±ÊúâÈôêÈÅûËø¥Ë¶èÂâáÁµÑÊàêÁöÑÂΩ¢ÂºèÁ≥ªÁµ±Ôºå ÂèØ‰ª•Áî®‰æÜÁîüÊàêÊâÄÊúâÂèØËÉΩÁöÑÂêàÊ≥ïÂè•Â≠ê„ÄÇ\nÂ§ßÈÉ®ÂàÜÊâÄË¨ÇÁöÑË™ûË®ÄÂ≠∏ÁêÜË´ñÔºåÈÉΩÊòØÂú®ÁîüÊàêË™ûÊ≥ïÁöÑÊû∂Êßã‰πã‰∏ãÔºå‰∏ªÈ°åÂåÖÊã¨Ôºö\n\nLearnability\nGeneralization given insufficient data\nCentrality of syntactic structure\n. ."
  },
  {
    "objectID": "weeks/week-2.html#Ë™çÁü•ÂäüËÉΩË™ûË®ÄÂ≠∏-cognitivefunctional-linguistics",
    "href": "weeks/week-2.html#Ë™çÁü•ÂäüËÉΩË™ûË®ÄÂ≠∏-cognitivefunctional-linguistics",
    "title": "Week 2 Introduction",
    "section": "Ë™çÁü•ÂäüËÉΩË™ûË®ÄÂ≠∏ | Cognitive/Functional Linguistics",
    "text": "Ë™çÁü•ÂäüËÉΩË™ûË®ÄÂ≠∏ | Cognitive/Functional Linguistics\n\nÊôöËøëÊâçÂºïÂÖ• NLP ÁöÑËß£Èáã\nLanguage as a complex system"
  },
  {
    "objectID": "weeks/week-2.html#ÈÄô‰∫õÈÉΩË†ªÈÄ≤ÈöéÁöÑ‰ΩÜÊàëÂÄëÊÖ¢ÊÖ¢ÂæûÂπæÂÄãÈáçË¶ÅÁöÑË™ûË®ÄÈù¢ÂêëÈñãÂßã",
    "href": "weeks/week-2.html#ÈÄô‰∫õÈÉΩË†ªÈÄ≤ÈöéÁöÑ‰ΩÜÊàëÂÄëÊÖ¢ÊÖ¢ÂæûÂπæÂÄãÈáçË¶ÅÁöÑË™ûË®ÄÈù¢ÂêëÈñãÂßã",
    "title": "Week 2 Introduction",
    "section": "ÈÄô‰∫õÈÉΩË†ªÈÄ≤ÈöéÁöÑÔºå‰ΩÜÊàëÂÄëÊÖ¢ÊÖ¢ÂæûÂπæÂÄãÈáçË¶ÅÁöÑË™ûË®ÄÈù¢ÂêëÈñãÂßã„ÄÇ",
    "text": "ÈÄô‰∫õÈÉΩË†ªÈÄ≤ÈöéÁöÑÔºå‰ΩÜÊàëÂÄëÊÖ¢ÊÖ¢ÂæûÂπæÂÄãÈáçË¶ÅÁöÑË™ûË®ÄÈù¢ÂêëÈñãÂßã„ÄÇ\n\nË©ûÂΩô\nÂè•Ê≥ï\nË™ûÊÑè\nË™ûÈü≥ËàáÂ§öÊ®°ÊÖã\n\n\n\n\n\n\n\nWarning\n\n\n\nÊé¢Á©∂ÈÄô‰∫õÁèæË±°ÁöÑË™ûË®ÄÂ≠∏ÁêÜË´ñËÉåÊôØÊòØ‰ªÄÈ∫ºÔºüÂèàÊÄéÈ∫ºÈÄ£Êé•Âà∞ NLP ÁöÑÂ∑•Á®ãÂØ¶Ë∏êÔºü"
  },
  {
    "objectID": "weeks/week-2.html#ÊàëË™çÁà≤-accord-with-lecun-carmack-ii",
    "href": "weeks/week-2.html#ÊàëË™çÁà≤-accord-with-lecun-carmack-ii",
    "title": "Week 2 Introduction",
    "section": "ÊàëË™çÁà≤ (accord with LeCun, Carmack II)",
    "text": "ÊàëË™çÁà≤ (accord with LeCun, Carmack II)\n\nÁõÆÂâçÁöÑ LLM (Generative AI) ‰∏çÂ§™ÊòØ GAI (General AI) ÁöÑÂØ¶Ë∏êÊñπÂêëÔºå‰ΩÜÊòØÂèØ‰ª•Áï∂‰Ωú‰∏ÄÂÄã IA Ë¨πÊÖé‰ΩøÁî®„ÄÇ\nÁõÆÂâç‰æÜË™™ (2023 ‰∏äÂçäÂπ¥)ÔºåËÆì LLM ÁôºÂ±ïÊõ¥Â•Ω‰∏ÄÈªûÔºåÂêÑÂè∏ÂÖ∂ËÅ∑ÊòØÂÄãÊñπÊ≥ï üòÅ\n\ntoolTransformerÔºõ ‰∫ãÂØ¶ËàáÈÇèËºØÁöÑÂ≠∏Áøí perplexity.ai"
  },
  {
    "objectID": "weeks/week-2.html#‰ΩÜÁôºÂ±ïÈÄüÂ∫¶ÁúüÁöÑÂæàÂø´",
    "href": "weeks/week-2.html#‰ΩÜÁôºÂ±ïÈÄüÂ∫¶ÁúüÁöÑÂæàÂø´",
    "title": "Week 2 Introduction",
    "section": "‰ΩÜÁôºÂ±ïÈÄüÂ∫¶ÁúüÁöÑÂæàÂø´",
    "text": "‰ΩÜÁôºÂ±ïÈÄüÂ∫¶ÁúüÁöÑÂæàÂø´\nmultimodal LLM"
  },
  {
    "objectID": "weeks/week-2.html#ÂàÜÂá∫ÂÖ©Á®Æ-nlp",
    "href": "weeks/week-2.html#ÂàÜÂá∫ÂÖ©Á®Æ-nlp",
    "title": "Week 2 Introduction",
    "section": "ÂàÜÂá∫ÂÖ©Á®Æ NLP",
    "text": "ÂàÜÂá∫ÂÖ©Á®Æ NLP\n\n\n[1]ÔºàË®àÁÆóË™ûË®ÄÂ≠∏ÂÇ≥Áµ±ÔºâÂà©Áî®Ë®àÁÆóÊ®°ÂûãËàáÂ∑•ÂÖ∑Ôºå ‰ª•‰∫∫ÊáÇÁöÑÊñπÂºè ÔºàÁ¨¶Á¢ºÈÅãÁÆóÔºâËß£ÊûêËàáÁû≠Ëß£Ë™ûË®Ä (ÂèäÂÖ∂‰ªñÊ®°ÊÖã)„ÄÇ\n[2]Ôºàpost-LLM NLP Â∞éÂêëÔºâ‰ª•ÈõªËÖ¶ÊúÉÂ∞±Â•ΩÔºàÂºµÈáèÈÅãÁÆóÔºâÁöÑÊñπÂºèÔºåÈñãÁôºËàáË™øÊï¥Ê®°ÂûãÁöÑ (È©ö‰∫∫ÁöÑ) ËêΩÂú∞ÊáâÁî®„ÄÇ\n\n\n[1] Êèê‰æõÊ®°ÂûãÊåëÊà∞ËàáË§áÈõúÊÄß (e.g., ËæØË´ñ„ÄÅË™çÈåØ„ÄÅÂêåÁêÜ„ÄÅÊÑèË≠ò, ‚Ä¶)Ôºõ [2] ÂèçÈ•ãÁµ¶ÔºàË™ûË®ÄÔºâÁêÜË´ñÁôºÂ±ïÁØÑÂºèÁöÑÂêàÁêÜÊÄßÔºå‰∏¶‰∏îÊîπËÆäÁ§æÊúÉÁôºÂ±ï„ÄÇ"
  },
  {
    "objectID": "weeks/week-2.html#ÂõûÂà∞ÊàëÂÄëË™≤Á®ãË®≠Ë®à‰∏ä",
    "href": "weeks/week-2.html#ÂõûÂà∞ÊàëÂÄëË™≤Á®ãË®≠Ë®à‰∏ä",
    "title": "Week 2 Introduction",
    "section": "ÂõûÂà∞ÊàëÂÄëË™≤Á®ãË®≠Ë®à‰∏ä",
    "text": "ÂõûÂà∞ÊàëÂÄëË™≤Á®ãË®≠Ë®à‰∏ä\n\n[Lecture] Ë¨õËß£„ÄåËàá NLP Áõ∏ÈóúÁöÑË™ûË®ÄÂ≠∏ÁêÜË´ñ„Äç\n\nlinguistically-oriented DNN analysis (cf.¬†BlackBox NLP Á≥ªÂàóÁ†îË®éÊúÉ; Computation in Linguistics ÂçîÊúÉ)\n(ML Ë™≤Á®ãÊê≠ÈÖç)\n\n[Lab] ÈÄèÈÅéÂØ¶‰ΩúÁ∑¥ÁøíËàáÂ∑•ÂÖ∑‰ΩøÁî®\n\nË™ûË®ÄÊ®°ÁµÑËß£Êûê via spacy\nË™ûË®ÄÊ®°ÂûãÊáâÁî® via Hugging Face (transformers)"
  },
  {
    "objectID": "weeks/week-2.html#section",
    "href": "weeks/week-2.html#section",
    "title": "Week 2 Introduction",
    "section": "",
    "text": "References\n\n\nBaroni, Marco. 2021. ‚ÄúOn the Gap Between Computational and Theoretical Linguistics.‚Äù https://www.virtual2021.eacl.org/plenary_session_keynote_by_marco_baroni.html.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. ‚ÄúLanguage Models Are Few-Shot Learners.‚Äù Advances in Neural Information Processing Systems 33: 1877‚Äì1901.\n\n\nDong, Qingxiu, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. ‚ÄúA Survey for in-Context Learning.‚Äù arXiv Preprint arXiv:2301.00234.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. ‚ÄúChain of Thought Prompting Elicits Reasoning in Large Language Models.‚Äù arXiv Preprint arXiv:2201.11903.\n\n\nZhou, Yongchao, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. ‚ÄúLarge Language Models Are Human-Level Prompt Engineers.‚Äù arXiv Preprint arXiv:2211.01910.\n\n\n======= ::: {style=‚Äúfont-size: 0.875em;‚Äù} Back to course schedule ‚èé :::"
  },
  {
    "objectID": "weeks/week3.html",
    "href": "weeks/week3.html",
    "title": "Week 3 Word",
    "section": "",
    "text": "Ë©ûÂΩô„ÄÅÊßãË©ûËàáË®àÁÆó\n\nÊñáÊú¨ËôïÁêÜÔºöÊñáÊú¨Ê≠£ÂâáÂåñ„ÄÅË™ûÊñôÂ∫´ËàáÁ®ãÂºèÊ¶ÇÂøµÂ∑•ÂÖ∑\nWordhoodÔºö many perspectives\n[Lab] Corpus processing skills\n\n\n\nLinguistics for NLP\n\n\n\n\nLanguage and Text\n\n\nÊñáÊú¨ËôïÁêÜÊòØËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÂü∫Á§éÂ∑•‰ΩúÔºåÊòØ‰∏ÄÁ®ÆÂ∞çÊñáÊú¨ÈÄ≤Ë°åÂàÜÊûê„ÄÅËôïÁêÜ„ÄÅÁ∑®ËºØ„ÄÅËΩâÊèõÁöÑÊäÄË°ì„ÄÇ\n\nÂæàÈÅ©ÂêàÁî®‰æÜÊê≠ÈÖçÁ®ãÂºèÂ≠∏Áøí„ÄÇ\n\nÊñáÊú¨Ê≠£Ââá (text normalization) ÊòØ‰∏ÄÁ®ÆÂ∞çÊñáÊú¨ÈÄ≤Ë°å Ê®ôÊ∫ñÂåñ ÁöÑËΩâÊèõ‰ªªÂãô„ÄÇ\n\n\n\nÊ®ôÊ∫ñÂåñÂú®Ê≠§ÊÑèÁæ©ÊòØÂ∞çÁï∂‰∏ã‰ªªÂãôÊõ¥‰æøÊç∑„ÄÅÊõ¥ÂÆπÊòìËôïÁêÜÁöÑÊÑèÊÄù„ÄÇ\n\n\n\n\ntext normalization tasks\n\n\ntokenization ÔºöÂ∞áÊñáÊú¨ÂàÜÂâ≤Êàê‰∏ÄÂÄãÂÄãÁöÑÔºàË©ûÁ¨¶ÔºâÂñÆ‰Ωç„ÄÇ\nlemmatizationÔºöÂ∞á‰∏çÂêåË°®Èù¢ÂΩ¢ÂºèÁöÑË©ûÂΩôÈÇÑÂéüÊàêÁõ∏ÂêåË©ûÊ†π„ÄÇ\nstemming ÔºöÁ∞°ÂåñÁâàÁöÑ lemmatizationÔºåÂè™ÊääÂæåÁ∂¥ÊãøÊéâ„ÄÇ\n\n\n\n\n\n\nÊúâÊôÇÂÄôÔºåÂ¶ÇËôïÁêÜÂè§Êº¢Ë™ûÔºåÈÇÑÈúÄË¶ÅÊñ∑Âè• sentence segmentation„ÄÇ\n\n\n\n\n\n\n\n\n\n\n\nÊØîËºÉÔºöÂâçËôïÁêÜ‰ªªÂãô„ÄåÊØîËºÉ„Äç‰∏çÊ∂âÂèäË™ûË®ÄÂ≠∏Áü•Ë≠ò„ÄÇ\n\n\n\n- case foldingÔºöÂ∞áÊâÄÊúâÂ≠óÊØçËΩâÊèõÊàêÂ∞èÂØ´„ÄÇ\n- stopword removalÔºöÂéªÈô§ÂÅúÁî®Ë©û„ÄÇ\n\n\n\n\n\nTokenization vs Word Segmentation\n\n\nWord segmentation Ë≤å‰ºº word-based tokenizationÔºå‰ΩÜÂú®‰∏≠ÊñáËàá‰∏Ä‰∫õË™ûË®ÄÁöÑËÑàÁµ°‰∏≠ÔºåÂÆÉÂÄë‰∏çÊòØÂÆåÂÖ®‰∏ÄÊ®£ÁöÑÊ¶ÇÂøµ„ÄÇ\n‰∏ªË¶ÅÁöÑÂ∑ÆÁï∞ÈªûÂú®Êñº wordhood ÁöÑÊ¶ÇÂøµÊòØ‰∏çÂÆöÁöÑ„ÄÅ‰ΩøÁî®‰∏≠ÁöÑË™ûÊÑèÊ±∫ÂÆöÁöÑ„ÄÇ\n\n\n\n\n\n```{python}\n# Chinese\nimport jieba\njieba.lcut('ÈÇÑË¶ÅÂ¶ÇÊ≠§Ë≤ªÂ∑•ÔºåÊàëË™çÁà≤ÊòØduckÂèØ‰∏çÂøÖ')\n# ['ÈÇÑË¶Å', 'Â¶ÇÊ≠§', 'Ë≤ªÂ∑•', 'Ôºå', 'ÊàëË™ç', 'Áà≤', 'ÊòØ', 'duck', 'ÂèØ', '‰∏çÂøÖ']\n\n# Japanese\nimport nagisa\ntext = 'Python„ÅßÁ∞°Âçò„Å´‰Ωø„Åà„Çã„ÉÑ„Éº„É´„Åß„Åô'\ndoc = nagisa.tagging(text)\ndoc.words\n# ['Python', '„Åß', 'Á∞°Âçò', '„Å´', '‰Ωø„Åà„Çã', '„ÉÑ„Éº„É´', '„Åß„Åô']\n\n# Korean\nimport konlpy\nphrase = \"ÏïÑÎ≤ÑÏßÄÍ∞ÄÎ∞©ÏóêÎì§Ïñ¥Í∞ÄÏã†Îã§\"\nfrom konlpy.tag import Hannanum\nhannanum = Hannanum()\nhannanum.morphs(phrase)\n# ['ÏïÑÎ≤ÑÏßÄÍ∞ÄÎ∞©ÏóêÎì§Ïñ¥Í∞Ä', 'Ïù¥', 'Ïãú„Ñ¥Îã§']\n\n# Thai\nimport tltk\nphrase = \"\"\"‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÄ‡∏Ç‡∏ï‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£‡∏ä‡∏µ‡πâ‡πÅ‡∏à‡∏á‡∏ß‡πà‡∏≤ ‡πÑ‡∏î‡πâ‡∏ô‡∏≥‡∏õ‡πâ‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏õ‡∏•‡∏¥‡∏á‡πÑ‡∏õ‡∏õ‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ \n‡πÉ‡∏ô‡πÄ‡∏Ç‡∏ï‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏≤‡∏¢‡∏™‡∏∏‡∏Å‡∏¥‡∏à ‡∏≠‡∏≤‡∏¢‡∏∏ 65 ‡∏õ‡∏µ ‡∏ñ‡∏π‡∏Å‡∏õ‡∏•‡∏¥‡∏á‡∏Å‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå\"\"\"\npieces = tltk.nlp.pos_tag(phrase)\npieces\n# [[('‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', 'NOUN'),\n#   ('‡πÄ‡∏Ç‡∏ï', 'NOUN'),\n#   ('‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£', 'PROPN'),\n#   ('‡∏ä‡∏µ‡πâ‡πÅ‡∏à‡∏á', 'VERB'),\n#   ('‡∏ß‡πà‡∏≤', 'SCONJ'),\n#   ('<s/>', 'PUNCT')],\n#  ...\n\n```\n\n\n\ndata and tools\nË¶ÅÈÄ≤Ë°åÊñáÊú¨ÁöÑËôïÁêÜÔºåÊàëÂÄëÈúÄË¶ÅÔºàÂ§ßÈáèÔºâË™ûÊñôËàáÂ•ΩÁî®ÁöÑÁ®ãÂºèÂ∑•ÂÖ∑„ÄÇ\n\n\n\nCorpus and Corpora\n\nË™ûÊñôÂ∫´ ÊòØ‰∏ÄÁ®ÆÈáçË¶ÅÁöÑË™ûË®ÄË≥áÊ∫ê(language resource) „ÄÇ\nÊñáÊú¨Ë™ûÊñôÂ∫´ÊòØËºÉÂ∏∏Ë¢´‰ΩøÁî®ÁöÑË™ûË®ÄË≥áÊ∫êÔºåÂÆÉ‰∏çÂÉÖÊòØ‰∏ÄÂÄãÊñáÊú¨ÁöÑÈõÜÂêà (collection of texts)ÔºåËóâÁî±Ê®ôË®ò (annotation)Ôºå‰πüÊòØË™ûË®ÄÁü•Ë≠òÂ§ñÈ°ØÂåñÁöÑ‰∏ÄÁ®ÆÂΩ¢Âºè„ÄÇ\n\n\n\n\n\n\n\nTip\n\n\n\nË≥áÊñôÁßëÂ≠∏ËàáË™ûÊñôÂàÜÊûê:ÊñπÊ≥ïËàáÂØ¶ÂãôÔºöË™ûË®ÄÂ≠∏ËßíÂ∫¶ÁöÑË™ûÊñôÂ∫´ÊñπÊ≥ïÊïôÁßëÊõ∏\n\n\n\n\n\nLanguage Laws\nÊúâ‰∫ÜË™ûÊñôÂ∫´ÔºåÊàëÂÄëÂ∞±ÂèØ‰ª•ÈÄ≤Ë°åË™ûË®ÄÂ≠∏ÁöÑÈáèÂåñÁ†îÁ©∂Ôºå‰æãÂ¶ÇÁôºÁèæË™ûË®ÄÊ≥ïÂâá„ÄÇ\n\n\n\n\n\n\nImportant\n\n\n\nË¶ÅÂÖàÁü•ÈÅì type (ÈÄöÂ∏∏Áî® \\(V\\) Ë°®Á§∫) Ëàá token (ÈÄöÂ∏∏Áî® \\(N\\) Ë°®Á§∫) ÁöÑÂ∑ÆÂà•„ÄÇÂâçËÄÖÊòØÊâÄËßÄÂØüÁöÑË™ûÊñô‰∏≠Â≠óË©ûÁöÑÁ®ÆÈ°ûÔºåÂæåËÄÖÊòØÂ≠óË©ûÁöÑÁ∏ΩÊï∏„ÄÇ \\(|V|\\) ÂâáË°®Á§∫Á®ÆÈ°ûÁöÑÊï∏Èáè (vocabulary size)„ÄÇ\n\n\n\n\n\n\n\nZipf‚Äôs lawÔºöÂú®Áµ¶ÂÆöÁöÑË™ûÊñô‰∏≠ÔºåÂ∞çÊñº‰ªªÊÑè‰∏ÄÂÄãË™ûË©ûÔºåÂÖ∂È†ªÁéá (frequency) ÁöÑÊéíÂêçÔºàrankÔºâÂíåÈ†ªÁéáÁöÑ‰πòÁ©çÂ§ßËá¥ÊòØ‰∏ÄÂÄãÂ∏∏Êï∏„ÄÇ\nHeaps‚Äô law (Herdan‚Äôs law)ÔºöÂú®Áµ¶ÂÆöÁöÑË™ûÊñô‰∏≠Ôºå(\\(|V|\\)) Â§ßËá¥ÊòØË™ûÊñôÂ§ßÂ∞è (N) ÁöÑ‰∏ÄÂÄãÊåáÊï∏ÂáΩÊï∏„ÄÇ\n\n\\(|V| = kN^{\\beta}\\), where \\(k\\) and \\(\\beta\\) are positive constants, and 0 < \\(\\beta\\) < 1\n\n\nMenzerath‚ÄìAltmann lawÔºöthe increase of the size of a linguistic construct results in a decrease of the size of its constituents.\n\n\n\n\n\n\n\n\n\n\n\nzip‚Äôs law\n\n\n\n\n\n\n\n\nÂÖàÊÉ≥‰∏Ä‰∏ãÊÄéÈ∫ºÂØ¶‰Ωú„ÄÇË´ã chatGPT Áï∂Á®ãÂºèÂä©ÊïôÔºåÂÆåÊàê Heap‚Äôs Law ÁöÑÂØ¶È©óËßÄÂØü„ÄÇ\n```{python}\n# your code here\n```"
  },
  {
    "objectID": "weeks/week3.html#Á∑¥Áøí",
    "href": "weeks/week3.html#Á∑¥Áøí",
    "title": "Week 3 Word",
    "section": "Á∑¥Áøí",
    "text": "Á∑¥Áøí\nCorpus processing skills via command line (bash)"
  },
  {
    "objectID": "weeks/week3.html#Â∏∏Áî®ÁöÑ-subword-based-tokenization-ÊñπÊ≥ï",
    "href": "weeks/week3.html#Â∏∏Áî®ÁöÑ-subword-based-tokenization-ÊñπÊ≥ï",
    "title": "Week 3 Word",
    "section": "Â∏∏Áî®ÁöÑ Subword-based tokenization ÊñπÊ≥ï",
    "text": "Â∏∏Áî®ÁöÑ Subword-based tokenization ÊñπÊ≥ï\n\nSubword-based tokenization ÊòØÁõÆÂâçËºÉË¢´‰ΩøÁî®ÁöÑÊñπÊ≥ïÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•\n\nËôïÁêÜ word-based ÁöÑÂïèÈ°åÔºàvery large vocabulary size, large number of OOV tokens, and different meaning of very similar wordsÔºâ\n‰πüÂèØ‰ª•ËôïÁêÜ character-based ÁöÑÂïèÈ°åÔºàvery long sequences and less meaningful individual tokensÔºâ„ÄÇ\n\nÂ∏∏Áî®ÁöÑÊñπÊ≥ïÊúâÔºö\n\nbyte-pair encoding (BPE) (> GPT family)\nwordpiece (> BERT)\nunigram language modeling\nSentencee piece"
  },
  {
    "objectID": "weeks/week3.html#byte-pair-encoding-bpe",
    "href": "weeks/week3.html#byte-pair-encoding-bpe",
    "title": "Week 3 Word",
    "section": "Byte-pair encoding (BPE)",
    "text": "Byte-pair encoding (BPE)\nSubword-based tokenization algorithm (Sennrich, Haddow, and Birch 2015)\n\ntoken learner and token segmenter\n\n\n\n\nLearner ÂÅöÁöÑ‰∫ã\n\n\n\nÊàëÂÄëÁî®Âéü‰ΩúËÄÖÁöÑÊäïÂΩ±Áâá‰æÜË™™Êòé"
  },
  {
    "objectID": "weeks/week3.html#Á∑¥Áøí-2",
    "href": "weeks/week3.html#Á∑¥Áøí-2",
    "title": "Week 3 Word",
    "section": "Á∑¥Áøí",
    "text": "Á∑¥Áøí\n‰ΩøÁî® HuggingFace ÁöÑ tokenizer"
  },
  {
    "objectID": "weeks/week3.html#word-vs-morpheme",
    "href": "weeks/week3.html#word-vs-morpheme",
    "title": "Week 3 Word",
    "section": "Word vs Morpheme",
    "text": "Word vs Morpheme\n\nmorphemeÔºöË™ûË®Ä‰∏≠ÊúÄÂ∞èÁöÑÊÑèÁæ©ÂñÆ‰Ωç„ÄÇ\nwordÔºöË™ûË®Ä‰∏≠ÊúÄÂ∞èÁöÑÊ¶ÇÂøµÂñÆ‰Ωç„ÄÇ"
  },
  {
    "objectID": "weeks/week3.html#‰∏≠ÊñáÁöÑË©ûÁ¥†Ê¶ÇÂøµ",
    "href": "weeks/week3.html#‰∏≠ÊñáÁöÑË©ûÁ¥†Ê¶ÇÂøµ",
    "title": "Week 3 Word",
    "section": "‰∏≠ÊñáÁöÑË©ûÁ¥†Ê¶ÇÂøµ",
    "text": "‰∏≠ÊñáÁöÑË©ûÁ¥†Ê¶ÇÂøµ"
  },
  {
    "objectID": "weeks/week3.html#‰∏≠ÊñáÊñ∑Ë©ûÂïèÈ°å",
    "href": "weeks/week3.html#‰∏≠ÊñáÊñ∑Ë©ûÂïèÈ°å",
    "title": "Week 3 Word",
    "section": "‰∏≠ÊñáÊñ∑Ë©ûÂïèÈ°å",
    "text": "‰∏≠ÊñáÊñ∑Ë©ûÂïèÈ°å\nChinese word segmentation: principles and practices"
  },
  {
    "objectID": "weeks/week3.html#Áà≤‰ªÄÈ∫ºÁèæÂú®ÁöÑ-chatgpt-‰∏çÂ§™ÊúÉÊñ∑ÁÆóË©ûÊúâ‰∏çÂêàÁêÜÂóé",
    "href": "weeks/week3.html#Áà≤‰ªÄÈ∫ºÁèæÂú®ÁöÑ-chatgpt-‰∏çÂ§™ÊúÉÊñ∑ÁÆóË©ûÊúâ‰∏çÂêàÁêÜÂóé",
    "title": "Week 3 Word",
    "section": "Áà≤‰ªÄÈ∫º„ÄåÁèæÂú®ÁöÑ„Äç chatGPT ‰∏çÂ§™ÊúÉÊñ∑/ÁÆóË©ûÔºüÊúâ‰∏çÂêàÁêÜÂóéÔºü",
    "text": "Áà≤‰ªÄÈ∫º„ÄåÁèæÂú®ÁöÑ„Äç chatGPT ‰∏çÂ§™ÊúÉÊñ∑/ÁÆóË©ûÔºüÊúâ‰∏çÂêàÁêÜÂóéÔºü"
  },
  {
    "objectID": "weeks/week3.html#section-2",
    "href": "weeks/week3.html#section-2",
    "title": "Week 3 Word",
    "section": "",
    "text": "Danger\n\n\n\n\nIn polysynthetic languages like Inuktitut or Mohawk, words can be very long and complex, consisting of multiple morphemes that would be considered separate words in English."
  },
  {
    "objectID": "weeks/week3.html#wordhood-issues",
    "href": "weeks/week3.html#wordhood-issues",
    "title": "Week 3 Word",
    "section": "Wordhood issues",
    "text": "Wordhood issues\n\nÂæû‰∏çÂêåË™ûË®ÄÂ≠∏ÁöÑÈ†òÂüüÔºàphonological-prosodic/morphological-grammatical/sociolinguiticÔºâ‰æÜÁúã wordÔºå‰πüÈõ£Êúâ‰∏ÄÂÆöÁöÑË©û(ÁöÑÁïåÁ∑ö)„ÄÇ (e.g., hao-le-mei in Mandarin Chinese)\n\n\n\nË™ûË®ÄÁâπÊÄßÁöÑÂΩ±Èüø‰ΩøÂæóÂú®Ë°®Â±§ÂΩ¢ÂºèÁöÑË©ûÈï∑‰∏çÁ©©ÂÆöÔºåË∑ùÈõ¢‰πü‰∏çÊòéÁ¢∫„ÄÇ(e.g., Er gibt nicht auf. in German; Èõ¢ÂêàË©û in Chinese) > In agglutinative languages, for example, multiple morphemes are combined into single words, making it difficult to determine where one word ends and another begins. In these cases, the term for ‚Äòword‚Äô might encompass a broader concept than in English.\nÊúâÊôÇÁï∂Ë©ûÁ¥†ÔºåÊúâÊôÇÁï∂Ë©û a morpheme behaves like a word on one subset of wordhood parameters but like a bound item on another. (Zingler 2020)"
  },
  {
    "objectID": "weeks/week6.html",
    "href": "weeks/week6.html",
    "title": "Week 6 Word Meaning (III)",
    "section": "",
    "text": "Ë©ûÂΩôÁöÑÊÉÖÊÑüÊÑèÁæ©\nËá™ÂãïÊÉÖÁ∑íÂàÜÊûê sentiment analysis"
  },
  {
    "objectID": "weeks/week6.html#ÈÄôÂÄãÁØÑÂºèËΩâÁßªÈÇÑÂú®ËøÖÈÄüÁôºÁîü‰∏≠",
    "href": "weeks/week6.html#ÈÄôÂÄãÁØÑÂºèËΩâÁßªÈÇÑÂú®ËøÖÈÄüÁôºÁîü‰∏≠",
    "title": "Week 6 Word Meaning (III)",
    "section": "ÈÄôÂÄãÁØÑÂºèËΩâÁßªÈÇÑÂú®ËøÖÈÄüÁôºÁîü‰∏≠",
    "text": "ÈÄôÂÄãÁØÑÂºèËΩâÁßªÈÇÑÂú®ËøÖÈÄüÁôºÁîü‰∏≠\nPretrain, Prompt, and Predict (P. Liu et al. 2023)\n\n\n\nParadigm shift in NLP"
  },
  {
    "objectID": "weeks/week6.html#llm-2023-",
    "href": "weeks/week6.html#llm-2023-",
    "title": "Week 6 Word Meaning (III)",
    "section": "LLM 2023-",
    "text": "LLM 2023-\n\n\n\nË™ûË®ÄËàáÁü•Ë≠òË≥áÊ∫ê‰ΩúÁà≤‰∏ÄÁ®ÆÁ¨¶Á¢ºÂ∞àÂÆ∂Ê®°ÂºèÊòØÂêàÁêÜÁöÑ (Âú®Áî¢Â≠∏ÊîªÈò≤ÁöÑËÑàÁµ°‰∫¶ÊúâÂÖ∂ÊÑèÁæ©)„ÄÇÁõÆÂâçÂèàÁ®± neuro-symbolic approach„ÄÇ\nÊé•‰∏ã‰æÜÁöÑ Bakeoff (week 13) ÊàëÂÄëÊúÉËÆìÂ§ßÂÆ∂Á∑¥ÁøíÂØ¶‰ΩúÈÄôÂÄãÊñπÂêë„ÄÇ"
  },
  {
    "objectID": "weeks/week6.html#‰ªäÂ§©ÁöÑÊâìÁÆó",
    "href": "weeks/week6.html#‰ªäÂ§©ÁöÑÊâìÁÆó",
    "title": "Week 6 Word Meaning (III)",
    "section": "‰ªäÂ§©ÁöÑÊâìÁÆó",
    "text": "‰ªäÂ§©ÁöÑÊâìÁÆó\n\nÊàëÂÄë‰∏ÄËµ∑ËÆÄÈÅé SLP-chap 25„ÄÇ\nFrameNet data analysis"
  },
  {
    "objectID": "weeks/week6.html#Áõ∏ÈóúÁöÑÊï¥ÁµÑÊ¶ÇÂøµ",
    "href": "weeks/week6.html#Áõ∏ÈóúÁöÑÊï¥ÁµÑÊ¶ÇÂøµ",
    "title": "Week 6 Word Meaning (III)",
    "section": "Áõ∏ÈóúÁöÑÊï¥ÁµÑÊ¶ÇÂøµ",
    "text": "Áõ∏ÈóúÁöÑÊï¥ÁµÑÊ¶ÇÂøµ\nÂåÖÊã¨‰∫Ü deceptive opinions (Ë©êÈ®ôÊÑèË¶ã)„ÄÅemotion (ÊÉÖÁ∑í)„ÄÅsentiment (ÊÉÖÊÑü)„ÄÅsubjectivity (‰∏ªËßÄÊÄß)„ÄÅstance (Á´ãÂ†¥)„ÄÅtrustworthiness (ÂèØ‰ø°Â∫¶)„ÄÅpolarity (Ê•µÊÄß)„ÄÅirony (ÂèçË´∑)„ÄÅsarcasm (Âò≤Á¨ë)„ÄÅhumor (ÂπΩÈªò)„ÄÅpropaganda (ÂÆ£ÂÇ≥)„ÄÅbias (ÂÅèË¶ã)„ÄÅaggression (‰æµÁï•ÊÄß)„ÄÅtoxicity (ÊØíÊÄß)„ÄÅevaluation„ÄÅappraisal„ÄÅaffect„ÄÅmood„ÄÅfeelings„ÄÅbeliefs intention Á≠âÁ≠â„ÄÇ\n\n\n\n\n\n\nImportant\n\n\n\nÂÖàÂÅöÊ¶ÇÂøµÁöÑÂçÄÂàÜ„ÄÅÂØ¶Áî®Âá∫ÁôºÁöÑË©ûÊÑèÂçÄÂàÜÔºåÂÜç‰æÜÂÅöÂØ¶ÈöõÁöÑÊáâÁî®„ÄÇ"
  },
  {
    "objectID": "weeks/week6.html#ÊØîËºÉÂ∏∏Ë®éË´ñÁöÑÊÉÖÊÑüÂàÜÊûê",
    "href": "weeks/week6.html#ÊØîËºÉÂ∏∏Ë®éË´ñÁöÑÊÉÖÊÑüÂàÜÊûê",
    "title": "Week 6 Word Meaning (III)",
    "section": "ÊØîËºÉÂ∏∏Ë®éË´ñÁöÑÊÉÖÊÑüÂàÜÊûê",
    "text": "ÊØîËºÉÂ∏∏Ë®éË´ñÁöÑÊÉÖÊÑüÂàÜÊûê\n\nsentiment is defined as an attitude, thought, or judgment prompted by feeling, whereas opinion is defined as a view, judgment, or appraisal formed in the mind about a particular matter.(B. Liu 2020)\n\n\n\n\n\n\n\nsentiment Âíå opinion Âú®Ë™ûË®Ä‰ΩøÁî®‰∏äÁöÑÂ∑ÆÁï∞\n\n\n\n\nÊàëÊúâÈªûÊìîÂøÉ‰ªäÂπ¥ÂÖ®ÁêÉÁöÑÁ∂ìÊøü„ÄÇÔºàÊàë‰πüÊòØ/Êàë‰πüË†ªÊìîÂøÉÁöÑÔºâ\nÊàëË™çÁà≤‰ªäÂπ¥ÁöÑÁ∂ìÊøü‰∏çÊúÉÂæàÂ•Ω„ÄÇÔºàÊàëÂêåÊÑè/‰∏çÂêåÊÑèÔºâ"
  },
  {
    "objectID": "weeks/week6.html#levels-of-analysis",
    "href": "weeks/week6.html#levels-of-analysis",
    "title": "Week 6 Word Meaning (III)",
    "section": "Levels of Analysis",
    "text": "Levels of Analysis\n\ndocument-level (ÊñáÊ™îÁ¥ö)\nsentence-level (Âè•Â≠êÁ¥ö)\naspect-level (ÊñπÈù¢Á¥ö)\n\nÂØ¶‰Ωú‰∏äÂ§ßÈÉ®ÂàÜÈÉΩÁï∂ÊàêÔºàÈ°ûÂà• class„ÄÅÊ•µÂ∫¶ polarityÔºâÂàÜÈ°ûÂïèÈ°å„ÄÇAspect-based Sentiment Analysis Èõ£Â∫¶Êõ¥È´ò‰∏ÄÈªû (Zhang et al. 2022)„ÄÇ\n\n\n\nABSA"
  },
  {
    "objectID": "weeks/week6.html#‰ΩÜÂèØËÉΩ‰πüÊ≤íÈÇ£È∫ºÈõ£‰∫Ü",
    "href": "weeks/week6.html#‰ΩÜÂèØËÉΩ‰πüÊ≤íÈÇ£È∫ºÈõ£‰∫Ü",
    "title": "Week 6 Word Meaning (III)",
    "section": "‰ΩÜÂèØËÉΩ‰πüÊ≤íÈÇ£È∫ºÈõ£‰∫Ü",
    "text": "‰ΩÜÂèØËÉΩ‰πüÊ≤íÈÇ£È∫ºÈõ£‰∫Ü"
  },
  {
    "objectID": "weeks/week6.html#Â§öÊ®°ÊÖãÁöÑÊÉÖÊÑüË®àÁÆó",
    "href": "weeks/week6.html#Â§öÊ®°ÊÖãÁöÑÊÉÖÊÑüË®àÁÆó",
    "title": "Week 6 Word Meaning (III)",
    "section": "Â§öÊ®°ÊÖãÁöÑÊÉÖÊÑüË®àÁÆó",
    "text": "Â§öÊ®°ÊÖãÁöÑÊÉÖÊÑüË®àÁÆó\nmultimodal affective computing"
  }
]