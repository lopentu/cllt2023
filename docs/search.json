[
  {
    "objectID": "weeks/week3.html",
    "href": "weeks/week3.html",
    "title": "Week 3 Word",
    "section": "",
    "text": "è©å½™ã€æ§‹è©èˆ‡è¨ˆç®—\n\næ–‡æœ¬è™•ç†ï¼šæ–‡æœ¬æ­£å‰‡åŒ–ã€èªæ–™åº«èˆ‡ç¨‹å¼æ¦‚å¿µå·¥å…·\nWordhoodï¼š many perspectives\n[Lab] Corpus processing skills\n\n\n\nLinguistics for NLP\n\n\n\n\nLanguage and Text\n\n\næ–‡æœ¬è™•ç†æ˜¯è‡ªç„¶èªè¨€è™•ç†çš„åŸºç¤å·¥ä½œï¼Œæ˜¯ä¸€ç¨®å°æ–‡æœ¬é€²è¡Œåˆ†æã€è™•ç†ã€ç·¨è¼¯ã€è½‰æ›çš„æŠ€è¡“ã€‚\n\nå¾ˆé©åˆç”¨ä¾†æ­é…ç¨‹å¼å­¸ç¿’ã€‚\n\næ–‡æœ¬æ­£å‰‡ (text normalization) æ˜¯ä¸€ç¨®å°æ–‡æœ¬é€²è¡Œ æ¨™æº–åŒ– çš„è½‰æ›ä»»å‹™ã€‚\n\n\n\næ¨™æº–åŒ–åœ¨æ­¤æ„ç¾©æ˜¯å°ç•¶ä¸‹ä»»å‹™æ›´ä¾¿æ·ã€æ›´å®¹æ˜“è™•ç†çš„æ„æ€ã€‚\n\n\n\n\ntext normalization tasks\n\n\ntokenization ï¼šå°‡æ–‡æœ¬åˆ†å‰²æˆä¸€å€‹å€‹çš„ï¼ˆè©ç¬¦ï¼‰å–®ä½ã€‚\nlemmatizationï¼šå°‡ä¸åŒè¡¨é¢å½¢å¼çš„è©å½™é‚„åŸæˆç›¸åŒè©æ ¹ã€‚\nstemming ï¼šç°¡åŒ–ç‰ˆçš„ lemmatizationï¼ŒåªæŠŠå¾Œç¶´æ‹¿æ‰ã€‚\n\n\n\n\n\n\næœ‰æ™‚å€™ï¼Œå¦‚è™•ç†å¤æ¼¢èªï¼Œé‚„éœ€è¦æ–·å¥ sentence segmentationã€‚\n\n\n\n\n\n\n\n\n\n\n\næ¯”è¼ƒï¼šå‰è™•ç†ä»»å‹™ã€Œæ¯”è¼ƒã€ä¸æ¶‰åŠèªè¨€å­¸çŸ¥è­˜ã€‚\n\n\n\n- case foldingï¼šå°‡æ‰€æœ‰å­—æ¯è½‰æ›æˆå°å¯«ã€‚\n- stopword removalï¼šå»é™¤åœç”¨è©ã€‚\n\n\n\n\n\nTokenization vs Word Segmentation\n\n\nWord segmentation è²Œä¼¼ word-based tokenizationï¼Œä½†åœ¨ä¸­æ–‡èˆ‡ä¸€äº›èªè¨€çš„è„ˆçµ¡ä¸­ï¼Œå®ƒå€‘ä¸æ˜¯å®Œå…¨ä¸€æ¨£çš„æ¦‚å¿µã€‚\nä¸»è¦çš„å·®ç•°é»åœ¨æ–¼ wordhood çš„æ¦‚å¿µæ˜¯ä¸å®šçš„ã€ä½¿ç”¨ä¸­çš„èªæ„æ±ºå®šçš„ã€‚\n\n\n\n\n\n```{python}\n# Chinese\nimport jieba\njieba.lcut('é‚„è¦å¦‚æ­¤è²»å·¥ï¼Œæˆ‘èªçˆ²æ˜¯duckå¯ä¸å¿…')\n# ['é‚„è¦', 'å¦‚æ­¤', 'è²»å·¥', 'ï¼Œ', 'æˆ‘èª', 'çˆ²', 'æ˜¯', 'duck', 'å¯', 'ä¸å¿…']\n\n# Japanese\nimport nagisa\ntext = 'Pythonã§ç°¡å˜ã«ä½¿ãˆã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™'\ndoc = nagisa.tagging(text)\ndoc.words\n# ['Python', 'ã§', 'ç°¡å˜', 'ã«', 'ä½¿ãˆã‚‹', 'ãƒ„ãƒ¼ãƒ«', 'ã§ã™']\n\n# Korean\nimport konlpy\nphrase = \"ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤\"\nfrom konlpy.tag import Hannanum\nhannanum = Hannanum()\nhannanum.morphs(phrase)\n# ['ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€', 'ì´', 'ì‹œã„´ë‹¤']\n\n# Thai\nimport tltk\nphrase = \"\"\"à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™à¹€à¸‚à¸•à¸ˆà¸•à¸¸à¸ˆà¸±à¸à¸£à¸Šà¸µà¹‰à¹à¸ˆà¸‡à¸§à¹ˆà¸² à¹„à¸”à¹‰à¸™à¸³à¸›à¹‰à¸²à¸¢à¸›à¸£à¸°à¸à¸²à¸¨à¹€à¸•à¸·à¸­à¸™à¸›à¸¥à¸´à¸‡à¹„à¸›à¸›à¸±à¸à¸•à¸²à¸¡à¹à¸«à¸¥à¹ˆà¸‡à¸™à¹‰à¸³ \nà¹ƒà¸™à¹€à¸‚à¸•à¸­à¸³à¹€à¸ à¸­à¹€à¸¡à¸·à¸­à¸‡ à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸”à¸­à¹ˆà¸²à¸‡à¸—à¸­à¸‡ à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸™à¸²à¸¢à¸ªà¸¸à¸à¸´à¸ˆ à¸­à¸²à¸¢à¸¸ 65 à¸›à¸µ à¸–à¸¹à¸à¸›à¸¥à¸´à¸‡à¸à¸±à¸”à¹à¸¥à¹‰à¸§à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹„à¸›à¸à¸šà¹à¸à¸—à¸¢à¹Œ\"\"\"\npieces = tltk.nlp.pos_tag(phrase)\npieces\n# [[('à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™', 'NOUN'),\n#   ('à¹€à¸‚à¸•', 'NOUN'),\n#   ('à¸ˆà¸•à¸¸à¸ˆà¸±à¸à¸£', 'PROPN'),\n#   ('à¸Šà¸µà¹‰à¹à¸ˆà¸‡', 'VERB'),\n#   ('à¸§à¹ˆà¸²', 'SCONJ'),\n#   ('&lt;s/&gt;', 'PUNCT')],\n#  ...\n\n```\n\n\n\ndata and tools\nè¦é€²è¡Œæ–‡æœ¬çš„è™•ç†ï¼Œæˆ‘å€‘éœ€è¦ï¼ˆå¤§é‡ï¼‰èªæ–™èˆ‡å¥½ç”¨çš„ç¨‹å¼å·¥å…·ã€‚\n\n\n\nCorpus and Corpora\n\nèªæ–™åº« æ˜¯ä¸€ç¨®é‡è¦çš„èªè¨€è³‡æº(language resource) ã€‚\næ–‡æœ¬èªæ–™åº«æ˜¯è¼ƒå¸¸è¢«ä½¿ç”¨çš„èªè¨€è³‡æºï¼Œå®ƒä¸åƒ…æ˜¯ä¸€å€‹æ–‡æœ¬çš„é›†åˆ (collection of texts)ï¼Œè—‰ç”±æ¨™è¨˜ (annotation)ï¼Œä¹Ÿæ˜¯èªè¨€çŸ¥è­˜å¤–é¡¯åŒ–çš„ä¸€ç¨®å½¢å¼ã€‚\n\n\n\n\n\n\n\nTip\n\n\n\nè³‡æ–™ç§‘å­¸èˆ‡èªæ–™åˆ†æ:æ–¹æ³•èˆ‡å¯¦å‹™ï¼šèªè¨€å­¸è§’åº¦çš„èªæ–™åº«æ–¹æ³•æ•™ç§‘æ›¸\n\n\n\n\n\nLanguage Laws\næœ‰äº†èªæ–™åº«ï¼Œæˆ‘å€‘å°±å¯ä»¥é€²è¡Œèªè¨€å­¸çš„é‡åŒ–ç ”ç©¶ï¼Œä¾‹å¦‚ç™¼ç¾èªè¨€æ³•å‰‡ã€‚\n\n\n\n\n\n\nImportant\n\n\n\nè¦å…ˆçŸ¥é“ type (é€šå¸¸ç”¨ \\(V\\) è¡¨ç¤º) èˆ‡ token (é€šå¸¸ç”¨ \\(N\\) è¡¨ç¤º) çš„å·®åˆ¥ã€‚å‰è€…æ˜¯æ‰€è§€å¯Ÿçš„èªæ–™ä¸­å­—è©çš„ç¨®é¡ï¼Œå¾Œè€…æ˜¯å­—è©çš„ç¸½æ•¸ã€‚ \\(|V|\\) å‰‡è¡¨ç¤ºç¨®é¡çš„æ•¸é‡ (vocabulary size)ã€‚\n\n\n\n\n\n\n\nZipfâ€™s lawï¼šåœ¨çµ¦å®šçš„èªæ–™ä¸­ï¼Œå°æ–¼ä»»æ„ä¸€å€‹èªè©ï¼Œå…¶é »ç‡ (frequency) çš„æ’åï¼ˆrankï¼‰å’Œé »ç‡çš„ä¹˜ç©å¤§è‡´æ˜¯ä¸€å€‹å¸¸æ•¸ã€‚\nHeapsâ€™ law (Herdanâ€™s law)ï¼šåœ¨çµ¦å®šçš„èªæ–™ä¸­ï¼Œ(\\(|V|\\)) å¤§è‡´æ˜¯èªæ–™å¤§å° (N) çš„ä¸€å€‹æŒ‡æ•¸å‡½æ•¸ã€‚\n\n\\(|V| = kN^{\\beta}\\), where \\(k\\) and \\(\\beta\\) are positive constants, and 0 &lt; \\(\\beta\\) &lt; 1\n\n\nMenzerathâ€“Altmann lawï¼šthe increase of the size of a linguistic construct results in a decrease of the size of its constituents.\n\n\n\n\n\n\n\n\n\n\n\nzipâ€™s law\n\n\n\n\n\n\n\n\nå…ˆæƒ³ä¸€ä¸‹æ€éº¼å¯¦ä½œã€‚è«‹ chatGPT ç•¶ç¨‹å¼åŠ©æ•™ï¼Œå®Œæˆ Heapâ€™s Law çš„å¯¦é©—è§€å¯Ÿã€‚\n```{python}\n# your code here\n```"
  },
  {
    "objectID": "weeks/week3.html#èªè¨€å­¸çµ¦è‡ªç„¶èªè¨€è™•ç†çš„å°è±¡",
    "href": "weeks/week3.html#èªè¨€å­¸çµ¦è‡ªç„¶èªè¨€è™•ç†çš„å°è±¡",
    "title": "Week 3 Word",
    "section": "",
    "text": "Linguistics for NLP"
  },
  {
    "objectID": "weeks/week3.html#èªè¨€èˆ‡æ–‡æœ¬",
    "href": "weeks/week3.html#èªè¨€èˆ‡æ–‡æœ¬",
    "title": "Week 3 Word",
    "section": "",
    "text": "Language and Text\n\n\næ–‡æœ¬è™•ç†æ˜¯è‡ªç„¶èªè¨€è™•ç†çš„åŸºç¤å·¥ä½œï¼Œæ˜¯ä¸€ç¨®å°æ–‡æœ¬é€²è¡Œåˆ†æã€è™•ç†ã€ç·¨è¼¯ã€è½‰æ›çš„æŠ€è¡“ã€‚\n\nå¾ˆé©åˆç”¨ä¾†æ­é…ç¨‹å¼å­¸ç¿’ã€‚\n\næ–‡æœ¬æ­£å‰‡ (text normalization) æ˜¯ä¸€ç¨®å°æ–‡æœ¬é€²è¡Œ æ¨™æº–åŒ– çš„è½‰æ›ä»»å‹™ã€‚\n\n\n\næ¨™æº–åŒ–åœ¨æ­¤æ„ç¾©æ˜¯å°ç•¶ä¸‹ä»»å‹™æ›´ä¾¿æ·ã€æ›´å®¹æ˜“è™•ç†çš„æ„æ€ã€‚"
  },
  {
    "objectID": "weeks/week3.html#æ–‡æœ¬æ­£å‰‡åŒ–",
    "href": "weeks/week3.html#æ–‡æœ¬æ­£å‰‡åŒ–",
    "title": "Week 3 Word",
    "section": "",
    "text": "text normalization tasks\n\n\ntokenization ï¼šå°‡æ–‡æœ¬åˆ†å‰²æˆä¸€å€‹å€‹çš„ï¼ˆè©ç¬¦ï¼‰å–®ä½ã€‚\nlemmatizationï¼šå°‡ä¸åŒè¡¨é¢å½¢å¼çš„è©å½™é‚„åŸæˆç›¸åŒè©æ ¹ã€‚\nstemming ï¼šç°¡åŒ–ç‰ˆçš„ lemmatizationï¼ŒåªæŠŠå¾Œç¶´æ‹¿æ‰ã€‚"
  },
  {
    "objectID": "weeks/week3.html#section",
    "href": "weeks/week3.html#section",
    "title": "Week 3 Word",
    "section": "",
    "text": "æœ‰æ™‚å€™ï¼Œå¦‚è™•ç†å¤æ¼¢èªï¼Œé‚„éœ€è¦æ–·å¥ sentence segmentationã€‚"
  },
  {
    "objectID": "weeks/week3.html#æ–‡æœ¬æ­£å‰‡åŒ–å’Œå‰è™•ç†",
    "href": "weeks/week3.html#æ–‡æœ¬æ­£å‰‡åŒ–å’Œå‰è™•ç†",
    "title": "Week 3 Word",
    "section": "",
    "text": "æ¯”è¼ƒï¼šå‰è™•ç†ä»»å‹™ã€Œæ¯”è¼ƒã€ä¸æ¶‰åŠèªè¨€å­¸çŸ¥è­˜ã€‚\n\n\n\n- case foldingï¼šå°‡æ‰€æœ‰å­—æ¯è½‰æ›æˆå°å¯«ã€‚\n- stopword removalï¼šå»é™¤åœç”¨è©ã€‚"
  },
  {
    "objectID": "weeks/week3.html#åˆ‡ç¬¦å’Œæ–·è©",
    "href": "weeks/week3.html#åˆ‡ç¬¦å’Œæ–·è©",
    "title": "Week 3 Word",
    "section": "",
    "text": "Tokenization vs Word Segmentation\n\n\nWord segmentation è²Œä¼¼ word-based tokenizationï¼Œä½†åœ¨ä¸­æ–‡èˆ‡ä¸€äº›èªè¨€çš„è„ˆçµ¡ä¸­ï¼Œå®ƒå€‘ä¸æ˜¯å®Œå…¨ä¸€æ¨£çš„æ¦‚å¿µã€‚\nä¸»è¦çš„å·®ç•°é»åœ¨æ–¼ wordhood çš„æ¦‚å¿µæ˜¯ä¸å®šçš„ã€ä½¿ç”¨ä¸­çš„èªæ„æ±ºå®šçš„ã€‚"
  },
  {
    "objectID": "weeks/week3.html#east-asian-languages",
    "href": "weeks/week3.html#east-asian-languages",
    "title": "Week 3 Word",
    "section": "",
    "text": "```{python}\n# Chinese\nimport jieba\njieba.lcut('é‚„è¦å¦‚æ­¤è²»å·¥ï¼Œæˆ‘èªçˆ²æ˜¯duckå¯ä¸å¿…')\n# ['é‚„è¦', 'å¦‚æ­¤', 'è²»å·¥', 'ï¼Œ', 'æˆ‘èª', 'çˆ²', 'æ˜¯', 'duck', 'å¯', 'ä¸å¿…']\n\n# Japanese\nimport nagisa\ntext = 'Pythonã§ç°¡å˜ã«ä½¿ãˆã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™'\ndoc = nagisa.tagging(text)\ndoc.words\n# ['Python', 'ã§', 'ç°¡å˜', 'ã«', 'ä½¿ãˆã‚‹', 'ãƒ„ãƒ¼ãƒ«', 'ã§ã™']\n\n# Korean\nimport konlpy\nphrase = \"ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤\"\nfrom konlpy.tag import Hannanum\nhannanum = Hannanum()\nhannanum.morphs(phrase)\n# ['ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€', 'ì´', 'ì‹œã„´ë‹¤']\n\n# Thai\nimport tltk\nphrase = \"\"\"à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™à¹€à¸‚à¸•à¸ˆà¸•à¸¸à¸ˆà¸±à¸à¸£à¸Šà¸µà¹‰à¹à¸ˆà¸‡à¸§à¹ˆà¸² à¹„à¸”à¹‰à¸™à¸³à¸›à¹‰à¸²à¸¢à¸›à¸£à¸°à¸à¸²à¸¨à¹€à¸•à¸·à¸­à¸™à¸›à¸¥à¸´à¸‡à¹„à¸›à¸›à¸±à¸à¸•à¸²à¸¡à¹à¸«à¸¥à¹ˆà¸‡à¸™à¹‰à¸³ \nà¹ƒà¸™à¹€à¸‚à¸•à¸­à¸³à¹€à¸ à¸­à¹€à¸¡à¸·à¸­à¸‡ à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸”à¸­à¹ˆà¸²à¸‡à¸—à¸­à¸‡ à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸™à¸²à¸¢à¸ªà¸¸à¸à¸´à¸ˆ à¸­à¸²à¸¢à¸¸ 65 à¸›à¸µ à¸–à¸¹à¸à¸›à¸¥à¸´à¸‡à¸à¸±à¸”à¹à¸¥à¹‰à¸§à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹„à¸›à¸à¸šà¹à¸à¸—à¸¢à¹Œ\"\"\"\npieces = tltk.nlp.pos_tag(phrase)\npieces\n# [[('à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™', 'NOUN'),\n#   ('à¹€à¸‚à¸•', 'NOUN'),\n#   ('à¸ˆà¸•à¸¸à¸ˆà¸±à¸à¸£', 'PROPN'),\n#   ('à¸Šà¸µà¹‰à¹à¸ˆà¸‡', 'VERB'),\n#   ('à¸§à¹ˆà¸²', 'SCONJ'),\n#   ('&lt;s/&gt;', 'PUNCT')],\n#  ...\n\n```"
  },
  {
    "objectID": "weeks/week3.html#èªè¨€è³‡æºèˆ‡å·¥å…·",
    "href": "weeks/week3.html#èªè¨€è³‡æºèˆ‡å·¥å…·",
    "title": "Week 3 Word",
    "section": "",
    "text": "data and tools\nè¦é€²è¡Œæ–‡æœ¬çš„è™•ç†ï¼Œæˆ‘å€‘éœ€è¦ï¼ˆå¤§é‡ï¼‰èªæ–™èˆ‡å¥½ç”¨çš„ç¨‹å¼å·¥å…·ã€‚"
  },
  {
    "objectID": "weeks/week3.html#èªæ–™åº«",
    "href": "weeks/week3.html#èªæ–™åº«",
    "title": "Week 3 Word",
    "section": "",
    "text": "Corpus and Corpora\n\nèªæ–™åº« æ˜¯ä¸€ç¨®é‡è¦çš„èªè¨€è³‡æº(language resource) ã€‚\næ–‡æœ¬èªæ–™åº«æ˜¯è¼ƒå¸¸è¢«ä½¿ç”¨çš„èªè¨€è³‡æºï¼Œå®ƒä¸åƒ…æ˜¯ä¸€å€‹æ–‡æœ¬çš„é›†åˆ (collection of texts)ï¼Œè—‰ç”±æ¨™è¨˜ (annotation)ï¼Œä¹Ÿæ˜¯èªè¨€çŸ¥è­˜å¤–é¡¯åŒ–çš„ä¸€ç¨®å½¢å¼ã€‚\n\n\n\n\n\n\n\nTip\n\n\n\nè³‡æ–™ç§‘å­¸èˆ‡èªæ–™åˆ†æ:æ–¹æ³•èˆ‡å¯¦å‹™ï¼šèªè¨€å­¸è§’åº¦çš„èªæ–™åº«æ–¹æ³•æ•™ç§‘æ›¸"
  },
  {
    "objectID": "weeks/week3.html#å­—è©åˆ†ä½ˆæ³•å‰‡",
    "href": "weeks/week3.html#å­—è©åˆ†ä½ˆæ³•å‰‡",
    "title": "Week 3 Word",
    "section": "",
    "text": "Language Laws\næœ‰äº†èªæ–™åº«ï¼Œæˆ‘å€‘å°±å¯ä»¥é€²è¡Œèªè¨€å­¸çš„é‡åŒ–ç ”ç©¶ï¼Œä¾‹å¦‚ç™¼ç¾èªè¨€æ³•å‰‡ã€‚\n\n\n\n\n\n\nImportant\n\n\n\nè¦å…ˆçŸ¥é“ type (é€šå¸¸ç”¨ \\(V\\) è¡¨ç¤º) èˆ‡ token (é€šå¸¸ç”¨ \\(N\\) è¡¨ç¤º) çš„å·®åˆ¥ã€‚å‰è€…æ˜¯æ‰€è§€å¯Ÿçš„èªæ–™ä¸­å­—è©çš„ç¨®é¡ï¼Œå¾Œè€…æ˜¯å­—è©çš„ç¸½æ•¸ã€‚ \\(|V|\\) å‰‡è¡¨ç¤ºç¨®é¡çš„æ•¸é‡ (vocabulary size)ã€‚"
  },
  {
    "objectID": "weeks/week3.html#å¹¾å€‹å·²è­‰å¯¦çš„è‘—åçš„æ³•å‰‡",
    "href": "weeks/week3.html#å¹¾å€‹å·²è­‰å¯¦çš„è‘—åçš„æ³•å‰‡",
    "title": "Week 3 Word",
    "section": "",
    "text": "Zipfâ€™s lawï¼šåœ¨çµ¦å®šçš„èªæ–™ä¸­ï¼Œå°æ–¼ä»»æ„ä¸€å€‹èªè©ï¼Œå…¶é »ç‡ (frequency) çš„æ’åï¼ˆrankï¼‰å’Œé »ç‡çš„ä¹˜ç©å¤§è‡´æ˜¯ä¸€å€‹å¸¸æ•¸ã€‚\nHeapsâ€™ law (Herdanâ€™s law)ï¼šåœ¨çµ¦å®šçš„èªæ–™ä¸­ï¼Œ(\\(|V|\\)) å¤§è‡´æ˜¯èªæ–™å¤§å° (N) çš„ä¸€å€‹æŒ‡æ•¸å‡½æ•¸ã€‚\n\n\\(|V| = kN^{\\beta}\\), where \\(k\\) and \\(\\beta\\) are positive constants, and 0 &lt; \\(\\beta\\) &lt; 1\n\n\nMenzerathâ€“Altmann lawï¼šthe increase of the size of a linguistic construct results in a decrease of the size of its constituents."
  },
  {
    "objectID": "weeks/week3.html#section-1",
    "href": "weeks/week3.html#section-1",
    "title": "Week 3 Word",
    "section": "",
    "text": "zipâ€™s law"
  },
  {
    "objectID": "weeks/week3.html#ç·´ç¿’-1",
    "href": "weeks/week3.html#ç·´ç¿’-1",
    "title": "Week 3 Word",
    "section": "",
    "text": "å…ˆæƒ³ä¸€ä¸‹æ€éº¼å¯¦ä½œã€‚è«‹ chatGPT ç•¶ç¨‹å¼åŠ©æ•™ï¼Œå®Œæˆ Heapâ€™s Law çš„å¯¦é©—è§€å¯Ÿã€‚\n```{python}\n# your code here\n```"
  },
  {
    "objectID": "weeks/week3.html#ç·´ç¿’",
    "href": "weeks/week3.html#ç·´ç¿’",
    "title": "Week 3 Word",
    "section": "ç·´ç¿’",
    "text": "ç·´ç¿’\nCorpus processing skills via command line (bash)"
  },
  {
    "objectID": "weeks/week3.html#å¸¸ç”¨çš„-subword-based-tokenization-æ–¹æ³•",
    "href": "weeks/week3.html#å¸¸ç”¨çš„-subword-based-tokenization-æ–¹æ³•",
    "title": "Week 3 Word",
    "section": "å¸¸ç”¨çš„ Subword-based tokenization æ–¹æ³•",
    "text": "å¸¸ç”¨çš„ Subword-based tokenization æ–¹æ³•\n\nSubword-based tokenization æ˜¯ç›®å‰è¼ƒè¢«ä½¿ç”¨çš„æ–¹æ³•ï¼Œå› ç‚ºå®ƒå¯ä»¥\n\nè™•ç† word-based çš„å•é¡Œï¼ˆvery large vocabulary size, large number of OOV tokens, and different meaning of very similar wordsï¼‰\nä¹Ÿå¯ä»¥è™•ç† character-based çš„å•é¡Œï¼ˆvery long sequences and less meaningful individual tokensï¼‰ã€‚\n\nå¸¸ç”¨çš„æ–¹æ³•æœ‰ï¼š\n\nbyte-pair encoding (BPE) (&gt; GPT family)\nwordpiece (&gt; BERT)\nunigram language modeling\nSentencee piece"
  },
  {
    "objectID": "weeks/week3.html#byte-pair-encoding-bpe",
    "href": "weeks/week3.html#byte-pair-encoding-bpe",
    "title": "Week 3 Word",
    "section": "Byte-pair encoding (BPE)",
    "text": "Byte-pair encoding (BPE)\nSubword-based tokenization algorithm (Sennrich, Haddow, and Birch 2015)\n\ntoken learner and token segmenter\n\n\n\n\nLearner åšçš„äº‹\n\n\n\næˆ‘å€‘ç”¨åŸä½œè€…çš„æŠ•å½±ç‰‡ä¾†èªªæ˜"
  },
  {
    "objectID": "weeks/week3.html#ç·´ç¿’-2",
    "href": "weeks/week3.html#ç·´ç¿’-2",
    "title": "Week 3 Word",
    "section": "ç·´ç¿’",
    "text": "ç·´ç¿’\nä½¿ç”¨ HuggingFace çš„ tokenizer"
  },
  {
    "objectID": "weeks/week3.html#word-vs-morpheme",
    "href": "weeks/week3.html#word-vs-morpheme",
    "title": "Week 3 Word",
    "section": "Word vs Morpheme",
    "text": "Word vs Morpheme\n\nmorphemeï¼šèªè¨€ä¸­æœ€å°çš„æ„ç¾©å–®ä½ã€‚\nwordï¼šèªè¨€ä¸­æœ€å°çš„æ¦‚å¿µå–®ä½ã€‚"
  },
  {
    "objectID": "weeks/week3.html#ä¸­æ–‡çš„è©ç´ æ¦‚å¿µ",
    "href": "weeks/week3.html#ä¸­æ–‡çš„è©ç´ æ¦‚å¿µ",
    "title": "Week 3 Word",
    "section": "ä¸­æ–‡çš„è©ç´ æ¦‚å¿µ",
    "text": "ä¸­æ–‡çš„è©ç´ æ¦‚å¿µ"
  },
  {
    "objectID": "weeks/week3.html#ä¸­æ–‡æ–·è©å•é¡Œ",
    "href": "weeks/week3.html#ä¸­æ–‡æ–·è©å•é¡Œ",
    "title": "Week 3 Word",
    "section": "ä¸­æ–‡æ–·è©å•é¡Œ",
    "text": "ä¸­æ–‡æ–·è©å•é¡Œ\nChinese word segmentation: principles and practices"
  },
  {
    "objectID": "weeks/week3.html#çˆ²ä»€éº¼ç¾åœ¨çš„-chatgpt-ä¸å¤ªæœƒæ–·ç®—è©æœ‰ä¸åˆç†å—",
    "href": "weeks/week3.html#çˆ²ä»€éº¼ç¾åœ¨çš„-chatgpt-ä¸å¤ªæœƒæ–·ç®—è©æœ‰ä¸åˆç†å—",
    "title": "Week 3 Word",
    "section": "çˆ²ä»€éº¼ã€Œç¾åœ¨çš„ã€ chatGPT ä¸å¤ªæœƒæ–·/ç®—è©ï¼Ÿæœ‰ä¸åˆç†å—ï¼Ÿ",
    "text": "çˆ²ä»€éº¼ã€Œç¾åœ¨çš„ã€ chatGPT ä¸å¤ªæœƒæ–·/ç®—è©ï¼Ÿæœ‰ä¸åˆç†å—ï¼Ÿ"
  },
  {
    "objectID": "weeks/week3.html#section-2",
    "href": "weeks/week3.html#section-2",
    "title": "Week 3 Word",
    "section": "",
    "text": "Caution\n\n\n\n\nIn polysynthetic languages like Inuktitut or Mohawk, words can be very long and complex, consisting of multiple morphemes that would be considered separate words in English."
  },
  {
    "objectID": "weeks/week3.html#wordhood-issues",
    "href": "weeks/week3.html#wordhood-issues",
    "title": "Week 3 Word",
    "section": "Wordhood issues",
    "text": "Wordhood issues\n\nå¾ä¸åŒèªè¨€å­¸çš„é ˜åŸŸï¼ˆphonological-prosodic/morphological-grammatical/sociolinguiticï¼‰ä¾†çœ‹ wordï¼Œä¹Ÿé›£æœ‰ä¸€å®šçš„è©(çš„ç•Œç·š)ã€‚ (e.g., hao-le-mei in Mandarin Chinese)\n\n\n\nèªè¨€ç‰¹æ€§çš„å½±éŸ¿ä½¿å¾—åœ¨è¡¨å±¤å½¢å¼çš„è©é•·ä¸ç©©å®šï¼Œè·é›¢ä¹Ÿä¸æ˜ç¢ºã€‚(e.g., Er gibt nicht auf. in German; é›¢åˆè© in Chinese) &gt; In agglutinative languages, for example, multiple morphemes are combined into single words, making it difficult to determine where one word ends and another begins. In these cases, the term for â€˜wordâ€™ might encompass a broader concept than in English.\næœ‰æ™‚ç•¶è©ç´ ï¼Œæœ‰æ™‚ç•¶è© a morpheme behaves like a word on one subset of wordhood parameters but like a bound item on another. (Zingler 2020)"
  },
  {
    "objectID": "weeks/week7.html",
    "href": "weeks/week7.html",
    "title": "Week 7 Syntax",
    "section": "",
<<<<<<< HEAD
    "text": "å¥æ³•ç†è«–èˆ‡è¨ˆç®—è¡¨å¾µ\n\n\n\n\nå¥æ³•å°æ–¼ NLP/NLU å¾ˆé‡è¦ï¼Œå› çˆ²å®ƒæ¶‰åŠèªè¨€åœ¨è¡¨å±¤çš„çµæ§‹é‹ä½œã€‚\nä½†æ˜¯ (å½¢å¼) å¥æ³•åœ¨è¿‘ä»£èªè¨€å­¸ç™¼å±•å²ä¸­ä½”æ“šäº†å¾ˆå¤§ï¼ˆå¤ªå¤§ï¼ï¼‰çš„ä¸€éƒ¨åˆ†\n\n\n\n\n\n\n\nWarning\n\n\n\nN.Chomsky å°æ–¼èªæ–™åº«ï¼Œä¸€ç›´åˆ°ç¾åœ¨çš„ LLM çš„ï¼ˆé„™è¦–ï¼‰æ…‹åº¦éƒ½ä¸€æ¨£çš„ï¼Œçˆ²ä»€éº¼å‘¢ï¼Ÿ 2012 on corpus linguistics; 2023 on chatGPT\n\n\n\n\n\nä¸åŒçš„æ•¸å­¸å“²å­¸\n\n\n\n\n\nä¸–ç´€è«–è¾¯å¿«è¦ç™¼ç”Ÿï¼ˆçµæŸï¼‰ï¼Ÿ\n\n\n\n\n\nFormal syntax/Geneartive Grammar\nTheory\n\nã€å¥æ³•ã€‘æ˜¯ç¬¬ä¸€é †ä½ã€‚In generative grammar, pride of place is given to syntax, ä¹Ÿå…·å‚™è‡ªå·±çš„ã€ç¨ç«‹æ¨¡çµ„ã€‘ã€‚\nã€å½¢å¼è§€ã€‘ï¼šå¿ƒæ™ºä¸­çš„å¥æ³•é‹ç®—å¯ä»¥å®Œå…¨ç¨ç«‹æ–¼æ„ç¾©ä¹‹å¤–ã€‚\n\nData\n\ntop-downï¼Œæœ‰é™è¦å‰‡é§•é¦­ç„¡é™è¡¨é”ï¼Œé‡é»åœ¨æŒæ¡ competence è€Œé performaceã€‚\nå…§çœå¼èªæ„Ÿåˆ¤æ–· rely on introspective judgments as their primary source of data.\nthe poverty-of-stimulus hypothesis (â€œthe child has no dataâ€).\n\n\n\n\nå…©ç¨®åœ¨è¨ˆç®—èªè¨€å­¸ä¸Šå¸¸ä½¿ç”¨çš„å¥æ³•è¡¨å¾µ (representation)ï¼š\n- dependency \n- constituency"
  },
  {
    "objectID": "weeks/week6.html",
    "href": "weeks/week6.html",
    "title": "Week 6 Word Meaning (III)",
    "section": "",
    "text": "è©å½™çš„æƒ…æ„Ÿæ„ç¾©\nè‡ªå‹•æƒ…ç·’åˆ†æ sentiment analysis"
  },
  {
    "objectID": "weeks/week6.html#é€™å€‹ç¯„å¼è½‰ç§»é‚„åœ¨è¿…é€Ÿç™¼ç”Ÿä¸­",
    "href": "weeks/week6.html#é€™å€‹ç¯„å¼è½‰ç§»é‚„åœ¨è¿…é€Ÿç™¼ç”Ÿä¸­",
    "title": "Week 6 Word Meaning (III)",
    "section": "é€™å€‹ç¯„å¼è½‰ç§»é‚„åœ¨è¿…é€Ÿç™¼ç”Ÿä¸­",
    "text": "é€™å€‹ç¯„å¼è½‰ç§»é‚„åœ¨è¿…é€Ÿç™¼ç”Ÿä¸­\nPretrain, Prompt, and Predict (P. Liu et al. 2023)\n\n\n\nParadigm shift in NLP"
  },
  {
    "objectID": "weeks/week6.html#llm-2023-",
    "href": "weeks/week6.html#llm-2023-",
    "title": "Week 6 Word Meaning (III)",
    "section": "LLM 2023-",
    "text": "LLM 2023-\n\n\n\nèªè¨€èˆ‡çŸ¥è­˜è³‡æºä½œçˆ²ä¸€ç¨®ç¬¦ç¢¼å°ˆå®¶æ¨¡å¼æ˜¯åˆç†çš„ (åœ¨ç”¢å­¸æ”»é˜²çš„è„ˆçµ¡äº¦æœ‰å…¶æ„ç¾©)ã€‚ç›®å‰åˆç¨± neuro-symbolic approachã€‚\næ¥ä¸‹ä¾†çš„ Bakeoff (week 13) æˆ‘å€‘æœƒè®“å¤§å®¶ç·´ç¿’å¯¦ä½œé€™å€‹æ–¹å‘ã€‚"
  },
  {
    "objectID": "weeks/week6.html#ä»Šå¤©çš„æ‰“ç®—",
    "href": "weeks/week6.html#ä»Šå¤©çš„æ‰“ç®—",
    "title": "Week 6 Word Meaning (III)",
    "section": "ä»Šå¤©çš„æ‰“ç®—",
    "text": "ä»Šå¤©çš„æ‰“ç®—\n\næˆ‘å€‘ä¸€èµ·è®€é SLP-chap 25ã€‚\nFrameNet data analysis"
  },
  {
    "objectID": "weeks/week6.html#ç›¸é—œçš„æ•´çµ„æ¦‚å¿µ",
    "href": "weeks/week6.html#ç›¸é—œçš„æ•´çµ„æ¦‚å¿µ",
    "title": "Week 6 Word Meaning (III)",
    "section": "ç›¸é—œçš„æ•´çµ„æ¦‚å¿µ",
    "text": "ç›¸é—œçš„æ•´çµ„æ¦‚å¿µ\nåŒ…æ‹¬äº† deceptive opinions (è©é¨™æ„è¦‹)ã€emotion (æƒ…ç·’)ã€sentiment (æƒ…æ„Ÿ)ã€subjectivity (ä¸»è§€æ€§)ã€stance (ç«‹å ´)ã€trustworthiness (å¯ä¿¡åº¦)ã€polarity (æ¥µæ€§)ã€irony (åè«·)ã€sarcasm (å˜²ç¬‘)ã€humor (å¹½é»˜)ã€propaganda (å®£å‚³)ã€bias (åè¦‹)ã€aggression (ä¾µç•¥æ€§)ã€toxicity (æ¯’æ€§)ã€evaluationã€appraisalã€affectã€moodã€feelingsã€beliefs intention ç­‰ç­‰ã€‚\n\n\n\n\n\n\nImportant\n\n\n\nå…ˆåšæ¦‚å¿µçš„å€åˆ†ã€å¯¦ç”¨å‡ºç™¼çš„è©æ„å€åˆ†ï¼Œå†ä¾†åšå¯¦éš›çš„æ‡‰ç”¨ã€‚"
  },
  {
    "objectID": "weeks/week6.html#æ¯”è¼ƒå¸¸è¨è«–çš„æƒ…æ„Ÿåˆ†æ",
    "href": "weeks/week6.html#æ¯”è¼ƒå¸¸è¨è«–çš„æƒ…æ„Ÿåˆ†æ",
    "title": "Week 6 Word Meaning (III)",
    "section": "æ¯”è¼ƒå¸¸è¨è«–çš„æƒ…æ„Ÿåˆ†æ",
    "text": "æ¯”è¼ƒå¸¸è¨è«–çš„æƒ…æ„Ÿåˆ†æ\n\nsentiment is defined as an attitude, thought, or judgment prompted by feeling, whereas opinion is defined as a view, judgment, or appraisal formed in the mind about a particular matter.(B. Liu 2020)\n\n\n\n\n\n\n\nsentiment å’Œ opinion åœ¨èªè¨€ä½¿ç”¨ä¸Šçš„å·®ç•°\n\n\n\n\næˆ‘æœ‰é»æ“”å¿ƒä»Šå¹´å…¨çƒçš„ç¶“æ¿Ÿã€‚ï¼ˆæˆ‘ä¹Ÿæ˜¯/æˆ‘ä¹Ÿè »æ“”å¿ƒçš„ï¼‰\næˆ‘èªçˆ²ä»Šå¹´çš„ç¶“æ¿Ÿä¸æœƒå¾ˆå¥½ã€‚ï¼ˆæˆ‘åŒæ„/ä¸åŒæ„ï¼‰"
  },
  {
    "objectID": "weeks/week6.html#levels-of-analysis",
    "href": "weeks/week6.html#levels-of-analysis",
    "title": "Week 6 Word Meaning (III)",
    "section": "Levels of Analysis",
    "text": "Levels of Analysis\n\ndocument-level (æ–‡æª”ç´š)\nsentence-level (å¥å­ç´š)\naspect-level (æ–¹é¢ç´š)\n\nå¯¦ä½œä¸Šå¤§éƒ¨åˆ†éƒ½ç•¶æˆï¼ˆé¡åˆ¥ classã€æ¥µåº¦ polarityï¼‰åˆ†é¡å•é¡Œã€‚Aspect-based Sentiment Analysis é›£åº¦æ›´é«˜ä¸€é» (Zhang et al. 2022)ã€‚\n\n\n\nABSA"
  },
  {
    "objectID": "weeks/week6.html#ä½†å¯èƒ½ä¹Ÿæ²’é‚£éº¼é›£äº†",
    "href": "weeks/week6.html#ä½†å¯èƒ½ä¹Ÿæ²’é‚£éº¼é›£äº†",
    "title": "Week 6 Word Meaning (III)",
    "section": "ä½†å¯èƒ½ä¹Ÿæ²’é‚£éº¼é›£äº†",
    "text": "ä½†å¯èƒ½ä¹Ÿæ²’é‚£éº¼é›£äº†"
  },
  {
    "objectID": "weeks/week6.html#å¤šæ¨¡æ…‹çš„æƒ…æ„Ÿè¨ˆç®—",
    "href": "weeks/week6.html#å¤šæ¨¡æ…‹çš„æƒ…æ„Ÿè¨ˆç®—",
    "title": "Week 6 Word Meaning (III)",
    "section": "å¤šæ¨¡æ…‹çš„æƒ…æ„Ÿè¨ˆç®—",
    "text": "å¤šæ¨¡æ…‹çš„æƒ…æ„Ÿè¨ˆç®—\nmultimodal affective computing"
=======
    "text": "é€±æ¬¡\næ—¥æœŸ\nä¸»é¡Œ\né ç¿’ ğŸ“–\næŠ•å½±ç‰‡\nåŠ©æ•™èª²\nèª²å ‚ç·´ç¿’\nä½œæ¥­\n\n\n\n\n1\n02/23\nèª²ç¨‹å°è«–\n\norientation\n\n\n\n\n\n2\n03/02\nè¨ˆç®—èªè¨€å­¸å°è«–\n\nw2\nslides\ncolab\nHW1, HW1_Ans\n\n\n3\n03/09\nè¨ˆç®—æ§‹è©èˆ‡è©å½™ (1)\nslp3-02\nw3\nslides\ncolab\nHW2, HW2_Ans\n\n\n4\n03/16\nè¨ˆç®—æ§‹è©èˆ‡è©å½™ (2)\nslp3-23\nw4\n\ncolab\nHW3, HW3_Ans\n\n\n5\n03/23\nè¨ˆç®—æ§‹è©èˆ‡è©å½™ (3)\nslp3-21\nw5\nguidline\ncolab, gsheet\nHW4\n\n\n6\n03/30\nè¨ˆç®—æ§‹è©èˆ‡è©å½™ (4)\nslp3-25\nw6\n\ncolab\nHW5\n\n\n7\n04/06\nå¥æ³•ç†è«–èˆ‡å‰–æå™¨\nslp3-18; (Linzen and Baroni, 2021)\n\n\n\n\n\n\n8\n04/13\nèªçŸ¥èªæ³•èˆ‡è¨ˆç®—\ncognitive linguistics meets computational linguistics\n\n\n\n\n\n\n9\n04/20\nèªæ„è¡¨å¾µ (1)\nslp3-06\n\n\n\n\n\n\n10\n04/27\nèªæ„è¡¨å¾µ (2)\nslp3-10\n\n\n\n\n\n\n11\n05/04\nèªæ„è¡¨å¾µ (3) & æ¥­å¸«åˆ†äº«\nslp3-11\n\n\n\n\n\n\n12\n05/11\nèªæ„è¡¨å¾µ (4)\nslp3-22\n\n\n\n\n\n\n13\n05/18\nBakeoff practice\n\n\n\n\n\n\n\n14\n05/25\nèªéŸ³ã€å¤šæ¨¡æ…‹èˆ‡è¨€è«‡åˆ†æ\nslp3-15; slp3-28\n\n\n\n\n\n\n15\n06/01\nç¶œåˆè¨è«–\n\n\n\n\n\n\n\n16\n06/08\næœŸæœ«å ±å‘Š"
>>>>>>> 5df83516aa504510ede50f845d5c32518bc23750
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, youâ€™re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and thatâ€™s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called â€œReferencesâ€ at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called â€œAppendixâ€.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf youâ€™re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\nâŒ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\nâœ… YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDonâ€™t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\nâŒ There is a negative linear relationship between mpg and hp.\nâœ… There is a negative linear relationship between a carâ€™s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Donâ€™t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the â€œso whatâ€: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e.Â what you want the audience to know about your topic after reading your report or viewing your presentation.\n\nâŒ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\nâœ… If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if itâ€™s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in EquationÂ 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in EquationÂ 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the EquationÂ 1 and EquationÂ 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e.Â \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, EquationÂ 6 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes EquationÂ 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{7}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{8}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e.Â \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write EquationÂ 8 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{9}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{10}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{11}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cookâ€™s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in EquationÂ 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in EquationÂ 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the EquationÂ 1 and EquationÂ 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining EquationÂ 4 and EquationÂ 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus EquationÂ 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e.Â outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from EquationÂ 4 using EquationÂ 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e.Â \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and EquationÂ 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e.Â \\(E(e_i) = 0\\). From EquationÂ 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cookâ€™s Distance",
    "text": "Cookâ€™s Distance\nCookâ€™s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cookâ€™s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cookâ€™s Distance can be calculated without deleting observations one at a time, since EquationÂ 12 below is mathematically equivalent to EquationÂ 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in EquationÂ 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of EquationÂ 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in EquationÂ 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in EquationÂ 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in EquationÂ 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form thatâ€™s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying EquationÂ 5 and EquationÂ 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into EquationÂ 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in EquationÂ 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in EquationÂ 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in EquationÂ 2 for interpretations and predictions, we will use EquationÂ 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing EquationÂ 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.Â \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in EquationÂ 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e.Â \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr.Â Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaikeâ€™s Information Criterion (AIC) and Schwarzâ€™s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from EquationÂ 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using EquationÂ 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in EquationÂ 2, i.e.Â maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e.Â the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in EquationÂ 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaikeâ€™s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, letâ€™s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Exportâ€¦ If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If youâ€™ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Exportâ€¦ If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If youâ€™ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, youâ€™ll upload your PDF and them mark the page(s) where each question can be found. Itâ€™s OK if a question spans multiple pages, just mark them all. Itâ€™s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, Iâ€™d rather you didnâ€™t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages weâ€™re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when youâ€™re working in the containers we have provided for you. If youâ€™re working on your local setup, we canâ€™t guarantee being able to resolve your issues, though weâ€™re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd Iâ€™d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyoneâ€™s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that canâ€™t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr.Â Mine Ã‡etinkaya-Rundel at mc301@duke.edu.\nIf there is a question thatâ€™s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include â€œSTA 210â€ in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a studentâ€™s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Dukeâ€™s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\nğŸ€ Good luck! ğŸ€\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for â€œInstructors in this siteâ€ only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the â€œWorkflow & formattingâ€ section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-1.html#overview",
    "href": "exams/exam-1.html#overview",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\nğŸ€ Good luck! ğŸ€"
  },
  {
    "objectID": "exams/exam-1.html#academic-integrity",
    "href": "exams/exam-1.html#academic-integrity",
    "title": "Exam 1",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-1.html#rules-notes",
    "href": "exams/exam-1.html#rules-notes",
    "title": "Exam 1",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for â€œInstructors in this siteâ€ only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-1.html#submission",
    "href": "exams/exam-1.html#submission",
    "title": "Exam 1",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the â€œWorkflow & formattingâ€ section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "weeks/week7.html#defining-dependencies",
    "href": "weeks/week7.html#defining-dependencies",
    "title": "Week 7 Syntax",
    "section": "Defining dependencies",
    "text": "Defining dependencies\n\n\n\nç”±æ³•åœ‹èªè¨€å­¸å®¶ Lucien TesniÃ¨re æå‡ºã€‚\nå°æ–¼èªåºè¼ƒçˆ²è‡ªç”±çš„ã€æ§‹è©è±å¯Œçš„ (morphologically rich languages) èªè¨€ï¼Œrule-based approach å¾ˆé›£æ‡‰ä»˜ã€‚(ä»¥ polymorphic languages çˆ²ä¾‹)\næ­¤å¤–ï¼Œæ‰¾åˆ°äº†ä¸»è¦èª (head) åŠå…¶ä¾å­˜ (dependent) æœ‰åŠ©æ–¼èªæ„å‰–æ (semantic parsing)ã€‚ï¼ˆå¯ä»¥å®¹ç´å¥æ³•å’Œèªæ„çµæ§‹ä¸¦å­˜ï¼‰\n\n(Gerdes, HajiÄovÃ¡, and Wanner 2013)"
  },
  {
    "objectID": "weeks/week7.html#dependency",
    "href": "weeks/week7.html#dependency",
    "title": "Week 7 Syntax",
    "section": "",
    "text": "ä¸€å€‹å¥å­çš„å¥æ³•çµæ§‹å¯ä»¥ç”¨ä¸€å€‹æœ‰å‘åœ– (directed graph) ä¾†è¡¨ç¤º\né€™å€‹åœ–çš„ç¯€é» (node) å°±æ˜¯å¥å­ä¸­çš„è© (word)\né€™å€‹åœ–çš„é‚Š (edge) å°±æ˜¯è©èˆ‡è©ä¹‹é–“çš„ä¾å­˜é—œä¿‚ (dependency relation)\né€™å€‹åœ–çš„æ ¹ (root) å°±æ˜¯å¥å­çš„ä¸»èª (subject)\né€™å€‹åœ–çš„æ ¹ä»¥å¤–çš„ç¯€é» (non-root node) å°±æ˜¯å¥å­çš„ä¿®é£¾èª (modifier)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "weeks/week7.html#dependency-çš„å®šç¾©",
    "href": "weeks/week7.html#dependency-çš„å®šç¾©",
    "title": "Week 7 Syntax",
    "section": "Dependency çš„å®šç¾©",
    "text": "Dependency çš„å®šç¾©\n\nä¸€å€‹å¥å­çš„ dependency tree æ˜¯ä¸€å€‹æœ‰å‘ç„¡ç’°åœ–ï¼Œå…¶ä¸­æ¯å€‹ç¯€é»ä»£è¡¨ä¸€å€‹è©ï¼Œæ¯å€‹é‚Šä»£è¡¨ä¸€å€‹ä¾å­˜é—œä¿‚ (dependency relation)ã€‚\nä¾å­˜é—œä¿‚æ˜¯ä¸€å€‹äºŒå…ƒé—œä¿‚ï¼Œå…¶ä¸­ä¸€å€‹å…ƒç´ æ˜¯ headï¼Œå¦ä¸€å€‹å…ƒç´ æ˜¯ dependentã€‚"
  },
  {
    "objectID": "weeks/week7.html#comparison",
    "href": "weeks/week7.html#comparison",
    "title": "Week 7 Syntax",
    "section": "Comparison",
    "text": "Comparison"
  },
  {
    "objectID": "weeks/week7.html#constituency-çš„å®šç¾©",
    "href": "weeks/week7.html#constituency-çš„å®šç¾©",
    "title": "Week 7 Syntax",
    "section": "## Constituency çš„å®šç¾©",
    "text": "## Constituency çš„å®šç¾©\n\n\n\n\n\n\nCaution\n\n\n\nå¾ä¸–ç•Œèªè¨€ä¾†çœ‹ï¼Œé€™å€‹æ¦‚å¿µæ˜¯å¾ˆå€¼å¾—çˆ­è­°çš„ã€‚"
  },
  {
    "objectID": "weeks/week7.html#section",
    "href": "weeks/week7.html#section",
    "title": "Week 7 Syntax",
    "section": "",
    "text": "each node is connected to other nodes through directed edges, which indicate the dependency between the words. The direction of the edge typically goes from the head (governor) to the dependent (modifier).\nThis representation helps to identify the structure and hierarchical relations within a sentence, making it easier to understand and process."
  },
  {
    "objectID": "weeks/week7.html#modeling-constituency",
    "href": "weeks/week7.html#modeling-constituency",
    "title": "Week 7 Syntax",
    "section": "Modeling constituency",
    "text": "Modeling constituency\n\nç›®å‰æœ€çˆ²å»£æ³›ç”¨ä¾† model constituency çš„å½¢å¼ç³»çµ±å«åš Context-free Grammar (CfG)ï¼Œ åœ¨èªè¨€å­¸ä¸­ç¨± Phrase Structure Grammar (PSG) ã€‚\nç”± Chomsky åœ¨ 1950 å¹´ä»£æå‡ºï¼Œæ˜¯ä¸€ç¨®ç”¨ä¾†æè¿°èªè¨€å¥æ³•çµæ§‹çš„å½¢å¼ç³»çµ±ã€‚"
  },
  {
    "objectID": "weeks/week7.html#context-free-grammar",
    "href": "weeks/week7.html#context-free-grammar",
    "title": "Week 7 Syntax",
    "section": "Context-free Grammar",
    "text": "Context-free Grammar\n\na set of rules (or productions) that describe how to construct a sentence from smaller units.\nå½¢å¼ä¸Šå®šç¾©ï¼ŒCfG æ˜¯ä¸€å€‹å››å…ƒçµ„ (4-tuple) \\((N, \\Sigma, R, S)\\)ï¼Œå…¶ä¸­ï¼š\n\n\\(N\\) æ˜¯ä¸€å€‹æœ‰é™é›†åˆï¼Œç¨±ç‚º non-terminal symbolsï¼Œä»£è¡¨å¥æ³•çµæ§‹çš„çµ„æˆéƒ¨åˆ†ã€‚\n\\(\\Sigma\\) æ˜¯ä¸€å€‹æœ‰é™é›†åˆï¼Œç¨±ç‚º terminal symbolsï¼Œä»£è¡¨å¥å­ä¸­çš„è©ã€‚\n\\(R\\) æ˜¯ä¸€å€‹æœ‰é™é›†åˆï¼Œç¨±ç‚º rulesï¼Œä»£è¡¨å¥æ³•çµæ§‹çš„ç”Ÿæˆè¦å‰‡ã€‚\n\\(S\\) æ˜¯ä¸€å€‹ \\(N\\) ä¸­çš„å…ƒç´ ï¼Œç¨±ç‚º start symbolï¼Œä»£è¡¨å¥å­çš„æ ¹ç¯€é»ã€‚"
  },
  {
    "objectID": "weeks/week7.html#ä¾‹å­",
    "href": "weeks/week7.html#ä¾‹å­",
    "title": "Week 7 Syntax",
    "section": "ä¾‹å­",
    "text": "ä¾‹å­\n\n\n\nå¦³å¿«æŠŠæˆ‘ç¬‘æ­»äº†\n\n\nå¦‚ä½•è§£æä»¥ä¸‹å¥å­ï¼Ÿ\n\nä»–èªªå¾—éƒ½ä¸å°"
  },
  {
    "objectID": "weeks/week7.html#parsing",
    "href": "weeks/week7.html#parsing",
    "title": "Week 7 Syntax",
    "section": "Parsing",
    "text": "Parsing\nå¥æ³•å‰–æ\n\n(è‡ªå‹•åœ°) æŠŠä¸€å€‹å¥å­ (a string of word) æ˜ å°„åˆ°å…¶å°æ‡‰çš„å¥æ³•çµæ§‹ (a parse tree)ï¼Œå°±å«åš parsingã€‚"
  },
  {
    "objectID": "weeks/week7.html#treebanks",
    "href": "weeks/week7.html#treebanks",
    "title": "Week 7 Syntax",
    "section": "Treebanks",
    "text": "Treebanks\n\nä¸€å€‹ treebank æ˜¯ä¸€å€‹èªæ–™åº«ï¼Œå…¶ä¸­çš„æ¯å€‹å¥å­éƒ½æœ‰ä¸€å€‹å°æ‡‰çš„å¥æ³•æ¨¹ (parse tree)ã€‚\né€™äº›å¥æ³•æ¨¹é€šå¸¸æ˜¯ç”±è¨“ç·´éçš„èªè¨€å­¸å®¶æ¨™è¨»ã€æˆ–ä¿®è¨‚éçš„ã€‚\n\n\n\nPenn Treebank\nCKIP Treebank"
  },
  {
    "objectID": "weeks/week7.html#ä¾‹å­-1",
    "href": "weeks/week7.html#ä¾‹å­-1",
    "title": "Week 7 Syntax",
    "section": "ä¾‹å­",
    "text": "ä¾‹å­"
  },
  {
    "objectID": "weeks/week7.html#formal-language-and-chomsky-normal-form",
    "href": "weeks/week7.html#formal-language-and-chomsky-normal-form",
    "title": "Week 7 Syntax",
    "section": "Formal language and Chomsky normal form",
    "text": "Formal language and Chomsky normal form\n\nIt is useful to have a normal from (i.e., each of the production/rule taks a particular form) for formal languages, so that we can compare different formal languages and their grammars.\nChomsky normal form (CNF) is a normal form for context-free grammars.\n\nA grammar is in CNF if all its rules are of the form:\n\n\\(A \\rightarrow BC\\)\n\\(A \\rightarrow a\\)\n\\(S \\rightarrow \\epsilon\\)\n\n\n\ncf.Â å½¢å¼èªè¨€çš„è¨ˆç®—ç†è«–\n\nChomsky-adjunction"
  },
  {
    "objectID": "weeks/week7.html#structural-ambiguity",
    "href": "weeks/week7.html#structural-ambiguity",
    "title": "Week 7 Syntax",
    "section": "Structural Ambiguity",
    "text": "Structural Ambiguity\nçµæ§‹æ­§ç¾©\n\nä¸€å€‹å¥å­çš„å¥æ³•çµæ§‹å¯èƒ½æœ‰å¤šç¨®ä¸åŒçš„è§£ææ–¹å¼ï¼Œé€™å°±æ˜¯çµæ§‹æ­§ç¾© (structural ambiguity)ã€‚\næœ€å¸¸è¦‹çš„çµæ§‹æ­§ç¾©é¡å‹ï¼šé™„åŠ  (attachment ambiguity) å’Œä¸¦åˆ— (coordination ambiguity)ã€‚\n\n\nI shot an elephant in my pajamas"
  },
  {
    "objectID": "weeks/week7.html#å¥æ³•çš„è¨ˆç®—è¡¨å¾µ",
    "href": "weeks/week7.html#å¥æ³•çš„è¨ˆç®—è¡¨å¾µ",
    "title": "Week 7 Syntax",
    "section": "",
    "text": "å…©ç¨®åœ¨è¨ˆç®—èªè¨€å­¸ä¸Šå¸¸ä½¿ç”¨çš„å¥æ³•è¡¨å¾µ (representation)ï¼š\n- dependency \n- constituency"
  },
  {
    "objectID": "weeks/week7.html#ç·´ç¿’ä¸­æ–‡æ­§ç¾©å¥",
    "href": "weeks/week7.html#ç·´ç¿’ä¸­æ–‡æ­§ç¾©å¥",
    "title": "Week 7 Syntax",
    "section": "ç·´ç¿’ä¸­æ–‡æ­§ç¾©å¥",
    "text": "ç·´ç¿’ä¸­æ–‡æ­§ç¾©å¥\n\n\\(VP + NP_{1} + de + NP_{2}\\)\n\\(A + NP_{1} + NP_{2}\\)"
  },
  {
    "objectID": "weeks/week7.html#cky-parsing-algorithm",
    "href": "weeks/week7.html#cky-parsing-algorithm",
    "title": "Week 7 Syntax",
    "section": "CKY Parsing algorithm",
    "text": "CKY Parsing algorithm"
  },
  {
    "objectID": "weeks/week7.html#dynamic-programming",
    "href": "weeks/week7.html#dynamic-programming",
    "title": "Week 7 Syntax",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming"
  },
  {
    "objectID": "weeks/week7.html#evaluation",
    "href": "weeks/week7.html#evaluation",
    "title": "Week 7 Syntax",
    "section": "Evaluation",
    "text": "Evaluation\nç†æƒ³ä¸Šçš„ exact match æŒ‡æ¨™æ˜¯ä¸å¤ªå¯è¡Œçš„ã€‚\n\nLabeled Attachment Score (LAS): the ratio of correctly detected Head-Dependent pairs along with their tag / total Head-Dependent pairs in testing data.\nUnlabeled Attachment Score (UAS): the ratio of correctly detected Head_Dependent pairs (irrespective of the tag) / total Head-Dependent pairs in testing data.\n\n\n\nReference æŒ‡çš„æ˜¯æ¨™æº–ç­”æ¡ˆï¼ŒSystem æŒ‡çš„æ˜¯ç³»çµ±ç”Ÿæˆçš„ç­”æ¡ˆã€‚\nTotal head-dependent pairs æŒ‡çš„æ˜¯å¥å­ä¸­æ‰€æœ‰çš„ Head-Dependent pair, ä¸ç®¡åœ¨ Reference æˆ– System éƒ½æ˜¯ 6 ã€‚\nTotal Head-Dependent pairs correctly detected with tags = 4 (å› çˆ² System ä¸­çš„ nsubj å’Œ det éƒ½æ˜¯æ­£ç¢ºçš„)ã€‚\nTotal Head-Dependent pairs correctly detected = 5 (å› çˆ²åªç®¡ pairsï¼Œä¸ç®¡ tagsï¼Œæ‰€ä»¥ book \\(\\rightarrow\\) flight ä¹Ÿç®—)ã€‚"
  },
  {
    "objectID": "weeks/week7.html#dependency-relations",
    "href": "weeks/week7.html#dependency-relations",
    "title": "Week 7 Syntax",
    "section": "Dependency relations",
    "text": "Dependency relations\n\n\né—œä¿‚ç¨®é¡æœ‰å¾ˆå¤šåˆ†é¡æ–¹å¼ï¼Œé€™è£æ¡å– Universal Dependencies çš„åˆ†é¡æ–¹å¼ã€‚\n\n\n\n\nUniversal Dependencies"
  },
  {
    "objectID": "weeks/week7.html#dependency-formalisms",
    "href": "weeks/week7.html#dependency-formalisms",
    "title": "Week 7 Syntax",
    "section": "Dependency Formalisms",
    "text": "Dependency Formalisms\n\nåœ–è«–ä¸Šä¾†çœ‹ï¼Œdependencies can be represented as a directed graph \\(G= (V, A)\\) where V(set of vertices) represents words (and punctuation marks as well) in the sentence & A( set of arcs) represent the grammar relationship between elements of V.\n\n\n\nA dependency parse tree is the directed graph which has the below features:\n\nRoot æ²’æœ‰å‚³å…¥çš„å¼§ (incoming arc) (can only be Head in Head-Dependent pair)\né™¤äº† root ä¹‹å¤–çš„æ¯ä¸€å€‹ç¯€é»ï¼Œæ‡‰è©²éƒ½åªæœ‰ä¸€æ¢å‚³å…¥çš„å¼§ (Only one Parent/Head)\nå¾ root åˆ°æ¯ä¸€å€‹ç¯€é»éƒ½åªæœ‰ä¸€æ¢è·¯ã€‚"
  },
  {
    "objectID": "weeks/week7.html#projectivity",
    "href": "weeks/week7.html#projectivity",
    "title": "Week 7 Syntax",
    "section": "Projectivity",
    "text": "Projectivity\n\nProjective arc: An arc/arrows_with_tag are projective when â€˜Headâ€™ associated with the arc has a path to reach each word that lies between â€˜Headâ€™ & â€˜Dependentâ€™.\n\nä»¥ (18.2) çˆ²ä¾‹ï¼Œthe å’Œ flights çš„ arc æ˜¯ projectiveï¼Œå› çˆ²åœ¨æ­¤çš„ Head (å³ flights) åˆ°å®ƒçš„ Dependent (å³ the) ä¹‹é–“æœ‰å€‹ morning é€™å€‹è©ï¼Œ Head ä¹Ÿæœ‰è·¯å¯åˆ°å¾—äº†ã€‚\nåŒæ¨£åœ°ï¼Œâ€˜canceledâ€™ (HEAD) åˆ° â€˜flightsâ€™ (DEPENDENT) é€™æ¢ arc ä¹Ÿæ˜¯ projectiveï¼Œå› çˆ² Head æœ‰è·¯å¯åˆ°é”å®ƒå’Œå…¶ Dependent ä¹‹é–“çš„è©ã€‚(i.e., â€˜theâ€™ (canceled \\(\\rightarrow\\) flights \\(\\rightarrow\\) the) ä»¥åŠ â€˜morningâ€™ (canceled \\(\\rightarrow\\) flights \\(\\rightarrow\\) morning))\n\n\nProjective parse tree: A parse tree with all its arcs projective. The above tree is projective; A tree with at least one of the arcs as non-projective is called non-projective parse tree."
  },
  {
    "objectID": "weeks/week7.html#graph-based-dependency-parsing",
    "href": "weeks/week7.html#graph-based-dependency-parsing",
    "title": "Week 7 Syntax",
    "section": "Graph-based Dependency Parsing",
    "text": "Graph-based Dependency Parsing\nCreating dependency structures based on the use of maximum spanning tree methods from graph theory."
  },
  {
    "objectID": "weeks/week7.html#chu-liuedmonds-algorithm",
    "href": "weeks/week7.html#chu-liuedmonds-algorithm",
    "title": "Week 7 Syntax",
    "section": "Chu-Liu/Edmondsâ€™ Algorithm",
    "text": "Chu-Liu/Edmondsâ€™ Algorithm\nè§£ [2] çš„æ–¹æ³•\n\nIt turns out that finding the best dependency parse for \\(S\\) is equivalent to finding the maximum spanning tree over \\(G\\).\n\n\nChu-Liu/Edmondsâ€™ Algorithm is a greedy algorithm for finding the maximum spanning tree of a graph."
  },
  {
    "objectID": "weeks/week7.html#evaluation-1",
    "href": "weeks/week7.html#evaluation-1",
    "title": "Week 7 Syntax",
    "section": "Evaluation",
    "text": "Evaluation\nç†æƒ³ä¸Šçš„ exact match æŒ‡æ¨™æ˜¯ä¸å¤ªå¯è¡Œçš„ã€‚\n\nLabeled Attachment Score (LAS): the ratio of correctly detected Head-Dependent pairs along with their tag / total Head-Dependent pairs in testing data.\nUnlabeled Attachment Score (UAS): the ratio of correctly detected Head_Dependent pairs (irrespective of the tag) / total Head-Dependent pairs in testing data.\n\n\n\nReference æŒ‡çš„æ˜¯æ¨™æº–ç­”æ¡ˆï¼ŒSystem æŒ‡çš„æ˜¯ç³»çµ±ç”Ÿæˆçš„ç­”æ¡ˆã€‚\nTotal head-dependent pairs æŒ‡çš„æ˜¯å¥å­ä¸­æ‰€æœ‰çš„ Head-Dependent pair, ä¸ç®¡åœ¨ Reference æˆ– System éƒ½æ˜¯ 6 ã€‚\nTotal Head-Dependent pairs correctly detected with tags = 4 (å› çˆ² System ä¸­çš„ nsubj å’Œ det éƒ½æ˜¯æ­£ç¢ºçš„)ã€‚\nTotal Head-Dependent pairs correctly detected = 5 (å› çˆ²åªç®¡ pairsï¼Œä¸ç®¡ tagsï¼Œæ‰€ä»¥ book \\(\\rightarrow\\) flight ä¹Ÿç®—)ã€‚"
  },
  {
    "objectID": "weeks/week7.html#å¹¾å€‹æ¦‚å¿µ",
    "href": "weeks/week7.html#å¹¾å€‹æ¦‚å¿µ",
    "title": "Week 7 Syntax",
    "section": "å¹¾å€‹æ¦‚å¿µ",
    "text": "å¹¾å€‹æ¦‚å¿µ\n\nHead-Dependent çš„ç®­é ­æ–¹å‘ï¼šthe origin word is the Head and the destination word is Dependent. (e.g., â€˜preferâ€™ is Head & â€˜Iâ€™ is Dependent.)\nRoot: Word which is the root of our parse tree. (It is â€˜preferâ€™ in the above example).\nGrammar Functions and Arcs: Tags between each Head-Dependent pair is a grammar function determining the relation between the Head & Dependent. The arrowhead carrying the tag is called an Arc."
  },
  {
    "objectID": "weeks/week7.html#practice",
    "href": "weeks/week7.html#practice",
    "title": "Week 7 Syntax",
    "section": "Practice",
    "text": "Practice\nä»¥ä¸‹é€™å€‹ dependency parse tree æ˜¯ projective or non-projective?"
  },
  {
    "objectID": "weeks/week7.html#å…ˆè¦ç­è§£ä½•è¬‚æœ€å¤§æ“´å¼µæ¨¹-maximum-spanning-tree",
    "href": "weeks/week7.html#å…ˆè¦ç­è§£ä½•è¬‚æœ€å¤§æ“´å¼µæ¨¹-maximum-spanning-tree",
    "title": "Week 7 Syntax",
    "section": "å…ˆè¦ç­è§£ä½•è¬‚æœ€å¤§æ“´å¼µæ¨¹ (Maximum Spanning Tree)",
    "text": "å…ˆè¦ç­è§£ä½•è¬‚æœ€å¤§æ“´å¼µæ¨¹ (Maximum Spanning Tree)\n\n\n\nMST"
  },
  {
    "objectID": "weeks/week7.html#graph-based-dependency-parsing-1",
    "href": "weeks/week7.html#graph-based-dependency-parsing-1",
    "title": "Week 7 Syntax",
    "section": "Graph-based Dependency Parsing",
    "text": "Graph-based Dependency Parsing\næ­¸çµåˆ°å…©å€‹å•é¡Œï¼š\n[1] assigning a score to each edge çµ¦é‚Šåˆ†æ•¸\n[2] finding the best parse tree given the scores of all potential edges. æ‰¾å‡ºæœ€å¥½çš„ä¾å­˜æ¨¹"
  },
  {
    "objectID": "weeks/week7.html#èˆ‰ä¾‹",
    "href": "weeks/week7.html#èˆ‰ä¾‹",
    "title": "Week 7 Syntax",
    "section": "èˆ‰ä¾‹",
    "text": "èˆ‰ä¾‹"
  },
  {
    "objectID": "weeks/week7.html#å¥æ³•-syntax",
    "href": "weeks/week7.html#å¥æ³•-syntax",
    "title": "Week 7 Syntax",
    "section": "",
    "text": "å¥æ³•å°æ–¼ NLP/NLU å¾ˆé‡è¦ï¼Œå› çˆ²å®ƒæ¶‰åŠèªè¨€åœ¨è¡¨å±¤çš„çµæ§‹é‹ä½œã€‚\nä½†æ˜¯ (å½¢å¼) å¥æ³•åœ¨è¿‘ä»£èªè¨€å­¸ç™¼å±•å²ä¸­ä½”æ“šäº†å¾ˆå¤§ï¼ˆå¤ªå¤§ï¼ï¼‰çš„ä¸€éƒ¨åˆ†\n\n\n\n\n\n\n\nWarning\n\n\n\nN.Chomsky å°æ–¼èªæ–™åº«ï¼Œä¸€ç›´åˆ°ç¾åœ¨çš„ LLM çš„ï¼ˆé„™è¦–ï¼‰æ…‹åº¦éƒ½ä¸€æ¨£çš„ï¼Œçˆ²ä»€éº¼å‘¢ï¼Ÿ 2012 on corpus linguistics; 2023 on chatGPT\n\n\n\n\n\nä¸åŒçš„æ•¸å­¸å“²å­¸\n\n\n\n\n\nä¸–ç´€è«–è¾¯å¿«è¦ç™¼ç”Ÿï¼ˆçµæŸï¼‰ï¼Ÿ"
  },
  {
    "objectID": "weeks/week7.html#å½¢å¼å¥æ³•",
    "href": "weeks/week7.html#å½¢å¼å¥æ³•",
    "title": "Week 7 Syntax",
    "section": "",
    "text": "Formal syntax/Geneartive Grammar\nTheory\n\nã€å¥æ³•ã€‘æ˜¯ç¬¬ä¸€é †ä½ã€‚In generative grammar, pride of place is given to syntax, ä¹Ÿå…·å‚™è‡ªå·±çš„ã€ç¨ç«‹æ¨¡çµ„ã€‘ã€‚\nã€å½¢å¼è§€ã€‘ï¼šå¿ƒæ™ºä¸­çš„å¥æ³•é‹ç®—å¯ä»¥å®Œå…¨ç¨ç«‹æ–¼æ„ç¾©ä¹‹å¤–ã€‚\n\nData\n\ntop-downï¼Œæœ‰é™è¦å‰‡é§•é¦­ç„¡é™è¡¨é”ï¼Œé‡é»åœ¨æŒæ¡ competence è€Œé performaceã€‚\nå…§çœå¼èªæ„Ÿåˆ¤æ–· rely on introspective judgments as their primary source of data.\nthe poverty-of-stimulus hypothesis (â€œthe child has no dataâ€)."
  },
  {
    "objectID": "weeks/week7.html#è™•ç†å½¢å¼ä¸Šçš„å„ç¨®ç©ºç¼ºå•é¡Œ",
    "href": "weeks/week7.html#è™•ç†å½¢å¼ä¸Šçš„å„ç¨®ç©ºç¼ºå•é¡Œ",
    "title": "Week 7 Syntax",
    "section": "è™•ç†å½¢å¼ä¸Šçš„å„ç¨®ç©ºç¼ºå•é¡Œ",
    "text": "è™•ç†å½¢å¼ä¸Šçš„å„ç¨®ç©ºç¼ºå•é¡Œ\nå‹•è©çš„è«–å…ƒè„«è½æœ‰ä¸åŒçš„ç¬¦ç¢¼æ¨™è¨˜\n\nç§»ä½ (é‚£å€‹è˜‹æœï¼Œæˆ‘æ‰”äº†)\néš±å«ï¼ˆå¥¹æ‰“ç®—æ‰“ç¶²çƒï¼‰\nç©ºæ ¼ï¼ˆä»–è²·äº†å…©æ ¹é¦™è•‰ï¼Œçµ¦äº†ä»–æœ‹å‹ä¸€æ ¹ï¼‰"
  },
  {
    "objectID": "weeks/week7.html#cky-parsing-algorithms",
    "href": "weeks/week7.html#cky-parsing-algorithms",
    "title": "Week 7 Syntax",
    "section": "CKY Parsing algorithms",
    "text": "CKY Parsing algorithms\n\nDynamic Programming\nEvaluation"
  },
  {
    "objectID": "weeks/week7.html#å‹•è©çš„é…åƒ¹ç†è«–",
    "href": "weeks/week7.html#å‹•è©çš„é…åƒ¹ç†è«–",
    "title": "Week 7 Syntax",
    "section": "å‹•è©çš„é…åƒ¹ç†è«–",
    "text": "å‹•è©çš„é…åƒ¹ç†è«–\nä¸»è¦èªæˆ–ä¸­å¿ƒèª\n\nä¾†è‡ªåŒ–å­¸å…ƒç´ çš„é¡æ¯”ï¼šå¥å­åƒæ˜¯åˆ†å­ï¼Œç”±åŸå­çµ„æˆï¼Œè€ŒåŸå­çš„æ ¸å¿ƒæ˜¯é…åƒ¹ (valence) ï¼ˆæŒ‰ä¸€å®šçš„åƒ¹çµåˆåœ¨ä¸€èµ·ï¼‰ã€‚\nå‹•è©æ˜¯ä¸­å¿ƒï¼Œæ‰€æœ‰æˆåˆ†éƒ½æ˜¯ç’°ç¹ç€ä¸­å¿ƒå‹•è©é–‹å±•çš„ã€‚\nåƒ¹æ•¸ã€åƒ¹è³ªï¼ˆsemantic rolesï¼‰ã€åƒ¹å½¢ (syntactic position)"
  },
  {
    "objectID": "weeks/week7.html#dependency-çš„å½¢å¼å®šç¾©",
    "href": "weeks/week7.html#dependency-çš„å½¢å¼å®šç¾©",
    "title": "Week 7 Syntax",
    "section": "Dependency çš„å½¢å¼å®šç¾©",
    "text": "Dependency çš„å½¢å¼å®šç¾©\n\nä¸€å€‹å¥å­çš„ dependency tree æ˜¯ä¸€å€‹æœ‰å‘ç„¡ç’°åœ–ï¼Œå…¶ä¸­æ¯å€‹ç¯€é»ä»£è¡¨ä¸€å€‹è©ï¼Œæ¯å€‹é‚Šä»£è¡¨ä¸€å€‹ä¾å­˜é—œä¿‚ (dependency relation)ã€‚\nä¾å­˜é—œä¿‚æ˜¯ä¸€å€‹éå°ç¨±çš„äºŒå…ƒé—œä¿‚ï¼Œå…¶ä¸­ä¸€å€‹å…ƒç´ æ˜¯ headï¼Œå¦ä¸€å€‹å…ƒç´ æ˜¯ dependentã€‚\n\n\n\n\n\n\n\n\nTip\n\n\n\néå°ç¨±çš„é—œä¿‚ä¹Ÿå¸¶å‡ºäº†éšå±¤çš„æ¦‚å¿µï¼Œæ­¤å¤–ï¼Œæ¯å€‹å…ƒç´ éƒ½å¯èƒ½åŒæ™‚æ”¯é…èˆ‡è¢«æ”¯é…ã€‚"
  }
]