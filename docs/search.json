[
  {
    "objectID": "weeks/week8.html",
    "href": "weeks/week8.html",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "",
    "text": "意義是一切！\n\n上週提到的形式語言學（形式語法與邏輯語意）在程式語言的編譯上具有很大的用途。\n語意學很長一段時間將重點放在具有真值 (truth value) 判斷的陳述句，同時開展了相當完整的邏輯與推導系統。\n但是，語言的使用者不只是要表達真實的意義，也要表達自己的情緒、態度、立場、意圖、想法、慾望等交際意義。"
  },
  {
    "objectID": "weeks/week8.html#語言學到現在累積的幾個經驗",
    "href": "weeks/week8.html#語言學到現在累積的幾個經驗",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "語言學到現在累積的幾個經驗",
    "text": "語言學到現在累積的幾個經驗\n(Hsieh 2023)\n\n化約論 (reductionism) (複雜的問題可以通過分析其組成部分來解決) 在探究語言的意義上有很大的局限性。\n模塊觀 (modularist view) 是 useful fiction。例如把語意與語用分開處理，是教學方便。\n語法自主 (autonomy of syntax) 與內在天賦論 (innateness) 是可疑的。 \\(\\rightarrow\\) 語言學習是意義與交互作用中驅動的。句法與語意不可分，是在使用中湧現的產物。\n\n\n\n\n\n\n\nNote\n\n\n\n語言不僅是單一的物理現象、生理現象、社會現象，心理現象。它是人文化成的複雜系統。"
  },
  {
    "objectID": "weeks/week8.html#語言學的認知假設",
    "href": "weeks/week8.html#語言學的認知假設",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "語言學的認知假設",
    "text": "語言學的認知假設\n認知語言學 (cognitive linguistics) ，或語言與認知\n\n在腦神經科學與認知科學的基礎下，80-90 年代，一些語言學家開始關注語言與人類認知之間的相互作用。\n\n\n核心觀點是語言不僅是一種抽象的符號系統，而且是與我們的思維、知識和經驗密切相關的複雜心智現象。"
  },
  {
    "objectID": "weeks/week8.html#主要研究範疇",
    "href": "weeks/week8.html#主要研究範疇",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "主要研究範疇",
    "text": "主要研究範疇\n\n\n\n\n\n\nImportant\n\n\n\n概念隱喻（Conceptual Metaphor）、構式語法（Construction Grammar）、語言類型學（Linguistic Typology）與文化、語言習得和使用、語言與時間空間、語言與概念組織等"
  },
  {
    "objectID": "weeks/week8.html#核心觀點",
    "href": "weeks/week8.html#核心觀點",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "核心觀點",
    "text": "核心觀點\n象似性 (iconicity)：語言的符號與其對應的客觀對象之間的相似性。the degree to which a linguistic expression resembles the object or event it denotes (Peirce).\n\n皮爾斯提出了三種不同類型的符號 (sign)：\n\n標誌符號 (icon) : 符號與客觀對象有形象上的關係。 \n指示符號（index）: 符號與客觀對象有直接的或因果的關係。\n象徵符號（symbol）: 符號與客觀對象有約定俗成的關係。 \n\n\npic source"
  },
  {
    "objectID": "weeks/week8.html#象似性在中文文字造字上的表現",
    "href": "weeks/week8.html#象似性在中文文字造字上的表現",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "象似性在中文文字造字上的表現",
    "text": "象似性在中文文字造字上的表現\n象形 (iconic)、指示 (indexical)、…?"
  },
  {
    "objectID": "weeks/week8.html#象似性在漢語上的表現",
    "href": "weeks/week8.html#象似性在漢語上的表現",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "象似性在漢語上的表現",
    "text": "象似性在漢語上的表現\n語言中的象似性主要是指「語言結構」跟人的「概念結構」之間的自然聯繫 (戴浩一, 1985)。展現在順序、距離、重疊等方面。\n\n順序象似性\n\n她搭捷運到這裏 vs 她到這裏搭捷運"
  },
  {
    "objectID": "weeks/week8.html#象似性在漢語上的表現-1",
    "href": "weeks/week8.html#象似性在漢語上的表現-1",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "象似性在漢語上的表現",
    "text": "象似性在漢語上的表現\n\n距離象似性 (概念上距離近的成分設法在句法分佈上彼此鄰近)\n\n吃飽了vs *吃了飽 vs 吃個飽\n他送給她一張卡片vs 他送一張卡片給他"
  },
  {
    "objectID": "weeks/week8.html#象似性在漢語上的表現-2",
    "href": "weeks/week8.html#象似性在漢語上的表現-2",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "象似性在漢語上的表現",
    "text": "象似性在漢語上的表現\n\n重疊象似性"
  },
  {
    "objectID": "weeks/week8.html#主觀性場景與注意力焦點",
    "href": "weeks/week8.html#主觀性場景與注意力焦點",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "主觀性、場景與注意力/焦點",
    "text": "主觀性、場景與注意力/焦點\n\nSubjectivity 心理活動的選擇性\nFocus 心理活動所選擇的客觀物件"
  },
  {
    "objectID": "weeks/week8.html#意象圖示-image-schemas",
    "href": "weeks/week8.html#意象圖示-image-schemas",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "意象圖示 | Image Schemas",
    "text": "意象圖示 | Image Schemas\n\n意象: 在某物不在場時，我們的心智還能想像得出該物的具體。\n圖示：把經驗加工組織成某種認知結構，較長期地儲存在記憶之中。\n意象圖示：在人與客觀世界感知與互動中一種反覆出現的動態的模式。 e.g, SOURCE-PATH-GOAL schema"
  },
  {
    "objectID": "weeks/week8.html#完形與語言壓縮",
    "href": "weeks/week8.html#完形與語言壓縮",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "完形與語言壓縮",
    "text": "完形與語言壓縮"
  },
  {
    "objectID": "weeks/week8.html#構式知識語料庫",
    "href": "weeks/week8.html#構式知識語料庫",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "構式知識語料庫",
    "text": "構式知識語料庫\n北大現代漢語構式數據庫"
  },
  {
    "objectID": "weeks/week8.html#構式與機器學習",
    "href": "weeks/week8.html#構式與機器學習",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "構式與機器學習",
    "text": "構式與機器學習\nComputational Construction Grammar (C*G)\n參考投影片 c2xg python"
  },
  {
    "objectID": "weeks/week8.html#總結",
    "href": "weeks/week8.html#總結",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "總結",
    "text": "總結\n\n人的認知結構（象似性、思維時序關係、空間層次關係等）參與決定了語言的意義結構。"
  },
  {
    "objectID": "weeks/week8.html#evaluation-metrics",
    "href": "weeks/week8.html#evaluation-metrics",
    "title": "Week 8: Cognitive Linguistics and NLP",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n爲何需要這些不同的評估方法？ \n\\(F_{\\beta}= \\frac{(\\beta^{2} +1) PR} {\\beta^{2}P+R}\\)"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %>%\n  count(manufacturer) %>%\n  mutate(manufacturer = str_to_title(manufacturer)) %>%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel <- lm(mpg ~ hp, data = mtcars)\ntidy(model) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 6 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{7}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{8}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 8 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{9}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{10}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{11}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/math_pre.html",
    "href": "supplemental/math_pre.html",
    "title": "Math",
    "section": "",
    "text": "將函數圖形某個點為中心，將圖形無限放大時，圖形會趨近於直線。此時的直線斜率就是函數在該點的微分值。\n\n\n\n\n以 \\(y= x^3 -x\\), 點 \\((\\frac{1}{2}, -\\frac{3}{8} 為中心當例子)\\)"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "weeks/week-1.html#自然語言處理",
    "href": "weeks/week-1.html#自然語言處理",
    "title": "Week1 Orientation",
    "section": "自然語言處理",
    "text": "自然語言處理\n\n研究慣例上，將語言資訊的問題切分成不同的子任務，成熟之後就變成大家較熟悉的應用。\n\n語音辨識與語音合成\n文本摘要\n機器翻譯\n自動問答\n。。。。\n\n\n\n\n曾經語音與多模態(multimodality)是比較不歸類在 NLP 的領域，但最近幾年的進展與趨勢，也與 NLP 的研究社群密切互動。"
  },
  {
    "objectID": "weeks/week-1.html#計算語言學-1",
    "href": "weeks/week-1.html#計算語言學-1",
    "title": "Week1 Orientation",
    "section": "計算語言學",
    "text": "計算語言學\n\n\n漢語的雙音節化是怎麼演化的 （computational historical linguistics）\n新詞 (neologism)、新構式 (constructions) 是怎麼在社群中突現、傳遞、消亡的 （computational sociolinguistics）\n面對完全沒有接觸過的語言，如何理解與編寫語法系統？（computational linguistic typology/ xenolinguistics）\n……. (你的想法？)"
  },
  {
    "objectID": "weeks/week-1.html#從語言的處理到理解",
    "href": "weeks/week-1.html#從語言的處理到理解",
    "title": "Week1 Orientation",
    "section": "從語言的處理到理解",
    "text": "從語言的處理到理解\nFrom Natural Language Processing to National Language Understanding\n\n對於處理與理解，都是操作型定義 (operationalized)。"
  },
  {
    "objectID": "weeks/week-1.html#傳統自然語言處理的發展",
    "href": "weeks/week-1.html#傳統自然語言處理的發展",
    "title": "Week1 Orientation",
    "section": "傳統自然語言處理的發展",
    "text": "傳統自然語言處理的發展\n\n\n\n中文 AI 知識庫與自然語言處理核心套件研討會"
  },
  {
    "objectID": "weeks/week-1.html#個人觀點",
    "href": "weeks/week-1.html#個人觀點",
    "title": "Week1 Orientation",
    "section": "(個人觀點)",
    "text": "(個人觀點)\n\n\n(形式)語言學與傳統自然語言處理中的 Modularity 預設， 是好用的虛構 (useful fiction, Haugen and Dil, 1972)，方便整理知識，但非真實的語言現象。\n語言：人類，爲了溝通目的，在時空脈絡下，發展出的形意符號系統。 (cultural Species; form-meaning pairing; symbolic; self-adaptive complex system; situated and embodied cognition; etc.)\n符碼本質是形意映射，而指涉意義是約定俗成、很吃情境的。因此符碼運算與表徵是給人看的。語言的理解，涉及到更複雜的連續的過程，而非一個分段、分解的過程。\n語言學不等於 rule-based。"
  },
  {
    "objectID": "weeks/week-1.html#大型預訓練語言模型",
    "href": "weeks/week-1.html#大型預訓練語言模型",
    "title": "Week1 Orientation",
    "section": "大型預訓練語言模型",
    "text": "大型預訓練語言模型\npre-trained Large Language Models (pre-LLMs)\n\n改變了(目前)的自然語言處理研究方向"
  },
  {
    "objectID": "weeks/week-1.html#gpt-3-and-beyond",
    "href": "weeks/week-1.html#gpt-3-and-beyond",
    "title": "Week1 Orientation",
    "section": "GPT-3 and Beyond",
    "text": "GPT-3 and Beyond\n很清楚的科普介紹"
  },
  {
    "objectID": "weeks/week-1.html#post-ai-nlp-in-context-learning-prompt-training",
    "href": "weeks/week-1.html#post-ai-nlp-in-context-learning-prompt-training",
    "title": "Week1 Orientation",
    "section": "post-AI NLP: in-context learning (prompt-training)",
    "text": "post-AI NLP: in-context learning (prompt-training)\na new paradigm of NLP that is based on the recent advances in AI, especially in the field of language modeling.\n\nRecently, we have seen dramatic advances in natural language processing (NLP) driven by huge pre-trained language models such as GPT-3 and DALLE-2. Instead of building many small task-specific models, there is a movement to create and use these more all-purpose huge language models for many NLP applications.\n\nThe most intriguing finding is that these models employ a new learning paradigm: in-context learning, where they learn to do a downstream task simply by conditioning on a prompt consisting of a few input-output examples without any parameter updates"
  },
  {
    "objectID": "weeks/week-1.html#如何影響教學",
    "href": "weeks/week-1.html#如何影響教學",
    "title": "Week1 Orientation",
    "section": "如何影響教學",
    "text": "如何影響教學\n(順便介紹助教群 😄)"
  },
  {
    "objectID": "weeks/week-1.html#爲什麼我們可以放心或應該擔心",
    "href": "weeks/week-1.html#爲什麼我們可以放心或應該擔心",
    "title": "Week1 Orientation",
    "section": "爲什麼我們可以放心（或應該擔心）？",
    "text": "爲什麼我們可以放心（或應該擔心）？\naka. 無中生有，跟真的一樣"
  },
  {
    "objectID": "weeks/week-1.html#我們怎麼實作這門課",
    "href": "weeks/week-1.html#我們怎麼實作這門課",
    "title": "Week1 Orientation",
    "section": "我們怎麼實作這門課？",
    "text": "我們怎麼實作這門課？\n\n我們的主要學習重點不會放在自然語言處理中的機器學習模型原理 (請大家在其他的課程中並行學習)，而是放在語言學理論與自然語言處理的關係。\n選擇有語言學背景的教材 (如 Jurafsky & Martin SLP3, Christopher Potts cs224u, etc.) 來進行課程的教學，並補充不同的觀點。\n\n\n\n\n\n\n\nWarning\n\n\n\n這是沒有教科書的學門\n\n\n\n實作部分側重應用。"
  },
  {
    "objectID": "weeks/week-1.html#看看當前的研究教學",
    "href": "weeks/week-1.html#看看當前的研究教學",
    "title": "Week1 Orientation",
    "section": "看看當前的研究教學",
    "text": "看看當前的研究教學\nIntroduction and course overview"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2 Introduction",
    "section": "",
    "text": "實務應用上會使用 SOTA 的工具。科學探索上要學習背後的理論歷史。\n\n\n\n\n自然語言處理和計算語言學的異同\n但兩者的基礎類似 (機器學習、程式能力、演算法)\n\n前者目前更側重語言模型的工程訓練\n後者偏向邏輯推理與模組解析的語言理論研究\n\n\n\n\n\n\nNLP 簡史\n語言理論與深度學習\n入門實習\n\n\n\n\n\n\nimage source\n\n\n\n\n\n\n在大數據大演算力的時代，auto-regressive LLMs (以 chatGPT (Brown et al. 2020) 爲代表) 改變了當前 NLP 生態的發展方向。 \n\n\n\n\nNLP Process\n\n\n\n\nNLP Process with a Language Model\n\n\nimage source\n\n\n\n\n\nLearning from a few examples in context: 將 \\(X\\) 以及可能支持其預測 \\(Y\\) 的一些 context（可能是其他標記數據，背景知識，相關文檔等）一起輸入模型中，實現對 \\(Y\\) 的預測。\n當下學習，不重新調參數。Different from supervised learning requiring a training stage that uses backward gradients to update model parameters, ICL does not conduct parameter updates and directly performs predictions on the pretrained language models.\n\n\n\n\n\n\n\\(A:B=C:?\\)\n\n\n\n\n\n\nImportant\n\n\n\n\n會用自然語言寫 prompting templates、排序與挑選脈絡，變成一種解析技能。(Zhou et al. 2022)\n\n\n\n\n\n\n邏輯推理是機器可學習的嗎？\n\na chain of thought (i.e., a series of intermediate reasoning steps) significantly improves the ability of large language models to perform complex reasoning.(Wei et al. 2022)\n看看解語言謎題的結果 Translation Puzzles are In‑context Learning Tasks\n\n\n\n\n\n\n\n\n語言模型從大量語料中學習語詞的機率分佈 (e.g., how likely a word or sequence can occur in the context).\nTransformer-based language models。GPT-3 和 BERT 是其中具代表性的。\n目前火紅的 chatGPT (Brown et al. 2020) (GPT-3 variants) 還利用人工反饋之強化學習微調 pre-trained on a huge corpus of web and book data and fine-tuned with human conversation examples through supervised learning and reinforcement learning from human feedback (RLHF).\n\n\n\n\n\n\n\n是一種 encoder-decoder 神經網路類型，\n\n\n\n\n\nTransformer 使用的注意力計算機制 (attention)，可以用來很好的處理序列數據 (e.g., 文本、語音、音樂)。\n注意力機制讓模型更去注意到序列元素之間的相關性。(e.g., He-Tom, apples-them) \n\n\nsource\n\n\n\n\n\n語言表達能力流暢 (儘管說的是真是假不確定)\n貌似可以進行討論 (但是背後推理機制不透明) making probabilistic predictions in the generation process based on the corpus it has been trained on, which can lead to false claims about facts. Typical hedging behavior of ChatGPT will certainly increase the safety of the system.\n不開源；非商業組織如大學，通常缺乏跟上語言模型快速發展所需的計算和財務資源。\n\n\n\n\n\n\n\n\n\n玩一題語奧 (NACLO, UKLO, Panini)。分組討論 prompt/in-context 的設計，比看看 ChrF 的效果。"
  },
  {
    "objectID": "weeks/week-2.html#prellms-學到了什麼",
    "href": "weeks/week-2.html#prellms-學到了什麼",
    "title": "Week 2 Introduction",
    "section": "preLLMs 學到了什麼？",
    "text": "preLLMs 學到了什麼？\n\n學習到的不僅僅是鄰近字串機率，還有一些結構性的知識。 以許多語言的 long-distance number agreement 為例。(Baroni 2021):p8\n如果先瞭解語言怎麼被我們學習到的 usage-based theory of language acquisition\n\n結構性的東西，在一定的數據學習之後，會 emerged。不需要預先規劃與給定。\n以 Papadimitriou and Jurafsky EMNLP 2020 爲例。\n\n這就是爲何形式語言學與認知功能語言學的震驚程度不同 😆"
  },
  {
    "objectID": "weeks/week-2.html#語言學與機器學習的關係-imo",
    "href": "weeks/week-2.html#語言學與機器學習的關係-imo",
    "title": "Week 2 Introduction",
    "section": "語言學與機器學習的關係 (IMO)",
    "text": "語言學與機器學習的關係 (IMO)\n\n規則學習 | 形式語言學 (formal syntax)\n特徵學習 | 統計語言學 (statistical linguistics)\n深度學習 | 認知與功能語言學 (cognitive/functional linguistics)"
  },
  {
    "objectID": "weeks/week-2.html#理論語言學-theorectical-linguistics",
    "href": "weeks/week-2.html#理論語言學-theorectical-linguistics",
    "title": "Week 2 Introduction",
    "section": "理論語言學 | Theorectical Linguistics",
    "text": "理論語言學 | Theorectical Linguistics\n(mostly known as Formal Syntax, or Generative Grammar)1\n\n\n生成語法預設語言是個由有限遞迴規則組成的形式系統， 可以用來生成所有可能的合法句子。\n大部分所謂的語言學理論，都是在生成語法的架構之下，主題包括：\n\nLearnability\nGeneralization given insufficient data\nCentrality of syntactic structure\n. ."
  },
  {
    "objectID": "weeks/week-2.html#認知功能語言學-cognitivefunctional-linguistics",
    "href": "weeks/week-2.html#認知功能語言學-cognitivefunctional-linguistics",
    "title": "Week 2 Introduction",
    "section": "認知功能語言學 | Cognitive/Functional Linguistics",
    "text": "認知功能語言學 | Cognitive/Functional Linguistics\n\n晚近才引入 NLP 的解釋\nLanguage as a complex system"
  },
  {
    "objectID": "weeks/week-2.html#這些都蠻進階的但我們慢慢從幾個重要的語言面向開始",
    "href": "weeks/week-2.html#這些都蠻進階的但我們慢慢從幾個重要的語言面向開始",
    "title": "Week 2 Introduction",
    "section": "這些都蠻進階的，但我們慢慢從幾個重要的語言面向開始。",
    "text": "這些都蠻進階的，但我們慢慢從幾個重要的語言面向開始。\n\n詞彙\n句法\n語意\n語音與多模態\n\n\n\n\n\n\n\nWarning\n\n\n\n探究這些現象的語言學理論背景是什麼？又怎麼連接到 NLP 的工程實踐？"
  },
  {
    "objectID": "weeks/week-2.html#我認爲-accord-with-lecun-carmack-ii",
    "href": "weeks/week-2.html#我認爲-accord-with-lecun-carmack-ii",
    "title": "Week 2 Introduction",
    "section": "我認爲 (accord with LeCun, Carmack II)",
    "text": "我認爲 (accord with LeCun, Carmack II)\n\n目前的 LLM (Generative AI) 不太是 GAI (General AI) 的實踐方向，但是可以當作一個 IA 謹慎使用。\n目前來說 (2023 上半年)，讓 LLM 發展更好一點，各司其職是個方法 😁\n\ntoolTransformer； 事實與邏輯的學習 perplexity.ai"
  },
  {
    "objectID": "weeks/week-2.html#但發展速度真的很快",
    "href": "weeks/week-2.html#但發展速度真的很快",
    "title": "Week 2 Introduction",
    "section": "但發展速度真的很快",
    "text": "但發展速度真的很快\nmultimodal LLM"
  },
  {
    "objectID": "weeks/week-2.html#分出兩種-nlp",
    "href": "weeks/week-2.html#分出兩種-nlp",
    "title": "Week 2 Introduction",
    "section": "分出兩種 NLP",
    "text": "分出兩種 NLP\n\n\n[1]（計算語言學傳統）利用計算模型與工具， 以人懂的方式 （符碼運算）解析與瞭解語言 (及其他模態)。\n[2]（post-LLM NLP 導向）以電腦會就好（張量運算）的方式，開發與調整模型的 (驚人的) 落地應用。\n\n\n[1] 提供模型挑戰與複雜性 (e.g., 辯論、認錯、同理、意識, …)； [2] 反饋給（語言）理論發展範式的合理性，並且改變社會發展。"
  },
  {
    "objectID": "weeks/week-2.html#回到我們課程設計上",
    "href": "weeks/week-2.html#回到我們課程設計上",
    "title": "Week 2 Introduction",
    "section": "回到我們課程設計上",
    "text": "回到我們課程設計上\n\n[Lecture] 講解「與 NLP 相關的語言學理論」\n\nlinguistically-oriented DNN analysis (cf. BlackBox NLP 系列研討會; Computation in Linguistics 協會)\n(ML 課程搭配)\n\n[Lab] 透過實作練習與工具使用\n\n語言模組解析 via spacy\n語言模型應用 via Hugging Face (transformers)"
  },
  {
    "objectID": "weeks/week-2.html#section",
    "href": "weeks/week-2.html#section",
    "title": "Week 2 Introduction",
    "section": "",
    "text": "References\n\n\nBaroni, Marco. 2021. “On the Gap Between Computational and Theoretical Linguistics.” https://www.virtual2021.eacl.org/plenary_session_keynote_by_marco_baroni.html.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nDong, Qingxiu, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. “A Survey for in-Context Learning.” arXiv Preprint arXiv:2301.00234.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.” arXiv Preprint arXiv:2201.11903.\n\n\nZhou, Yongchao, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. “Large Language Models Are Human-Level Prompt Engineers.” arXiv Preprint arXiv:2211.01910.\n\n\n======= ::: {style=“font-size: 0.875em;”} Back to course schedule ⏎ :::"
  },
  {
    "objectID": "weeks/week3.html",
    "href": "weeks/week3.html",
    "title": "Week 3 Word",
    "section": "",
    "text": "詞彙、構詞與計算\n\n文本處理：文本正則化、語料庫與程式概念工具\nWordhood： many perspectives\n[Lab] Corpus processing skills\n\n\n\nLinguistics for NLP\n\n\n\n\nLanguage and Text\n\n\n文本處理是自然語言處理的基礎工作，是一種對文本進行分析、處理、編輯、轉換的技術。\n\n很適合用來搭配程式學習。\n\n文本正則 (text normalization) 是一種對文本進行 標準化 的轉換任務。\n\n\n\n標準化在此意義是對當下任務更便捷、更容易處理的意思。\n\n\n\n\ntext normalization tasks\n\n\ntokenization ：將文本分割成一個個的（詞符）單位。\nlemmatization：將不同表面形式的詞彙還原成相同詞根。\nstemming ：簡化版的 lemmatization，只把後綴拿掉。\n\n\n\n\n\n\n有時候，如處理古漢語，還需要斷句 sentence segmentation。\n\n\n\n\n\n\n\n\n\n\n\n比較：前處理任務「比較」不涉及語言學知識。\n\n\n\n- case folding：將所有字母轉換成小寫。\n- stopword removal：去除停用詞。\n\n\n\n\n\nTokenization vs Word Segmentation\n\n\nWord segmentation 貌似 word-based tokenization，但在中文與一些語言的脈絡中，它們不是完全一樣的概念。\n主要的差異點在於 wordhood 的概念是不定的、使用中的語意決定的。\n\n\n\n\n\n```{python}\n# Chinese\nimport jieba\njieba.lcut('還要如此費工，我認爲是duck可不必')\n# ['還要', '如此', '費工', '，', '我認', '爲', '是', 'duck', '可', '不必']\n\n# Japanese\nimport nagisa\ntext = 'Pythonで簡単に使えるツールです'\ndoc = nagisa.tagging(text)\ndoc.words\n# ['Python', 'で', '簡単', 'に', '使える', 'ツール', 'です']\n\n# Korean\nimport konlpy\nphrase = \"아버지가방에들어가신다\"\nfrom konlpy.tag import Hannanum\nhannanum = Hannanum()\nhannanum.morphs(phrase)\n# ['아버지가방에들어가', '이', '시ㄴ다']\n\n# Thai\nimport tltk\nphrase = \"\"\"สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ \nในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์\"\"\"\npieces = tltk.nlp.pos_tag(phrase)\npieces\n# [[('สำนักงาน', 'NOUN'),\n#   ('เขต', 'NOUN'),\n#   ('จตุจักร', 'PROPN'),\n#   ('ชี้แจง', 'VERB'),\n#   ('ว่า', 'SCONJ'),\n#   ('<s/>', 'PUNCT')],\n#  ...\n\n```\n\n\n\ndata and tools\n要進行文本的處理，我們需要（大量）語料與好用的程式工具。\n\n\n\nCorpus and Corpora\n\n語料庫 是一種重要的語言資源(language resource) 。\n文本語料庫是較常被使用的語言資源，它不僅是一個文本的集合 (collection of texts)，藉由標記 (annotation)，也是語言知識外顯化的一種形式。\n\n\n\n\n\n\n\nTip\n\n\n\n資料科學與語料分析:方法與實務：語言學角度的語料庫方法教科書\n\n\n\n\n\nLanguage Laws\n有了語料庫，我們就可以進行語言學的量化研究，例如發現語言法則。\n\n\n\n\n\n\nImportant\n\n\n\n要先知道 type (通常用 \\(V\\) 表示) 與 token (通常用 \\(N\\) 表示) 的差別。前者是所觀察的語料中字詞的種類，後者是字詞的總數。 \\(|V|\\) 則表示種類的數量 (vocabulary size)。\n\n\n\n\n\n\n\nZipf’s law：在給定的語料中，對於任意一個語詞，其頻率 (frequency) 的排名（rank）和頻率的乘積大致是一個常數。\nHeaps’ law (Herdan’s law)：在給定的語料中，(\\(|V|\\)) 大致是語料大小 (N) 的一個指數函數。\n\n\\(|V| = kN^{\\beta}\\), where \\(k\\) and \\(\\beta\\) are positive constants, and 0 < \\(\\beta\\) < 1\n\n\nMenzerath–Altmann law：the increase of the size of a linguistic construct results in a decrease of the size of its constituents.\n\n\n\n\n\n\n\n\n\n\n\nzip’s law\n\n\n\n\n\n\n\n\n先想一下怎麼實作。請 chatGPT 當程式助教，完成 Heap’s Law 的實驗觀察。\n```{python}\n# your code here\n```"
  },
  {
    "objectID": "weeks/week3.html#練習",
    "href": "weeks/week3.html#練習",
    "title": "Week 3 Word",
    "section": "練習",
    "text": "練習\nCorpus processing skills via command line (bash)"
  },
  {
    "objectID": "weeks/week3.html#常用的-subword-based-tokenization-方法",
    "href": "weeks/week3.html#常用的-subword-based-tokenization-方法",
    "title": "Week 3 Word",
    "section": "常用的 Subword-based tokenization 方法",
    "text": "常用的 Subword-based tokenization 方法\n\nSubword-based tokenization 是目前較被使用的方法，因為它可以\n\n處理 word-based 的問題（very large vocabulary size, large number of OOV tokens, and different meaning of very similar words）\n也可以處理 character-based 的問題（very long sequences and less meaningful individual tokens）。\n\n常用的方法有：\n\nbyte-pair encoding (BPE) (> GPT family)\nwordpiece (> BERT)\nunigram language modeling\nSentencee piece"
  },
  {
    "objectID": "weeks/week3.html#byte-pair-encoding-bpe",
    "href": "weeks/week3.html#byte-pair-encoding-bpe",
    "title": "Week 3 Word",
    "section": "Byte-pair encoding (BPE)",
    "text": "Byte-pair encoding (BPE)\nSubword-based tokenization algorithm (Sennrich, Haddow, and Birch 2015)\n\ntoken learner and token segmenter\n\n\n\n\nLearner 做的事\n\n\n\n我們用原作者的投影片來說明"
  },
  {
    "objectID": "weeks/week3.html#練習-2",
    "href": "weeks/week3.html#練習-2",
    "title": "Week 3 Word",
    "section": "練習",
    "text": "練習\n使用 HuggingFace 的 tokenizer"
  },
  {
    "objectID": "weeks/week3.html#word-vs-morpheme",
    "href": "weeks/week3.html#word-vs-morpheme",
    "title": "Week 3 Word",
    "section": "Word vs Morpheme",
    "text": "Word vs Morpheme\n\nmorpheme：語言中最小的意義單位。\nword：語言中最小的概念單位。"
  },
  {
    "objectID": "weeks/week3.html#中文的詞素概念",
    "href": "weeks/week3.html#中文的詞素概念",
    "title": "Week 3 Word",
    "section": "中文的詞素概念",
    "text": "中文的詞素概念"
  },
  {
    "objectID": "weeks/week3.html#中文斷詞問題",
    "href": "weeks/week3.html#中文斷詞問題",
    "title": "Week 3 Word",
    "section": "中文斷詞問題",
    "text": "中文斷詞問題\nChinese word segmentation: principles and practices"
  },
  {
    "objectID": "weeks/week3.html#爲什麼現在的-chatgpt-不太會斷算詞有不合理嗎",
    "href": "weeks/week3.html#爲什麼現在的-chatgpt-不太會斷算詞有不合理嗎",
    "title": "Week 3 Word",
    "section": "爲什麼「現在的」 chatGPT 不太會斷/算詞？有不合理嗎？",
    "text": "爲什麼「現在的」 chatGPT 不太會斷/算詞？有不合理嗎？"
  },
  {
    "objectID": "weeks/week3.html#section-2",
    "href": "weeks/week3.html#section-2",
    "title": "Week 3 Word",
    "section": "",
    "text": "Danger\n\n\n\n\nIn polysynthetic languages like Inuktitut or Mohawk, words can be very long and complex, consisting of multiple morphemes that would be considered separate words in English."
  },
  {
    "objectID": "weeks/week3.html#wordhood-issues",
    "href": "weeks/week3.html#wordhood-issues",
    "title": "Week 3 Word",
    "section": "Wordhood issues",
    "text": "Wordhood issues\n\n從不同語言學的領域（phonological-prosodic/morphological-grammatical/sociolinguitic）來看 word，也難有一定的詞(的界線)。 (e.g., hao-le-mei in Mandarin Chinese)\n\n\n\n語言特性的影響使得在表層形式的詞長不穩定，距離也不明確。(e.g., Er gibt nicht auf. in German; 離合詞 in Chinese) > In agglutinative languages, for example, multiple morphemes are combined into single words, making it difficult to determine where one word ends and another begins. In these cases, the term for ‘word’ might encompass a broader concept than in English.\n有時當詞素，有時當詞 a morpheme behaves like a word on one subset of wordhood parameters but like a bound item on another. (Zingler 2020)"
  },
  {
    "objectID": "weeks/week4.html",
    "href": "weeks/week4.html",
    "title": "Week 4 Word Meaning",
    "section": "",
    "text": "詞義的概念\n詞彙語意資源\n詞義的計算與應用 (feat.GPT4 的詞義理解能力)"
  },
  {
    "objectID": "weeks/week4.html#多義與歧義",
    "href": "weeks/week4.html#多義與歧義",
    "title": "Week 4 Word Meaning",
    "section": "多義與歧義",
    "text": "多義與歧義\npolysemy and ambiguity\n\n多義 (polysemy): 同一詞彙在不同語境下有不同的使用意義\n\n\n\n\n\n\n\nImportant\n\n\n\nhomonymy"
  },
  {
    "objectID": "weeks/week4.html#詞意的區辨",
    "href": "weeks/week4.html#詞意的區辨",
    "title": "Week 4 Word Meaning",
    "section": "詞意的區辨",
    "text": "詞意的區辨\n\nzeugma (共軛修飾法)"
  },
  {
    "objectID": "weeks/week4.html#中文詞網-詞意區辨原則",
    "href": "weeks/week4.html#中文詞網-詞意區辨原則",
    "title": "Week 4 Word Meaning",
    "section": "(中文詞網) 詞意區辨原則",
    "text": "(中文詞網) 詞意區辨原則\n\n一義一項\n一物一義\n一事一義\n義不隨境遷\n義面由觀點何語境決定"
  },
  {
    "objectID": "weeks/week4.html#詞意與義面",
    "href": "weeks/week4.html#詞意與義面",
    "title": "Week 4 Word Meaning",
    "section": "詞意與義面",
    "text": "詞意與義面\nsense and meaning facet"
  },
  {
    "objectID": "weeks/week4.html#詞意的粒度",
    "href": "weeks/week4.html#詞意的粒度",
    "title": "Week 4 Word Meaning",
    "section": "詞意的粒度",
    "text": "詞意的粒度\nword sense granularity\n\nKillagraf “I don’t believe in word sensse”\n細粒度 fine-grained (WordNet) vs 粗粒度 coarse-grained word senses (e.g., OntoNotes))"
  },
  {
    "objectID": "weeks/week4.html#詞意的規則結構",
    "href": "weeks/week4.html#詞意的規則結構",
    "title": "Week 4 Word Meaning",
    "section": "詞意的（規則）結構",
    "text": "詞意的（規則）結構\nsense alternation\n\n規則多義 (regular polysemy)\n隱喻、比喻 (metaphor, metonymy, etc)"
  },
  {
    "objectID": "weeks/week4.html#詞意的-概念-結構",
    "href": "weeks/week4.html#詞意的-概念-結構",
    "title": "Week 4 Word Meaning",
    "section": "詞意的 (概念) 結構",
    "text": "詞意的 (概念) 結構\nqualia structure"
  },
  {
    "objectID": "weeks/week4.html#附帶一題",
    "href": "weeks/week4.html#附帶一題",
    "title": "Week 4 Word Meaning",
    "section": "附帶一題",
    "text": "附帶一題\n\n\n\n\n\n\nNote\n\n\n\n機器(深度)學習課程\n\n中文 以李宏毅老師 2021 年的課程優先（參考此份播放清單，再往 2022-23 參考。（感謝建成提供）\n英文 NTU: Deep Learning for Natural Language Processing: From Theory to Practice； MIT introduction to Deep Learning 都還不錯。"
  },
  {
    "objectID": "weeks/week4.html#專有名詞的語意",
    "href": "weeks/week4.html#專有名詞的語意",
    "title": "Week 4 Word Meaning",
    "section": "專有名詞的語意",
    "text": "專有名詞的語意\nsemantic issues of Proper Names\n\nNamed Entity Taxonymy\n專名辨識任務 (Named Entity Recognition, NER)\n\n\nMillian theorists claim that proper names do not possess a lexical meaning but directly refer to a certain entity,Fregean scholars assert that proper names do carry meanings, and the problem is just about the “meaning” employed."
  },
  {
    "objectID": "weeks/week4.html#專有名詞的語意-1",
    "href": "weeks/week4.html#專有名詞的語意-1",
    "title": "Week 4 Word Meaning",
    "section": "專有名詞的語意",
    "text": "專有名詞的語意\nwordhood vs properhood\n\n專名有詞意嗎？\n專名的詞意有規律嗎？"
  },
  {
    "objectID": "weeks/week4.html#詞義的計算與應用-1-詞義消歧",
    "href": "weeks/week4.html#詞義的計算與應用-1-詞義消歧",
    "title": "Week 4 Word Meaning",
    "section": "詞義的計算與應用 (1): 詞義消歧",
    "text": "詞義的計算與應用 (1): 詞義消歧\nword sense disambiguation (WSD)\n\n決定詞彙在句子中的最合的詞意\n只做一個詞 (lexical sample) 和全部做 (all-words task)\n演算\n\nBaseline: 挑頻率最高的（在 wordnet 排第一個的詞意）\n通常都是使用 supervised learning (e.g., nearest neighbors with contextual embeddings)\n也有所謂 knowledge-based 的方法 (e.g., Lesk algorithm 比對字典釋義共享詞彙數量)"
  },
  {
    "objectID": "weeks/week4.html#glossbert",
    "href": "weeks/week4.html#glossbert",
    "title": "Week 4 Word Meaning",
    "section": "GlossBert",
    "text": "GlossBert"
  },
  {
    "objectID": "weeks/week4.html#gpt4-的詞義消歧能力",
    "href": "weeks/week4.html#gpt4-的詞義消歧能力",
    "title": "Week 4 Word Meaning",
    "section": "GPT4 的詞義消歧能力",
    "text": "GPT4 的詞義消歧能力"
  },
  {
    "objectID": "weeks/week4.html#詞義的計算與應用-2-詞意推導",
    "href": "weeks/week4.html#詞義的計算與應用-2-詞意推導",
    "title": "Week 4 Word Meaning",
    "section": "詞義的計算與應用 (2): 詞意推導",
    "text": "詞義的計算與應用 (2): 詞意推導\nword sense induction (WSI)\n\nunsupervised machine learning (不使用 wordnet 的詞意資源當答案)\nagglomerative clustering"
  },
  {
    "objectID": "weeks/week4.html#gpt4-的詞義推導能力",
    "href": "weeks/week4.html#gpt4-的詞義推導能力",
    "title": "Week 4 Word Meaning",
    "section": "GPT4 的詞義推導能力",
    "text": "GPT4 的詞義推導能力"
  },
  {
    "objectID": "weeks/week4.html#詞義的計算與應用-3-詞意理解與行動",
    "href": "weeks/week4.html#詞義的計算與應用-3-詞意理解與行動",
    "title": "Week 4 Word Meaning",
    "section": "詞義的計算與應用 (3): 詞意理解與行動",
    "text": "詞義的計算與應用 (3): 詞意理解與行動"
  },
  {
    "objectID": "weeks/week4.html#gpt4-的詞義語用能力",
    "href": "weeks/week4.html#gpt4-的詞義語用能力",
    "title": "Week 4 Word Meaning",
    "section": "GPT4 的詞義語用能力",
    "text": "GPT4 的詞義語用能力"
  },
  {
    "objectID": "weeks/week4.html#詞義的計算與應用-4-詞意角色標註",
    "href": "weeks/week4.html#詞義的計算與應用-4-詞意角色標註",
    "title": "Week 4 Word Meaning",
    "section": "詞義的計算與應用 (4): 詞意角色標註",
    "text": "詞義的計算與應用 (4): 詞意角色標註\nsemantic role labeling (SRL)"
  },
  {
    "objectID": "weeks/week4.html#任務評測",
    "href": "weeks/week4.html#任務評測",
    "title": "Week 4 Word Meaning",
    "section": "任務評測",
    "text": "任務評測\nEvaluation\n\n\n代表性的會議與前瞻任務 bakeoff SemEval: International Workshop on Semantic Evaluation"
  },
  {
    "objectID": "weeks/week5.html",
    "href": "weeks/week5.html",
    "title": "Week 5 Word Meaning (II)",
    "section": "",
    "text": "詞彙關係、詞彙網路、知識本體\n事件語意與計算表徵"
  },
  {
    "objectID": "weeks/week5.html#in-class-exercise.1",
    "href": "weeks/week5.html#in-class-exercise.1",
    "title": "Week 5 Word Meaning (II)",
    "section": "In-class exercise.1",
    "text": "In-class exercise.1\nCreate a tiny knowledge graph based on the news article in Chapter 21 of the textbook. 參考"
  },
  {
    "objectID": "weeks/week5.html#relations-in-named-entity-tasks",
    "href": "weeks/week5.html#relations-in-named-entity-tasks",
    "title": "Week 5 Word Meaning (II)",
    "section": "Relations in Named Entity Tasks",
    "text": "Relations in Named Entity Tasks\n\nNamed entity recognition (NER) is the task of identifying and classifying named entities in text into pre-defined categories such as person names, organization names, locations, etc."
  },
  {
    "objectID": "weeks/week5.html#regular-polysemy-detecection",
    "href": "weeks/week5.html#regular-polysemy-detecection",
    "title": "Week 5 Word Meaning (II)",
    "section": "Regular Polysemy Detecection",
    "text": "Regular Polysemy Detecection\nSemEval 2010-Task 8"
  },
  {
    "objectID": "weeks/week5.html#qualia-structure",
    "href": "weeks/week5.html#qualia-structure",
    "title": "Week 5 Word Meaning (II)",
    "section": "Qualia structure",
    "text": "Qualia structure\n\n\n\nPustejovsky"
  },
  {
    "objectID": "weeks/week5.html#rdf-resource-description-framework",
    "href": "weeks/week5.html#rdf-resource-description-framework",
    "title": "Week 5 Word Meaning (II)",
    "section": "RDF (Resource Description Framework)",
    "text": "RDF (Resource Description Framework)\n\nRDF is a standard metalanguage (W3 recommendation) for data interchange on the Web.\nRDF triples are the basic unit of data in RDF, a triple consists of < subject, predicate, object >\nRDF triples are used to describe resources (e.g., people, places, things, etc.) and their properties (e.g., name, age, etc.).\nDBpedia and Wikidata are two popular knowledge bases that use RDF triples to represent knowledge."
  },
  {
    "objectID": "weeks/week5.html#relation-extraction-algorithms",
    "href": "weeks/week5.html#relation-extraction-algorithms",
    "title": "Week 5 Word Meaning (II)",
    "section": "Relation Extraction Algorithms",
    "text": "Relation Extraction Algorithms\nFive main classes of relation extraction algorithms:\n\nPattern-based (Hearst patterns)\nFeature-based supervised relation classifier\nNeural supervised relation classifiers\nSemi-/un- supervised"
  },
  {
    "objectID": "weeks/week5.html#spanbert",
    "href": "weeks/week5.html#spanbert",
    "title": "Week 5 Word Meaning (II)",
    "section": "SpanBERT",
    "text": "SpanBERT\n(Joshi et al. 2020) exceeds BERT by 3.3% F1."
  },
  {
    "objectID": "weeks/week5.html#semisupervised-relation-extraction",
    "href": "weeks/week5.html#semisupervised-relation-extraction",
    "title": "Week 5 Word Meaning (II)",
    "section": "Semisupervised Relation Extraction",
    "text": "Semisupervised Relation Extraction\nbootstrapping\n\nBootstrapping is a semi-supervised learning method that uses a small amount of labeled data to train a classifier, and then uses the classifier to label a large amount of unlabeled data."
  },
  {
    "objectID": "weeks/week5.html#semisupervised-relation-extraction-1",
    "href": "weeks/week5.html#semisupervised-relation-extraction-1",
    "title": "Week 5 Word Meaning (II)",
    "section": "Semisupervised Relation Extraction",
    "text": "Semisupervised Relation Extraction\nDistant Supervision\n\nDistant supervision is a semi-supervised learning method that uses a large amount of unlabeled data and a small amount of labeled data to train a classifier."
  },
  {
    "objectID": "weeks/week5.html#evaluation-metrics",
    "href": "weeks/week5.html#evaluation-metrics",
    "title": "Week 5 Word Meaning (II)",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics"
  },
  {
    "objectID": "weeks/week5.html#實務運用上的練習",
    "href": "weeks/week5.html#實務運用上的練習",
    "title": "Week 5 Word Meaning (II)",
    "section": "實務運用上的練習",
    "text": "實務運用上的練習\n\n\n\n\n\n\nTip\n\n\n\nExtract money and currency values (entities labelled as MONEY) and find the noun phrase they are referring to - for example: “Net income was $9.4 million compared to the prior year of $2.7 million.”\n$9.4 million → Net income.\n$2.7 million → the prior year"
  },
  {
    "objectID": "weeks/week5.html#一般解法",
    "href": "weeks/week5.html#一般解法",
    "title": "Week 5 Word Meaning (II)",
    "section": "一般解法",
    "text": "一般解法\n\nStep 1: use spaCy’s named entity recognizer to extract money and currency values (entities labelled as MONEY)\nStep2: use spaCy’s dependency parser to find the noun phrase they are referring to."
  },
  {
    "objectID": "weeks/week5.html#gpt4-解法",
    "href": "weeks/week5.html#gpt4-解法",
    "title": "Week 5 Word Meaning (II)",
    "section": "GPT4 解法",
    "text": "GPT4 解法"
  },
  {
    "objectID": "weeks/week5.html#實務練習二",
    "href": "weeks/week5.html#實務練習二",
    "title": "Week 5 Word Meaning (II)",
    "section": "實務練習二",
    "text": "實務練習二\n\n用 wordnet 做知識圖譜，week5.ipnb\n計算詞彙的詞意頻率"
  },
  {
    "objectID": "weeks/week5.html#event-types-aktionsart-lexical-aspect",
    "href": "weeks/week5.html#event-types-aktionsart-lexical-aspect",
    "title": "Week 5 Word Meaning (II)",
    "section": "Event types (Aktionsart; Lexical Aspect)",
    "text": "Event types (Aktionsart; Lexical Aspect)\n\n\nstate, activitis, accomplishments, achievements, semelfactives\n\n\n\n\n\n\n\nTip\n\n\n\nAktionsart is a German term that refers to the type of action that a verb expresses."
  },
  {
    "objectID": "weeks/week5.html#event-representation",
    "href": "weeks/week5.html#event-representation",
    "title": "Week 5 Word Meaning (II)",
    "section": "Event Representation",
    "text": "Event Representation\n\nEvent representation is the task of representing events in a structured format."
  },
  {
    "objectID": "weeks/week5.html#event-extraction",
    "href": "weeks/week5.html#event-extraction",
    "title": "Week 5 Word Meaning (II)",
    "section": "Event Extraction",
    "text": "Event Extraction\n\nEvent extraction is the task of identifying mentions of events and classifying them in text.\nan event mention is a span of text (expressions) that refers to an event that can be assigned to a particular point or interval in time."
  },
  {
    "objectID": "weeks/week5.html#chinese-framenet",
    "href": "weeks/week5.html#chinese-framenet",
    "title": "Week 5 Word Meaning (II)",
    "section": "Chinese FrameNet",
    "text": "Chinese FrameNet\n\n(中國) Chinese FrameNet project (CFN) by the State Key Laboratory of Intelligent Technology and Systems at Tsinghua University in Beijing. 失效的計劃連接\n(台灣) NTU 半自動生成版\n(香港) 中文 VerbNet"
  },
  {
    "objectID": "weeks/week5.html#以-caused-motion-為例",
    "href": "weeks/week5.html#以-caused-motion-為例",
    "title": "Week 5 Word Meaning (II)",
    "section": "以 CAUSED-MOTION 為例",
    "text": "以 CAUSED-MOTION 為例"
  },
  {
    "objectID": "weeks/week5.html#frames-constructions-and-framenet",
    "href": "weeks/week5.html#frames-constructions-and-framenet",
    "title": "Week 5 Word Meaning (II)",
    "section": "Frames, constructions, and FrameNet",
    "text": "Frames, constructions, and FrameNet\nCharles J. Fillmore 過世前的經典論文"
  },
  {
    "objectID": "weeks/week5.html#延伸出來的認知語言研究",
    "href": "weeks/week5.html#延伸出來的認知語言研究",
    "title": "Week 5 Word Meaning (II)",
    "section": "延伸出來的認知語言研究",
    "text": "延伸出來的認知語言研究\n視角 (perspective) 怎麼反應在語言上？\n\n\n\ncommercial_event frame"
  },
  {
    "objectID": "weeks/week6.html",
    "href": "weeks/week6.html",
    "title": "Week 6 Word Meaning (III)",
    "section": "",
    "text": "詞彙的情感意義\n自動情緒分析 sentiment analysis"
  },
  {
    "objectID": "weeks/week6.html#這個範式轉移還在迅速發生中",
    "href": "weeks/week6.html#這個範式轉移還在迅速發生中",
    "title": "Week 6 Word Meaning (III)",
    "section": "這個範式轉移還在迅速發生中",
    "text": "這個範式轉移還在迅速發生中\nPretrain, Prompt, and Predict (P. Liu et al. 2023)\n\n\n\nParadigm shift in NLP"
  },
  {
    "objectID": "weeks/week6.html#llm-2023-",
    "href": "weeks/week6.html#llm-2023-",
    "title": "Week 6 Word Meaning (III)",
    "section": "LLM 2023-",
    "text": "LLM 2023-\n\n\n\n語言與知識資源作爲一種符碼專家模式是合理的 (在產學攻防的脈絡亦有其意義)。目前又稱 neuro-symbolic approach。\n接下來的 Bakeoff (week 13) 我們會讓大家練習實作這個方向。"
  },
  {
    "objectID": "weeks/week6.html#今天的打算",
    "href": "weeks/week6.html#今天的打算",
    "title": "Week 6 Word Meaning (III)",
    "section": "今天的打算",
    "text": "今天的打算\n\n我們一起讀過 SLP-chap 25。\nFrameNet data analysis"
  },
  {
    "objectID": "weeks/week6.html#相關的整組概念",
    "href": "weeks/week6.html#相關的整組概念",
    "title": "Week 6 Word Meaning (III)",
    "section": "相關的整組概念",
    "text": "相關的整組概念\n包括了 deceptive opinions (詐騙意見)、emotion (情緒)、sentiment (情感)、subjectivity (主觀性)、stance (立場)、trustworthiness (可信度)、polarity (極性)、irony (反諷)、sarcasm (嘲笑)、humor (幽默)、propaganda (宣傳)、bias (偏見)、aggression (侵略性)、toxicity (毒性)、evaluation、appraisal、affect、mood、feelings、beliefs intention 等等。\n\n\n\n\n\n\nImportant\n\n\n\n先做概念的區分、實用出發的詞意區分，再來做實際的應用。"
  },
  {
    "objectID": "weeks/week6.html#比較常討論的情感分析",
    "href": "weeks/week6.html#比較常討論的情感分析",
    "title": "Week 6 Word Meaning (III)",
    "section": "比較常討論的情感分析",
    "text": "比較常討論的情感分析\n\nsentiment is defined as an attitude, thought, or judgment prompted by feeling, whereas opinion is defined as a view, judgment, or appraisal formed in the mind about a particular matter.(B. Liu 2020)\n\n\n\n\n\n\n\nsentiment 和 opinion 在語言使用上的差異\n\n\n\n\n我有點擔心今年全球的經濟。（我也是/我也蠻擔心的）\n我認爲今年的經濟不會很好。（我同意/不同意）"
  },
  {
    "objectID": "weeks/week6.html#levels-of-analysis",
    "href": "weeks/week6.html#levels-of-analysis",
    "title": "Week 6 Word Meaning (III)",
    "section": "Levels of Analysis",
    "text": "Levels of Analysis\n\ndocument-level (文檔級)\nsentence-level (句子級)\naspect-level (方面級)\n\n實作上大部分都當成（類別 class、極度 polarity）分類問題。Aspect-based Sentiment Analysis 難度更高一點 (Zhang et al. 2022)。\n\n\n\nABSA"
  },
  {
    "objectID": "weeks/week6.html#但可能也沒那麼難了",
    "href": "weeks/week6.html#但可能也沒那麼難了",
    "title": "Week 6 Word Meaning (III)",
    "section": "但可能也沒那麼難了",
    "text": "但可能也沒那麼難了"
  },
  {
    "objectID": "weeks/week6.html#多模態的情感計算",
    "href": "weeks/week6.html#多模態的情感計算",
    "title": "Week 6 Word Meaning (III)",
    "section": "多模態的情感計算",
    "text": "多模態的情感計算\nmultimodal affective computing"
  },
  {
    "objectID": "weeks/week7.html",
    "href": "weeks/week7.html",
    "title": "Week 7 Syntax",
    "section": "",
    "text": "句法理論與計算表徵\n\n\n\n\n句法對於 NLP/NLU 很重要，因爲它涉及語言在表層的結構運作。\n但是 (形式) 句法在近代語言學發展史中佔據了很大（太大！）的一部分\n\n\n\n\n\n\n\nWarning\n\n\n\nN.Chomsky 對於語料庫，一直到現在的 LLM 的（鄙視）態度都一樣的，爲什麼呢？ 2012 on corpus linguistics; 2023 on chatGPT\n\n\n\n\n\n不同的數學哲學\n\n\n\n\n\n世紀論辯快要發生（結束）？\n\n\n\n\n\nFormal syntax/Geneartive Grammar\nTheory\n\n【句法】是第一順位。In generative grammar, pride of place is given to syntax, 也具備自己的【獨立模組】。\n【形式觀】：心智中的句法運算可以完全獨立於意義之外。\n\nData\n\ntop-down，有限規則駕馭無限表達，重點在掌握 competence 而非 performace。\n內省式語感判斷 rely on introspective judgments as their primary source of data.\nthe poverty-of-stimulus hypothesis (“the child has no data”).\n\n\n\n\n兩種在計算語言學上常使用的句法表徵 (representation)：\n- dependency \n- constituency"
  },
  {
    "objectID": "weeks/week7.html#modeling-constituency",
    "href": "weeks/week7.html#modeling-constituency",
    "title": "Week 7 Syntax",
    "section": "Modeling constituency",
    "text": "Modeling constituency\n\n目前最爲廣泛用來 model constituency 的形式系統叫做 Context-free Grammar (CfG)， 在語言學中稱 Phrase Structure Grammar (PSG) 。\n由 Chomsky 在 1950 年代提出，是一種用來描述語言句法結構的形式系統。"
  },
  {
    "objectID": "weeks/week7.html#context-free-grammar",
    "href": "weeks/week7.html#context-free-grammar",
    "title": "Week 7 Syntax",
    "section": "Context-free Grammar",
    "text": "Context-free Grammar\n\na set of rules (or productions) that describe how to construct a sentence from smaller units.\n形式上定義，CfG 是一個四元組 (4-tuple) \\((N, \\Sigma, R, S)\\)，其中：\n\n\\(N\\) 是一個有限集合，稱為 non-terminal symbols，代表句法結構的組成部分。\n\\(\\Sigma\\) 是一個有限集合，稱為 terminal symbols，代表句子中的詞。\n\\(R\\) 是一個有限集合，稱為 rules，代表句法結構的生成規則。\n\\(S\\) 是一個 \\(N\\) 中的元素，稱為 start symbol，代表句子的根節點。"
  },
  {
    "objectID": "weeks/week7.html#例子",
    "href": "weeks/week7.html#例子",
    "title": "Week 7 Syntax",
    "section": "例子",
    "text": "例子\n\n\n\n妳快把我笑死了\n\n\n如何解析以下句子？\n\n他說的都不對"
  },
  {
    "objectID": "weeks/week7.html#處理形式上的各種空缺問題",
    "href": "weeks/week7.html#處理形式上的各種空缺問題",
    "title": "Week 7 Syntax",
    "section": "處理形式上的各種空缺問題",
    "text": "處理形式上的各種空缺問題\n動詞的論元脫落有不同的符碼標記\n\n移位 (那個蘋果，我扔了)\n隱含（她打算打網球）\n空格（他買了兩根香蕉，給了他朋友一根）"
  },
  {
    "objectID": "weeks/week7.html#parsing",
    "href": "weeks/week7.html#parsing",
    "title": "Week 7 Syntax",
    "section": "Parsing",
    "text": "Parsing\n句法剖析\n\n(自動地) 把一個句子 (a string of word) 映射到其對應的句法結構 (a parse tree)，就叫做 parsing。"
  },
  {
    "objectID": "weeks/week7.html#treebanks",
    "href": "weeks/week7.html#treebanks",
    "title": "Week 7 Syntax",
    "section": "Treebanks",
    "text": "Treebanks\n\n一個 treebank 是一個語料庫，其中的每個句子都有一個對應的句法樹 (parse tree)。\n這些句法樹通常是由訓練過的語言學家標註、或修訂過的。\n\n\n\n\n\n\n\nInfo\n\n\n\n\nPenn Treebank\nCKIP Treebank"
  },
  {
    "objectID": "weeks/week7.html#例子-1",
    "href": "weeks/week7.html#例子-1",
    "title": "Week 7 Syntax",
    "section": "例子",
    "text": "例子"
  },
  {
    "objectID": "weeks/week7.html#formal-language-and-chomsky-normal-form",
    "href": "weeks/week7.html#formal-language-and-chomsky-normal-form",
    "title": "Week 7 Syntax",
    "section": "Formal language and Chomsky normal form",
    "text": "Formal language and Chomsky normal form\n\nIt is useful to have a normal from (i.e., each of the production/rule taks a particular form) for formal languages, so that we can compare different formal languages and their grammars.\nChomsky normal form (CNF) is a normal form for context-free grammars.\n\nA grammar is in CNF if all its rules are of the form:\n\n\\(A \\rightarrow BC\\)\n\\(A \\rightarrow a\\)\n\\(S \\rightarrow \\epsilon\\)\n\n\n\ncf. 形式語言的計算理論\n\nChomsky-adjunction"
  },
  {
    "objectID": "weeks/week7.html#structural-ambiguity",
    "href": "weeks/week7.html#structural-ambiguity",
    "title": "Week 7 Syntax",
    "section": "Structural Ambiguity",
    "text": "Structural Ambiguity\n結構歧義\n\n一個句子的句法結構可能有多種不同的解析方式，這就是結構歧義 (structural ambiguity)。\n最常見的結構歧義類型：附加 (attachment ambiguity) 和並列 (coordination ambiguity)。\n\n\nI shot an elephant in my pajamas"
  },
  {
    "objectID": "weeks/week7.html#練習中文歧義句",
    "href": "weeks/week7.html#練習中文歧義句",
    "title": "Week 7 Syntax",
    "section": "練習中文歧義句",
    "text": "練習中文歧義句\n\n\\(VP + NP_{1} + de + NP_{2}\\)\n\\(A + NP_{1} + NP_{2}\\)"
  },
  {
    "objectID": "weeks/week7.html#cky-parsing-algorithms",
    "href": "weeks/week7.html#cky-parsing-algorithms",
    "title": "Week 7 Syntax",
    "section": "CKY Parsing algorithms",
    "text": "CKY Parsing algorithms\n\nDynamic Programming\nEvaluation"
  },
  {
    "objectID": "weeks/week7.html#defining-dependencies",
    "href": "weeks/week7.html#defining-dependencies",
    "title": "Week 7 Syntax",
    "section": "Defining dependencies",
    "text": "Defining dependencies\n\n\n\n由法國語言學家 Lucien Tesnière 提出。\n對於語序較爲自由的、構詞豐富的 (morphologically rich languages) 語言，rule-based approach 很難應付。(以 polymorphic languages 爲例)\n此外，找到了主要語 (head) 及其依存 (dependent) 有助於語意剖析 (semantic parsing)。（可以容納句法和語意結構並存）\n\n(Gerdes, Hajičová, and Wanner 2013)"
  },
  {
    "objectID": "weeks/week7.html#動詞的配價理論",
    "href": "weeks/week7.html#動詞的配價理論",
    "title": "Week 7 Syntax",
    "section": "動詞的配價理論",
    "text": "動詞的配價理論\n主要語或中心語\n\n來自化學元素的類比：句子像是分子，由原子組成，而原子的核心是配價 (valence) （按一定的價結合在一起）。\n動詞是中心，所有成分都是環繞着中心動詞開展的。\n價數、價質（semantic roles）、價形 (syntactic position)"
  },
  {
    "objectID": "weeks/week7.html#dependency-的形式定義",
    "href": "weeks/week7.html#dependency-的形式定義",
    "title": "Week 7 Syntax",
    "section": "Dependency 的形式定義",
    "text": "Dependency 的形式定義\n\n一個句子的 dependency tree 是一個有向無環圖，其中每個節點代表一個詞，每個邊代表一個依存關係 (dependency relation)。\n依存關係是一個非對稱的二元關係，其中一個元素是 head，另一個元素是 dependent。\n\n\n\n\n\n\n\n\nTip\n\n\n\n非對稱的關係也帶出了階層的概念，此外，每個元素都可能同時支配與被支配。"
  },
  {
    "objectID": "weeks/week7.html#幾個概念",
    "href": "weeks/week7.html#幾個概念",
    "title": "Week 7 Syntax",
    "section": "幾個概念",
    "text": "幾個概念\n\nHead-Dependent 的箭頭方向：the origin word is the Head and the destination word is Dependent. (e.g., ‘prefer’ is Head & ‘I’ is Dependent.)\nRoot: Word which is the root of our parse tree. (It is ‘prefer’ in the above example).\nGrammar Functions and Arcs: Tags between each Head-Dependent pair is a grammar function determining the relation between the Head & Dependent. The arrowhead carrying the tag is called an Arc."
  },
  {
    "objectID": "weeks/week7.html#dependency-relations",
    "href": "weeks/week7.html#dependency-relations",
    "title": "Week 7 Syntax",
    "section": "Dependency relations",
    "text": "Dependency relations\n\n\n關係種類有很多分類方式，這裏採取 Universal Dependencies 的分類方式。\n\n\n\n\nUniversal Dependencies"
  },
  {
    "objectID": "weeks/week7.html#dependency-formalisms",
    "href": "weeks/week7.html#dependency-formalisms",
    "title": "Week 7 Syntax",
    "section": "Dependency Formalisms",
    "text": "Dependency Formalisms\n\n圖論上來看，dependencies can be represented as a directed graph \\(G= (V, A)\\) where V(set of vertices) represents words (and punctuation marks as well) in the sentence & A( set of arcs) represent the grammar relationship between elements of V.\n\n\n\nA dependency parse tree is the directed graph which has the below features:\n\nRoot 沒有傳入的弧 (incoming arc) (can only be Head in Head-Dependent pair)\n除了 root 之外的每一個節點，應該都只有一條傳入的弧 (Only one Parent/Head)\n從 root 到每一個節點都只有一條路。"
  },
  {
    "objectID": "weeks/week7.html#projectivity",
    "href": "weeks/week7.html#projectivity",
    "title": "Week 7 Syntax",
    "section": "Projectivity",
    "text": "Projectivity\n\nProjective arc: An arc/arrows_with_tag are projective when ‘Head’ associated with the arc has a path to reach each word that lies between ‘Head’ & ‘Dependent’.\n\n以 (18.2) 爲例，the 和 flights 的 arc 是 projective，因爲在此的 Head (即 flights) 到它的 Dependent (即 the) 之間有個 morning 這個詞， Head 也有路可到得了。\n同樣地，‘canceled’ (HEAD) 到 ‘flights’ (DEPENDENT) 這條 arc 也是 projective，因爲 Head 有路可到達它和其 Dependent 之間的詞。(i.e., ‘the’ (canceled \\(\\rightarrow\\) flights \\(\\rightarrow\\) the) 以及 ‘morning’ (canceled \\(\\rightarrow\\) flights \\(\\rightarrow\\) morning))\n\n\nProjective parse tree: A parse tree with all its arcs projective. The above tree is projective; A tree with at least one of the arcs as non-projective is called non-projective parse tree."
  },
  {
    "objectID": "weeks/week7.html#practice",
    "href": "weeks/week7.html#practice",
    "title": "Week 7 Syntax",
    "section": "Practice",
    "text": "Practice\n以下這個 dependency parse tree 是 projective or non-projective?"
  },
  {
    "objectID": "weeks/week7.html#graph-based-dependency-parsing-1",
    "href": "weeks/week7.html#graph-based-dependency-parsing-1",
    "title": "Week 7 Syntax",
    "section": "Graph-based Dependency Parsing",
    "text": "Graph-based Dependency Parsing\n歸結到兩個問題：\n[1] assigning a score to each edge 給邊分數\n[2] finding the best parse tree given the scores of all potential edges. 找出最好的依存樹"
  },
  {
    "objectID": "weeks/week7.html#chu-liuedmonds-algorithm",
    "href": "weeks/week7.html#chu-liuedmonds-algorithm",
    "title": "Week 7 Syntax",
    "section": "Chu-Liu/Edmonds’ Algorithm",
    "text": "Chu-Liu/Edmonds’ Algorithm\n解 [2] 的方法\n\nIt turns out that finding the best dependency parse for \\(S\\) is equivalent to finding the maximum spanning tree over \\(G\\).\n\n\nChu-Liu/Edmonds’ Algorithm is a greedy algorithm for finding the maximum spanning tree of a graph."
  },
  {
    "objectID": "weeks/week7.html#先要瞭解何謂最大擴張樹-maximum-spanning-tree",
    "href": "weeks/week7.html#先要瞭解何謂最大擴張樹-maximum-spanning-tree",
    "title": "Week 7 Syntax",
    "section": "先要瞭解何謂最大擴張樹 (Maximum Spanning Tree)",
    "text": "先要瞭解何謂最大擴張樹 (Maximum Spanning Tree)\n\n\n\nMST"
  },
  {
    "objectID": "weeks/week7.html#舉例",
    "href": "weeks/week7.html#舉例",
    "title": "Week 7 Syntax",
    "section": "舉例",
    "text": "舉例"
  },
  {
    "objectID": "weeks/week7.html#evaluation",
    "href": "weeks/week7.html#evaluation",
    "title": "Week 7 Syntax",
    "section": "Evaluation",
    "text": "Evaluation\n理想上的 exact match 指標是不太可行的。\n\nLabeled Attachment Score (LAS): the ratio of correctly detected Head-Dependent pairs along with their tag / total Head-Dependent pairs in testing data.\nUnlabeled Attachment Score (UAS): the ratio of correctly detected Head_Dependent pairs (irrespective of the tag) / total Head-Dependent pairs in testing data.\n\n\n\nReference 指的是標準答案，System 指的是系統生成的答案。\nTotal head-dependent pairs 指的是句子中所有的 Head-Dependent pair, 不管在 Reference 或 System 都是 6 。\nTotal Head-Dependent pairs correctly detected with tags = 4 (因爲 System 中的 nsubj 和 det 都是正確的)。\nTotal Head-Dependent pairs correctly detected = 5 (因爲只管 pairs，不管 tags，所以 book \\(\\rightarrow\\) flight 也算)。"
  },
  {
    "objectID": "weeks/week9.html",
    "href": "weeks/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "快速參考\n\n希望機器從餵給它的資料中學到一個預測方法。數學上地說，就是學到一個函式。\n\n\n\n\n\n\ntypes: source internet"
  },
  {
    "objectID": "weeks/week9.html#parametric-vs.-non-parametric",
    "href": "weeks/week9.html#parametric-vs.-non-parametric",
    "title": "Week 9",
    "section": "Parametric vs. Non-parametric",
    "text": "Parametric vs. Non-parametric\n\nParametric model：參數化的模型，參數的數目是固定的，模型的複雜度是固定的。\nNon-parametric model：非參數化的模型，參數的數目是不固定的，隨着資料大小而不同。"
  },
  {
    "objectID": "weeks/week9.html#生成式學習-generative-learning",
    "href": "weeks/week9.html#生成式學習-generative-learning",
    "title": "Week 9",
    "section": "生成式學習 (Generative Learning)",
    "text": "生成式學習 (Generative Learning)\na.k.a. 結構化學習 (structured learning)\n\n結構化學習是指輸出爲一個結構化的物件，如語音辨識、機器翻譯、語言模型等。"
  },
  {
    "objectID": "weeks/week9.html#機器學習-1",
    "href": "weeks/week9.html#機器學習-1",
    "title": "Week 9",
    "section": "機器學習",
    "text": "機器學習\n\n選擇 (候選) 的函式集合 \n決定評測函式的方法 (e.g., loss function)\n找最好的函式 (最佳化 optimization)\n\n\n\n\n\n\n\nNote\n\n\n\nchatGPT (貌似) 將生成式學習解成分類問題。"
  },
  {
    "objectID": "weeks/week9.html#我們用一個例子來說明",
    "href": "weeks/week9.html#我們用一個例子來說明",
    "title": "Week 9",
    "section": "我們用一個例子來說明",
    "text": "我們用一個例子來說明\n假設我們的任務是"
  },
  {
    "objectID": "weeks/week9.html#令人困擾的語言表徵問題",
    "href": "weeks/week9.html#令人困擾的語言表徵問題",
    "title": "Week 9",
    "section": "令人困擾的語言表徵問題",
    "text": "令人困擾的語言表徵問題\nNatural Language as (discrete) symbols or (continuous) signals?\n\n\n\nsource: Joty 2022"
  },
  {
    "objectID": "weeks/week9.html#embeddings",
    "href": "weeks/week9.html#embeddings",
    "title": "Week 9",
    "section": "Embeddings",
    "text": "Embeddings\n\n一種表徵方法，將一個物件用一個向量表徵。例如，word embeddings 將一個詞用一個向量表徵，該向量的每個元素代表該詞的某個特徵。"
  },
  {
    "objectID": "weeks/week9.html#之前做法",
    "href": "weeks/week9.html#之前做法",
    "title": "Week 9",
    "section": "之前做法",
    "text": "之前做法\n\none-hot encoding\ntf-idf (term frequency-inverse document frequency)\n詞袋假說 bag-of-words (BoW) assumption： 詞的順序 (order)、語境 (context) 不重要，只看詞的出現次數。"
  },
  {
    "objectID": "weeks/week9.html#ngram-語言模型",
    "href": "weeks/week9.html#ngram-語言模型",
    "title": "Week 9",
    "section": "ngram 語言模型",
    "text": "ngram 語言模型"
  },
  {
    "objectID": "weeks/week9.html#用深度學習學習表徵-representation-learning",
    "href": "weeks/week9.html#用深度學習學習表徵-representation-learning",
    "title": "Week 9",
    "section": "用深度學習學習表徵 (Representation Learning)",
    "text": "用深度學習學習表徵 (Representation Learning)\n\n自動學習特徵 (features) 或表徵 (representations)。\n從輸入 \\(x\\) ，利用多層神經網路學習多層次表徵 (\\(h^{1}, h^{2}, h^{3}, ..\\)) 以及最終的輸出 \\(h^{4}\\)。\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(x\\) 可以是 sound, pixels, words, etc. 深度學習在 speech, vision, language 上取得了很大的進展。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "計算語言學與語言學理論",
    "section": "",
    "text": "週次\n日期\n主題\n預習 📖\n投影片\n助教課\n課堂練習\n作業\n\n\n\n\n1\n02/23\n課程導論\n\norientation\n\n\n\n\n\n2\n03/02\n計算語言學導論\n\nw2\nslides\ncolab\nHW1, HW1_Ans\n\n\n3\n03/09\n計算構詞與詞彙 (1)\nslp3-02\nw3\nslides\ncolab\nHW2, HW2_Ans\n\n\n4\n03/16\n計算構詞與詞彙 (2)\nslp3-23\nw4\n\ncolab\nHW3, HW3_Ans\n\n\n5\n03/23\n計算構詞與詞彙 (3)\nslp3-21\nw5\nguidline\ncolab, gsheet\nHW4, HW4_Ans\n\n\n6\n03/30\n計算構詞與詞彙 (4)\nslp3-25\nw6\n\ncolab\nHW5, HW5_Ans\n\n\n7\n04/06\n句法理論與剖析器\nslp3-18; (Linzen and Baroni, 2021)\nw7\nslides\n\nHW6, HW6_Ans\n\n\n8\n04/13\n認知語法與計算\ncognitive linguistics meets computational linguistics\nw8\nslides\ncolab\nHW7\n\n\n9\n04/20\n語意表徵 (1): 分佈向量語意\nslp3-06\nw9\nslides\ncolab\nHW8\n\n\n10\n04/27\n語意表徵 (2): Neural Language Models and Pre-trained LLMs\nslp3-10\n\n\n\n\n\n\n11\n05/04\n語意表徵 (3): Prompt and Instruction Learning\nslp3-11\n\n\n\n\n\n\n12\n05/11\n語意表徵 (4): Prompt Engineering and LLM-based applications\n\n\n\n\n\n\n\n13\n05/18\nBakeoff (LLMs and LangChain practice)\n\n\n\n\n\n\n\n14\n05/25\n語音、多模態與言談分析 + 業師演講\nslp3-15; slp3-28\n\n\n\n\n\n\n15\n06/01\n綜合討論\n\n\n\n\n\n\n\n16\n06/08\n期末報告"
  }
]