{
  "hash": "1fffcd0ec27f97384624d9875ff12250",
  "result": {
    "markdown": "---\ntitle: \"Week 3 Word\"\nauthor: \"謝舒凱 台大語言所\"\nformat: \n    revealjs:\n      slide-number: true\n      preview-links: auto\n      chalkboard: true\n      multiplex: true\n      logo: /images/logo_round.png\n    html:\n        code-fold: true\nbibliography: ../cllt.bib\n---\n\n# 今天主題\n**詞彙、構詞與計算**\n\n- 文本處理：文本正則化、語料庫與程式概念工具\n\n- Wordhood： many perspectives\n\n- [Lab] Corpus processing skills \n\n\n\n## 語言學：給自然語言處理的印象\n[Linguistics for NLP]{.tiny-text}\n\n![](../images/NLP-Pyramid.png){width=80%}\n\n\n## 語言與文本\n[Language and Text]{.tiny-text}\n\n::: {.incremental}\n- 文本處理是自然語言處理的基礎工作，是一種對文本進行分析、處理、編輯、轉換的技術。\n\n    - 很適合用來搭配程式學習。\n\n- 文本正則 ([text normalization]{.blue-text}) 是一種對文本進行 *標準化* 的轉換任務。\n:::\n\n\n::: notes\n標準化在此意義是對當下任務更便捷、更容易處理的意思。\n:::\n\n\n## 文本正則化\n[text normalization tasks]{.tiny-text}\n\n:::{.incremental}\n\n- **tokenization** ：將文本分割成一個個的（詞符）單位。\n\n- **lemmatization**：將不同表面形式的詞彙還原成相同詞根。\n\n- **stemming** ：簡化版的 **lemmatization**，只把後綴拿掉。\n\n:::\n\n## \n\n- 有時候，如處理古漢語，還需要斷句 **sentence segmentation**。\n\n\n![](../images/sentSeg.jpeg){width=\"80%\"}\n\n\n\n\n\n## 文本正則化和前處理\n\n\n\n::: {.callout-important} \n## 比較：前處理任務「比較」不涉及語言學知識。   \n    - case folding：將所有字母轉換成小寫。\n    - stopword removal：去除停用詞。\n:::\n\n\n\n\n## 切符和斷詞\n**Tokenization vs Word Segmentation**\n\n::: {.incremental}\n- Word segmentation 貌似 word-based tokenization，但在中文與一些語言的脈絡中，它們不是完全一樣的概念。\n\n- 主要的差異點在於 wordhood 的概念是不定的、使用中的語意決定的。\n\n:::\n\n\n## East Asian languages\n\n```python\n# Chinese\nimport jieba\njieba.lcut('還要如此費工，我認爲是duck可不必')\n# ['還要', '如此', '費工', '，', '我認', '爲', '是', 'duck', '可', '不必']\n\n# Japanese\nimport nagisa\ntext = 'Pythonで簡単に使えるツールです'\ndoc = nagisa.tagging(text)\ndoc.words\n# ['Python', 'で', '簡単', 'に', '使える', 'ツール', 'です']\n\n# Korean\nimport konlpy\nphrase = \"아버지가방에들어가신다\"\nfrom konlpy.tag import Hannanum\nhannanum = Hannanum()\nhannanum.morphs(phrase)\n# ['아버지가방에들어가', '이', '시ㄴ다']\n\n# Thai\nimport tltk\nphrase = \"\"\"สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ \nในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์\"\"\"\npieces = tltk.nlp.pos_tag(phrase)\npieces\n# [[('สำนักงาน', 'NOUN'),\n#   ('เขต', 'NOUN'),\n#   ('จตุจักร', 'PROPN'),\n#   ('ชี้แจง', 'VERB'),\n#   ('ว่า', 'SCONJ'),\n#   ('<s/>', 'PUNCT')],\n#  ...\n\n```\n\n\n\n## 語言資源與工具\n[data and tools]{.tiny-text}\n\n要進行文本的處理，我們需要（大量）語料與好用的程式工具。\n\n\n## 語料庫\n[Corpus and Corpora]{.tiny-text}\n\n- **語料庫** 是一種重要的[語言資源]{.blue-text}(language resource) 。\n\n- 文本語料庫是較常被使用的語言資源，它不僅是一個文本的集合 (collection of texts)，藉由標記 (annotation)，也是語言知識外顯化的一種形式。\n\n::: {.callout-tip}\n[資料科學與語料分析:方法與實務](https://lopentu.github.io/corpus-book/?chap=%E8%AA%9E%E6%96%99%E8%99%95%E7%90%86%E6%96%B9%E6%B3%95%E5%B0%8E%E8%AB%96)：語言學角度的語料庫方法教科書\n\n:::\n\n## 字詞分佈法則\n[Language Laws]{.tiny-text}\n\n有了語料庫，我們就可以進行語言學的量化研究，例如發現語言法則。\n\n::: {.callout-important}\n要先知道 `type` (通常用 $V$ 表示) 與 `token` (通常用 $N$ 表示) 的差別。前者是所觀察的語料中字詞的種類，後者是字詞的總數。\n$|V|$ 則表示種類的數量 (vocabulary size)。\n:::\n\n\n## 幾個已證實的著名的法則\n:::{.incremental}\n\n- [Zipf's law](https://www.wikiwand.com/en/Zipf's_law)：在給定的語料中，對於任意一個語詞，其頻率 (frequency) 的排名（rank）和頻率的乘積大致是一個常數。\n\n\n- Heaps' law (Herdan's law)：在給定的語料中，($|V|$) 大致是語料大小 (N) 的一個指數函數。\n\n$|V| = kN^{\\beta}$, where $k$ and $\\beta$ are positive constants, and 0 < $\\beta$ < 1 \n\n<!-- - Zipf-Mandelbrot law： -->\n\n- [Menzerath–Altmann law](https://www.wikiwand.com/en/Menzerath%27s_law)：the increase of the size of a linguistic construct results in a decrease of the size of its constituents.\n\n\n:::\n\n## \n\n::: {layout-ncol=2}\n![zip's law](../images/Zipf_30wiki_en_labels.png){width=50%}\n\n:::\n\n\n\n\n## 練習 1\n先想一下怎麼實作。請 `chatGPT` 當程式助教，完成 Heap's Law 的實驗觀察。\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# your code here\n```\n:::\n\n\n# 正則表達式\n\n\n- 正則表達式 (Regular Expression) 是常應用在文本正則化任務的強大的**程式工具**。\n\n- 我們可以用正則表達式來描述一個字串的模式。\n\n- 一起來看本週大家已經[預習的章節](https://web.stanford.edu/~jurafsky/slp3/2.pdf) :laughing:\n\n\n# 指令列\n另外一個好用的計算工具是指令列。\n\n- 可參閱 [指令列入門](https://langsci-edu.github.io/HoCoML/cmd/cmd4linguist.html#1)\n\n\n## 練習\nCorpus processing skills via command line (`bash`)\n\n\n\n\n\n\n# 切符演算 | Tokenization algorithms \n> a way of splitting a string into tokens and assigning an identifier to each token before feeding it to the DNN model (e.g., Transformer).\n\n\n- 可以是 **word-based**：將文本切成一個個的詞彙單位；**subword-based**：切成子詞彙單位； 或者是 **character-based**：切成字元單位。(這裡的字元單位是指一個個的字母或標點符號，而不是中文字。)\n\n\n\n![](../images/tokenization.jpeg){width=\"80%\"}\n\n\n\n\n::: notes\n[source](https://www.freecodecamp.org/news/evolution-of-tokenization/)\n:::\n\n\n\n\n\n\n## 常用的 Subword-based tokenization 方法\n\n- Subword-based tokenization 是目前較被使用的方法，因為它可以\n    - 處理 word-based 的問題（very large vocabulary size, large number of OOV tokens, and different meaning of very similar words）\n    \n    - 也可以處理 character-based 的問題（very long sequences and less meaningful individual tokens）。\n\n- 常用的方法有：\n    - byte-pair encoding (BPE) (> GPT family)\n    - wordpiece (> BERT)\n    - unigram language modeling\n    - Sentencee piece\n\n\n\n\n## Byte-pair encoding (BPE)\n**Subword-based tokenization algorithm** [@sennrich2015neural]\n\n- token learner and token segmenter\n\n![Learner 做的事](../images/bpe.png){width=\"80%\"}\n\n- 我們用[原作者的投影片](https://web.stanford.edu/~jurafsky/slp3/slides/2_TextProc_Mar_25_2021.pdf)來說明\n\n\n\n\n\n\n\n## 練習\n[使用 HuggingFace 的 tokenizer](https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/)\n\n\n\n\n<!-- \n\n# String comparison\n## Minimum Edit Distance (MED)\n -->\n\n\n\n\n# Wordhood: a linguistic perspective\n\n- 詞的問題，可以帶出一個很本質的問題。\n\n- 詞是動態的形意結合，詞的組合不是符碼的靜態結合。\n\n- 粘合 vs 隼合\n\n## Word vs Morpheme\n\n- **morpheme**：語言中最小的意義單位。\n- **word**：語言中最小的概念單位。\n\n\n## 中文的詞素概念\n\n\n\n\n## 中文斷詞問題\nChinese word segmentation: principles and practices\n<!-- guideline advertise -->\n\n::: {layout-ncol=2}\n![](../images/book.png){width=50%}\n![](../images/routledge.webp){width=50%}\n:::\n\n\n\n## 爲什麼「現在的」 chatGPT 不太會斷/算詞？\n\n![](../images/chat.png){width=80%}\n \n\n\n# Wordhood: a linguistic typological perspective\n下週再繼續\n\n",
    "supporting": [
      "week3_files"
    ],
    "filters": [],
    "includes": {}
  }
}