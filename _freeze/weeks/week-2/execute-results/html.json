{
  "hash": "5f8fcc5be9e15e043e51adc2713c8226",
  "result": {
    "markdown": "---\ntitle: \"Week 2 Introduction\"\nauthor: \"謝舒凱 台大語言所\"\nformat: \n    revealjs:\n      slide-number: true\n      preview-links: auto\n      chalkboard: true\n      multiplex: true\n      logo: /images/logo_round.png\nbibliography: ../cllt.bib\n---\n\n# 計算語言學入門\n\n> 實務應用上會使用 SOTA 的工具。科學探索上要學習背後的理論歷史。\n\n\n\n## 上週重點\n\n- 自然語言處理和計算語言學的異同\n\n- 但兩者的基礎類似 (機器學習、程式能力、演算法)\n    - 前者目前更側重語言模型的工程訓練\n    - 後者偏向邏輯推理與模組解析的語言理論研究\n\n\n\n## 本週課程\n\n- NLP 簡史\n\n- 語言學與機器學習\n\n- 語言理論與深度學習\n\n\n## NLP 簡史\n<!-- 圖示 -->\n\n## 語言學與機器學習 \n\n- 規則學習\n\n- 特徵學習\n\n- 深度學習\n\n\n## 爲何漸行漸遠？\n\n> DNNs' input data and architectures are not based on the symbolic representations familiar with linguistics, such as a parse trees or logical formulas. Instead, DNNs learn to encode words and sentences as vectors (sequences of real-numbers)...\n\n\n\n## 預訓練大型語言模型 (Pre-trained Large Language Models) 改變了 NLP \n目前 aka: `chatGPT`\n\n- 語言表達能力流暢 (儘管說的是真是假不確定)\n\n- 貌似可以進行討論 (但是背後推理機制不透明)\n\n\n## 以一個討論的場景爲例\n\ndebate ling vs wife\n\n\n\n\n\n## 預訓練大型語言模型 (Pre-trained Large Language Models)\n\n- 學習到的不僅僅是鄰近字串機率，還有一些結構性的知識。\n以許多語言的 `long-distance number agreement` 為例。(p8)\n\n\n##\n\n- 結構性的東西，在一定的數據學習之後，會 emerged。不需要預先規劃與給定。\n\n- 以 Papadimitriou and Jurafsky EMNLP 2020 \n\n\n\n## 先要瞭解語言怎麼被我們學習到的\nusage-based theory of language acquisition\n\n\n## 這就是形式語言學與認知功能語言學的震驚程度不同\n\n# 模型與語言理論之間的關係曖昧\n\n\n- 背後有哲學思潮\n\n\n## 理論語言學 | Theorectical Linguistics\n(mostly known as Formal Syntax)[^1]\n\n![](../images/ling.png){width=70%}\n\n- 大部分所謂的語言學理論，都是在生成語法的架構之下，主題包括：\n\n    - Learnability\n    - Generalization given insufficient data\n    - Centrality of syntactic structure\n    - . . \n\n\n::: {.notes}\none hour summary\n:::\n\n[^1]: 以上採自 [@baroni2021] 的觀點\n\n\n\n\n\n\n\n\n## 認知功能語言學 | Cognitive/Functional Linguistics\n- 晚近才引入 NLP 的解釋\n\n- Language as a complex system\n\n\n\n\n\n\n::: notes\n所謂理論語言學\n:::\n\n\n\n## 這些都蠻進階的，但我們慢慢從幾個重要的語言面向開始。\n\n- 詞彙\n- 句法\n- 語意\n- 語音與多模態\n\n::: {.callout-warning}\n探究這些現象的語言學理論背景是什麼？又怎麼連接到 NLP 的工程實踐？\n:::\n\n## 我認爲 (accords LeCun, 卡馬克)\n\n- 目前的 LLM (Generative AI) 不太是 GAI (General AI) 的實踐方向，但是可以當作一個 IA 謹慎使用。\n\n- 目前來說 (2023 上半年)，讓 LLM 發展更好一點，各司其職是個方法 :grin:\n\n`toolTransformer`； 事實與邏輯的學習\n[perplexity.ai](https://www.perplexity.ai/?s=u&uuid=91d2c8ce-f610-4972-942e-cb60e6058b78)\n\n\n## 但發展速度有點抓不住\nmultimodal LLM\n\n\n\n\n## 分出兩種 NLP \n::: {.incremental}\n\n- [1]（計算語言學傳統）利用計算模型與工具， 以人懂的方式 <span style=\"color:red;\">（符碼運算）</span>解析與瞭解語言 (及其他模態)。\n\n\n- [2]（post-LLM NLP 導向）以電腦會就好<span style=\"color:red;\">（張量運算）</span>的方式，開發與調整模型的 (驚人的) 落地應用。 \n:::\n\n\n[1] 提供挑戰與複雜性 (e.g., 辯論、認錯、同理、意識, ...)； [2] 反饋理論發展範式的合理性。\n\n\n\n## 回到我們課程設計上\n\n\n- [Lecture] 講解「與 NLP 相關的語言學理論」\n    - linguistically-oriented DNN analysis (cf. [BlackBox NLP 系列研討會](); [Computation in Linguistics 協會](https://blogs.umass.edu/scil/))\n\n    - (ML 課程搭配)  \n\n- [Lab] 透過實作練習與工具使用\n    - 語言模組解析 via `spacy`\n    - 語言模型應用 via `Hugging Face (transformers)`\n\n### References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "week-2_files"
    ],
    "filters": [],
    "includes": {}
  }
}